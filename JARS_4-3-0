# import packages
from tkinter import ttk
import sqlite3
import configparser
import glob
import pyperclip as pc
import re
import sys
import time
import numpy as np
import random
import os
import warnings
import tkinter as tk
import pandas as pd
import xlsxwriter
import shutil
import math
from subprocess import call
import ctypes
import datetime
from os import path, sep, remove
from time import sleep
from random import randint
from tkinter.ttk import *
from plotnine import *
from tkinter import *
from pathlib import Path
from functools import reduce
from statistics import mean
from tkinter import Tk
from PIL import Image
from shutil import move, rmtree
from tkinter import filedialog
# import ttkthemes
from warnings import filterwarnings
from tkinter.filedialog import askopenfilename
import statsmodels.tsa.statespace._filters
import statsmodels.tsa.statespace._filters._conventional
import statsmodels.tsa.statespace._filters._univariate
import statsmodels.tsa.statespace._filters._univariate_diffuse
import statsmodels.tsa.statespace._filters._inversions
import statsmodels.tsa.statespace._smoothers
import statsmodels.tsa.statespace._smoothers._conventional
import statsmodels.tsa.statespace._smoothers._univariate
import statsmodels.tsa.statespace._smoothers._univariate_diffuse
import statsmodels.tsa.statespace._smoothers._classical
import statsmodels.tsa.statespace._smoothers._alternative

# pyinstaller --onefile --add-data "mc_icon.ico;." --add-data "jalogo_tall.png;." --add-data "jalogo_long.png;." --add-data "jars_logo_blue_1000f.png;." --add-data "jars_logo_oran_1000f.png;." --add-data "C:\Users\JSVAR\PycharmProjects\Language\venv\Lib\site-packages\pandas\io\formats\templates\html.tpl;." JARS_3-0-1.py
# pyinstaller --add-data "mc_icon.ico;." --add-data "jalogo_tall.png;." --add-data "jalogo_long.png;." --add-data "jars_logo_blue_1000f.png;." --add-data "jars_logo_oran_1000f.png;." --add-data "C:\Users\JSVAR\PycharmProjects\Language\venv\Lib\site-packages\pandas\io\formats\templates\html.tpl;." JARS_3-0-1.py
# pyinstaller JARS_4-2-3.spec

jars_apptitle = "Joseph Arnson's Reporting Suite v4.3.0"
apptitle = 'Distributor Scorecard v2.5.0'
datasearch_apptitle = 'DataSearch v1.6.6'


"""Utility Functions"""


def resource_path(relative_path):
    """Get absolute path to resource, works for dev and for PyInstaller"""
    base_path = getattr(sys, '_MEIPASS', os.path.dirname(os.path.abspath(__file__)))
    return os.path.join(base_path, relative_path)


def os_split_fixer(string: str):
    """Separate and rejoin selected file path with correct operating system separator"""
    delimiters = '/', '\\'
    maxsplit = 0
    regexPattern = '|'.join(map(re.escape, delimiters))
    splits = re.split(regexPattern, string, maxsplit)
    splitlist = []
    for split in splits:
        splitlist.append(split + os.sep)

    fspath = ''.join(splitlist)
    splitpath = fspath[:-1]
    return splitpath


def uniq_dir_maker(directoryname: str) -> list:
    """Create a unique directory at destination"""
    # file destination select dialogue
    Tk().withdraw()  # prevent root window
    # open file explorer folder select window
    dirspath = filedialog.askdirectory(title='Select the output file save destination')
    if not dirspath:
        ctypes.windll.user32.MessageBoxW(0, 'Folder select cancelled.', 'Process Cancelled', 0)
        return
    # correct directory file path
    dirsavepath = os_split_fixer(dirspath + os.sep + directoryname)
    # try to create directory
    try:
        # create directory at destination without overwriting
        Path(dirsavepath).mkdir(parents=True, exist_ok=False)
    # if directory already exists add incremental integers until unique
    except FileExistsError:
        # create incrementing variable
        i = 1
        # determine incremented filename
        while os.path.exists(f"{dirsavepath} ({str(i)})"):
            i += 1
        # update directory path with incremented variable
        dirsavepath = dirsavepath + ' (' + str(i) + ')'
        # create now unique directory
        Path(dirsavepath).mkdir(parents=True, exist_ok=False)
    # add os separator to new directory for saving
    savepath = dirsavepath + os.sep
    return [savepath, dirsavepath]


def conf_uniq_dir_maker(directoryname: str) -> list:
    """Create a unique directory at destination"""
    # file destination select dialogue
    Tk().withdraw()  # prevent root window
    # open file explorer folder select window
    dirspath = filedialog.askdirectory(title='Select the output file save destination')
    if not dirspath:
        ctypes.windll.user32.MessageBoxW(0, 'Folder select cancelled.', 'Process Cancelled', 0)
        return
    # correct directory file path
    dirsavepath = os_split_fixer(dirspath + os.sep + directoryname)
    # try to create directory
    try:
        # create directory at destination without overwriting
        Path(dirsavepath).mkdir(parents=True, exist_ok=False)
    # if directory already exists add incremental integers until unique
    except FileExistsError:
        # create incrementing variable
        i = 1
        # determine incremented filename
        while os.path.exists(f"{dirsavepath} ({str(i)})"):
            i += 1
        # update directory path with incremented variable
        dirsavepath = dirsavepath + ' (' + str(i) + ')'
        # create now unique directory
        Path(dirsavepath).mkdir(parents=True, exist_ok=False)
    # add os separator to new directory for saving
    savepath = dirsavepath + os.sep
    return [savepath, dirsavepath, dirspath]


def uniq_file_maker(file: str) -> str:
    """Create a unique file path"""
    # get file name and extension
    filename, filext = os.path.splitext(os.path.basename(file))
    # get file directory path
    directory = os.path.dirname(file)
    # get file without extension only
    filexx = os_split_fixer(directory + os.sep + filename)
    # check if file exists
    if Path(file).exists():
        # create incrementing variable
        i = 1
        # determine incremented filename
        while os.path.exists(f"{filexx} ({str(i)}){filext}"):
            # update the incrementing variable
            i += 1
        # update file name with incremented variable
        filename = directory + os.sep + filename + ' (' + str(i) + ')' + filext
        # correct file path os separators
        filename = os_split_fixer(filename)
    else:
        # pass original name if already unique
        filename = file
    return filename


def format_excel(writer, df: pd.DataFrame, sheet_name: str, table_name: str,
                 table_style: str = 'Table Style Medium 2', auto_filter: bool = True, total_row: bool = False):
    """Dynamically formats an Excel worksheet as a table using xlsxwriter"""
    # reference the worksheet we need to update
    worksheet = writer.sheets[sheet_name]
    # collect the columns of the passed DataFrame
    column_settings = [{'header': column} for column in df.columns]
    # collect the shape of the passed DataFrame
    max_row, max_col = df.shape
    # apply table formatting to the shape of the DataFrame on the sheet
    worksheet.add_table(0, 0, max_row, max_col - 1, {'name': table_name,
                                                     'style': table_style,
                                                     'autofilter': auto_filter,
                                                     'total_row': total_row,
                                                     'columns': column_settings})
    # loop over the columns and change the width
    for i, col in enumerate(df.columns):
        # calculate column width for autosizing based on longest str in column
        column_len = max(df[col].astype(str).str.len().max() + 2, len(col) + 2)
        # format worksheet column
        worksheet.set_column(i, i, column_len)
    return


def get_timestamps() -> list:
    """Gets series of timestamps as well as week number and full date"""
    # get timestamps
    now = datetime.datetime.now()
    week = datetime.datetime.today().isocalendar()[1]
    hour = now.hour
    tdate1 = now.today().strftime('%m-%d-%Y')  # mm-dd-yyyy
    tdate2 = now.today().strftime('%m/%d/%Y')  # mm/dd/yyyy
    # convert hour to AM or PM
    if hour == 0:
        hour = '12AM'
    elif hour == 12:
        hour = '12PM'
    elif hour >= 13:
        # subtract 12 hours to convert from 24H to 12H
        hour -= 12
        hour = str(hour) + 'PM'
    else:
        hour = str(hour) + 'AM'
    # create full timestamp
    full = f"{now.year}-{str(now.month).zfill(2)}-{str(now.day).zfill(2)}-{str(now.hour).zfill(2)}-" \
           f"{str(now.minute).zfill(2)}-{str(now.second).zfill(2)}"
    return [now.year, now.month, week, now.day, hour, now.minute, now.second, tdate1, tdate2, full]


def get_past_timestamps(time_obj: datetime) -> list:
    """Gets timestamps based on past historical date"""
    # get timestamps
    week = time_obj.isocalendar()[1]
    hour = time_obj.hour
    tdate1 = time_obj.today().strftime('%m-%d-%Y')  # mm-dd-yyyy
    tdate2 = time_obj.today().strftime('%m/%d/%Y')  # mm/dd/yyyy
    # convert hour to AM or PM
    if hour == 0:
        hour = '12AM'
    elif hour == 12:
        hour = '12PM'
    elif hour >= 13:
        # subtract 12 hours to convert from 24H to 12H
        hour -= 12
        hour = str(hour) + 'PM'
    else:
        hour = str(hour) + 'AM'
    return [time_obj.year, time_obj.month, week, time_obj.day, hour, time_obj.minute, time_obj.second, tdate1, tdate2]


"""Picture Functions"""


def get_concat_v_resize(im1, im2, resample=Image.BICUBIC, resize_big_image=True):
    if im1.width == im2.width:
        _im1 = im1
        _im2 = im2
    elif (((im1.width > im2.width) and resize_big_image) or
          ((im1.width < im2.width) and not resize_big_image)):
        _im1 = im1.resize((im2.width, int(im1.height * im2.width / im1.width)), resample=resample)
        _im2 = im2
    else:
        _im1 = im1
        _im2 = im2.resize((im1.width, int(im2.height * im1.width / im2.width)), resample=resample)
    dst = Image.new('RGB', (_im1.width, _im1.height + _im2.height))
    dst.paste(_im1, (0, 0))
    dst.paste(_im2, (0, _im1.height))
    return dst


def get_concat_h_resize(im1, im2, resample=Image.BICUBIC, resize_big_image=True):
    if im1.height == im2.height:
        _im1 = im1
        _im2 = im2
    elif (((im1.height > im2.height) and resize_big_image) or
          ((im1.height < im2.height) and not resize_big_image)):
        _im1 = im1.resize((int(im1.width * im2.height / im1.height), im2.height), resample=resample)
        _im2 = im2
    else:
        _im1 = im1
        _im2 = im2.resize((int(im2.width * im1.height / im2.height), im1.height), resample=resample)
    dst = Image.new('RGB', (_im1.width + _im2.width, _im1.height))
    dst.paste(_im1, (0, 0))
    dst.paste(_im2, (_im1.width, 0))
    return dst


def get_concat_v_blank(im1, im2, color=(255, 255, 255)):
    """Concatenate two images vertically with a white margin"""
    dst = Image.new('RGB', (max(im1.width, im2.width), im1.height + im2.height), color)
    dst.paste(im1, (0, 0))
    dst.paste(im2, (0, im1.height))
    return dst


def get_concat_h_blank(im1, im2, color=(255, 255, 255)):
    """Concatenate two images horizontally with a white margin"""
    dst = Image.new('RGB', (im1.width + im2.width, max(im1.height, im2.height)), color)
    dst.paste(im1, (0, 0))
    dst.paste(im2, (im1.width, 0))
    return dst


"""DataSearch Functions"""


def uniq_ds_dir_maker(directoryname: str) -> list:
    """Create a unique directory at destination"""
    # try to create directory
    try:
        # create directory at destination without overwriting
        Path(directoryname).mkdir(parents=True, exist_ok=False)
    # if directory already exists add incremental integers until unique
    except FileExistsError:
        # create incrementing variable
        i = 1
        # determine incremented filename
        while os.path.exists(f"{directoryname} ({str(i)})"):
            i += 1
        # update directory path with incremented variable
        directoryname = directoryname + ' (' + str(i) + ')'
        # create now unique directory
        Path(directoryname).mkdir(parents=True, exist_ok=False)
    # add os separator to new directory for saving
    savepath = directoryname + os.sep
    return [savepath, directoryname]


def uniq_ds_file_maker(file: str) -> str:
    """Create a unique file path"""
    # get file name and extension
    filename, filext = os.path.splitext(os.path.basename(file))
    # get file directory path
    directory = os.path.dirname(file)
    # get file without extension only
    filexx = os_split_fixer(directory + os.sep + filename)
    # check if file exists
    if Path(file).exists():
        # create incrementing variable
        i = 1
        # determine incremented filename
        while os.path.exists(f"{filexx} ({str(i)}){filext}"):
            # update the incrementing variable
            i += 1
        # update file name with incremented variable
        filename = directory + os.sep + filename + ' (' + str(i) + ')' + filext
        # correct file path os separators
        filename = os_split_fixer(filename)
    else:
        # pass original name if already unique
        filename = file
    return filename


def ds_get_timestamps() -> list:
    """Gets series of timestamps as well as week number and full date"""
    # get timestamps
    now = datetime.datetime.now()
    week = datetime.datetime.today().isocalendar()[1]
    hour = now.hour
    tdate1 = now.today().strftime('%m-%d-%Y')  # mm-dd-yyyy
    tdate2 = now.today().strftime('%m/%d/%Y')  # mm/dd/yyyy
    # convert hour to AM or PM
    if hour == 0:
        hour = '12AM'
    elif hour == 12:
        hour = '12PM'
    elif hour >= 13:
        # subtract 12 hours to convert from 24H to 12H
        hour -= 12
        hour = str(hour) + 'PM'
    else:
        hour = str(hour) + 'AM'
    # create full timestamp
    full = f"{now.year}-{now.month}-{now.day}-{str(now.hour).zfill(2)}-" \
           f"{str(now.minute).zfill(2)}-{str(now.second).zfill(2)}"
    return [now.year, now.month, week, now.day, hour, now.minute, now.second, tdate1, tdate2, full]


def ds_get_past_timestamps(time_obj: datetime) -> list:
    """Gets timestamps based on past historical date"""
    # get timestamps
    week = time_obj.isocalendar()[1]
    hour = time_obj.hour
    tdate1 = time_obj.today().strftime('%m-%d-%Y')  # mm-dd-yyyy
    tdate2 = time_obj.today().strftime('%m/%d/%Y')  # mm/dd/yyyy
    # weekdays as a tuple
    weekdays_list = ('Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday')
    weekday = weekdays_list[time_obj.weekday()]
    # convert hour to AM or PM
    if hour == 0:
        hour = '12AM'
    elif hour == 12:
        hour = '12PM'
    elif hour >= 13:
        # subtract 12 hours to convert from 24H to 12H
        hour -= 12
        hour = str(hour) + 'PM'
    else:
        hour = str(hour) + 'AM'
    return [time_obj.year, time_obj.month, week, time_obj.day, hour,
            time_obj.minute, time_obj.second, tdate1, tdate2, weekday]


def ds_format_excel(df: pd.DataFrame, sheet_name: str, table_name: str, table_style: str, auto_filter: bool, writer):
    """Dynamically formats an excel worksheet as a table using xlsxwriter"""
    # reference the worksheet we need to update
    worksheet = writer.sheets[sheet_name]
    # collect the columns and shape of the passed DataFrame
    column_settings = [{'header': column} for column in df.columns]
    max_row, max_col = df.shape
    # apply table formatting to the shape of the DataFrame on the sheet
    worksheet.add_table(0, 0, max_row, max_col - 1, {'name': table_name, 'style': table_style,
                                                     'autofilter': auto_filter, 'columns': column_settings})
    # loop over the columns and change the width
    for i, col in enumerate(df.columns):
        column_len = max(df[col].astype(str).str.len().max(), len(col) + 2)
        worksheet.set_column(i, i, column_len)
    return


def write_datasearch_ini():
    """Create INI configuration file to initialize DataSearch settings"""
    if not os.path.isdir('DataSearch'):
        # notify user that an error has occurred
        ctypes.windll.user32.MessageBoxW(0,
                                         'DataSearch has encountered a fatal error. Source files were either moved or deleted and cannot be found. Please restart DataSearch to continue.',
                                         'Fatal Error Encountered', 0)
        # close DataSearch
        sys.exit(0)
    # initialize configparser object
    write_config = configparser.ConfigParser()
    # add section
    write_config.add_section('Products_Settings')
    # set values for app settings
    write_config.set('Products_Settings', 'plant', '1')
    write_config.set('Products_Settings', 'osku', '1')
    write_config.set('Products_Settings', 'msku', '1')
    write_config.set('Products_Settings', 'segment', '1')
    write_config.set('Products_Settings', 'description', '1')
    write_config.set('Products_Settings', 'run_freq', '1')
    write_config.set('Products_Settings', 'production', '1')
    write_config.set('Products_Settings', 'shelf_life', '1')
    write_config.set('Products_Settings', 'ship_policy', '1')
    write_config.set('Products_Settings', 'bbl_conversion', '1')
    write_config.set('Products_Settings', 'units_p_pallet', '1')
    write_config.set('Products_Settings', 'search_stacking', '0')
    write_config.set('Products_Settings', 'search_exact', '0')
    # add section
    write_config.add_section('Customers_Settings')
    # set values for app settings
    write_config.set('Customers_Settings', 'shipto', '1')
    write_config.set('Customers_Settings', 'shipto_name', '1')
    write_config.set('Customers_Settings', 'csa', '1')
    write_config.set('Customers_Settings', 'state', '1')
    write_config.set('Customers_Settings', 'state_code', '1')
    write_config.set('Customers_Settings', 'district', '1')
    write_config.set('Customers_Settings', 'city', '1')
    write_config.set('Customers_Settings', 'postal_code', '1')
    write_config.set('Customers_Settings', 'search_stacking', '0')
    write_config.set('Customers_Settings', 'search_exact', '0')
    # add section
    write_config.add_section('Plants_Settings')
    # set values for app settings
    write_config.set('Plants_Settings', 'plant', '1')
    write_config.set('Plants_Settings', 'plant_name', '1')
    write_config.set('Plants_Settings', 'plant_address', '1')
    write_config.set('Plants_Settings', 'plant_city', '1')
    write_config.set('Plants_Settings', 'plant_type', '1')
    write_config.set('Plants_Settings', 'search_stacking', '0')
    write_config.set('Plants_Settings', 'search_exact', '0')
    # add section
    write_config.add_section('Copy_Settings')
    # set values for app settings
    write_config.set('Copy_Settings', 'delimiter', 'Space')
    # create ini file object
    cfgfile = open('DataSearch\\DataSearch.ini', 'w')
    # write ini file
    write_config.write(cfgfile)
    # save and close ini file
    cfgfile.close()
    return


def get_datasearch_ini() -> list:
    """Get DataSearch INI configuration file settings details"""
    # check file dependencies
    datasearch_source_checker()
    # initialize configparser object
    read_config = configparser.ConfigParser()
    # read ini file values
    read_config.read('DataSearch\\DataSearch.ini')
    # get values from config file
    plant = read_config.get('Products_Settings', 'plant')
    osku = read_config.get('Products_Settings', 'osku')
    msku = read_config.get('Products_Settings', 'msku')
    segment = read_config.get('Products_Settings', 'segment')
    description = read_config.get('Products_Settings', 'description')
    run_freq = read_config.get('Products_Settings', 'run_freq')
    production = read_config.get('Products_Settings', 'production')
    shelf_life = read_config.get('Products_Settings', 'shelf_life')
    ship_policy = read_config.get('Products_Settings', 'ship_policy')
    bbl_conversion = read_config.get('Products_Settings', 'bbl_conversion')
    units_p_pallet = read_config.get('Products_Settings', 'units_p_pallet')
    p_search_stacking = read_config.get('Products_Settings', 'search_stacking')
    p_search_exact = read_config.get('Products_Settings', 'search_exact')
    shipto = read_config.get('Customers_Settings', 'shipto')
    shipto_name = read_config.get('Customers_Settings', 'shipto_name')
    csa = read_config.get('Customers_Settings', 'csa')
    state = read_config.get('Customers_Settings', 'state')
    state_code = read_config.get('Customers_Settings', 'state_code')
    district = read_config.get('Customers_Settings', 'district')
    city = read_config.get('Customers_Settings', 'city')
    postal_code = read_config.get('Customers_Settings', 'postal_code')
    c_search_stacking = read_config.get('Customers_Settings', 'search_stacking')
    c_search_exact = read_config.get('Customers_Settings', 'search_exact')
    lplant = read_config.get('Plants_Settings', 'plant')
    plant_name = read_config.get('Plants_Settings', 'plant_name')
    plant_address = read_config.get('Plants_Settings', 'plant_address')
    plant_city = read_config.get('Plants_Settings', 'plant_city')
    plant_type = read_config.get('Plants_Settings', 'plant_type')
    l_search_stacking = read_config.get('Plants_Settings', 'search_stacking')
    l_search_exact = read_config.get('Plants_Settings', 'search_exact')
    delimiter = read_config.get('Copy_Settings', 'delimiter')
    # create list of results
    datasearch_ini_list = [plant, osku, msku, segment, description, run_freq, production, shelf_life, ship_policy,
                           bbl_conversion, units_p_pallet, p_search_stacking, p_search_exact, shipto, shipto_name,
                           csa, state, state_code, district, city, postal_code, c_search_stacking, c_search_exact,
                           lplant, plant_name, plant_address, plant_city, plant_type, l_search_stacking,
                           l_search_exact, delimiter]
    return datasearch_ini_list


def loud_update_datasearch_ini():
    """Create INI configuration file to initialize DataSearch settings"""
    # check file dependencies
    datasearch_source_checker()
    # initialize configparser object
    write_config = configparser.ConfigParser()
    # add section
    write_config.add_section('Products_Settings')
    # set values for app settings
    write_config.set('Products_Settings', 'plant', str(plant_intvar.get()))
    write_config.set('Products_Settings', 'osku', str(osku_intvar.get()))
    write_config.set('Products_Settings', 'msku', str(msku_intvar.get()))
    write_config.set('Products_Settings', 'segment', str(segment_intvar.get()))
    write_config.set('Products_Settings', 'description', str(description_intvar.get()))
    write_config.set('Products_Settings', 'run_freq', str(run_freq_intvar.get()))
    write_config.set('Products_Settings', 'production', str(production_intvar.get()))
    write_config.set('Products_Settings', 'shelf_life', str(shelf_life_intvar.get()))
    write_config.set('Products_Settings', 'ship_policy', str(ship_policy_intvar.get()))
    write_config.set('Products_Settings', 'bbl_conversion', str(bbl_conversion_intvar.get()))
    write_config.set('Products_Settings', 'units_p_pallet', str(units_p_pallet_intvar.get()))
    write_config.set('Products_Settings', 'search_stacking', str(p_search_stacking_intvar.get()))
    write_config.set('Products_Settings', 'search_exact', str(p_search_exact_intvar.get()))
    # add section
    write_config.add_section('Customers_Settings')
    # set values for app settings
    write_config.set('Customers_Settings', 'shipto', str(shipto_intvar.get()))
    write_config.set('Customers_Settings', 'shipto_name', str(shipto_name_intvar.get()))
    write_config.set('Customers_Settings', 'csa', str(csa_intvar.get()))
    write_config.set('Customers_Settings', 'state', str(state_intvar.get()))
    write_config.set('Customers_Settings', 'state_code', str(state_code_intvar.get()))
    write_config.set('Customers_Settings', 'district', str(region_intvar.get()))
    write_config.set('Customers_Settings', 'city', str(city_intvar.get()))
    write_config.set('Customers_Settings', 'postal_code', str(postal_code_intvar.get()))
    write_config.set('Customers_Settings', 'search_stacking', str(c_search_stacking_intvar.get()))
    write_config.set('Customers_Settings', 'search_exact', str(c_search_exact_intvar.get()))
    # add section
    write_config.add_section('Plants_Settings')
    # set values for app settings
    write_config.set('Plants_Settings', 'plant', str(lplant_intvar.get()))
    write_config.set('Plants_Settings', 'plant_name', str(plant_name_intvar.get()))
    write_config.set('Plants_Settings', 'plant_address', str(plant_address_intvar.get()))
    write_config.set('Plants_Settings', 'plant_city', str(plant_city_intvar.get()))
    write_config.set('Plants_Settings', 'plant_type', str(plant_type_intvar.get()))
    write_config.set('Plants_Settings', 'search_stacking', str(l_search_stacking_intvar.get()))
    write_config.set('Plants_Settings', 'search_exact', str(l_search_exact_intvar.get()))
    # add section
    write_config.add_section('Copy_Settings')
    # set values for app settings based on combobox
    write_config.set('Copy_Settings', 'delimiter', str(delimiter_var.get()))
    # create ini file object
    cfgfile = open('DataSearch\\DataSearch.ini', 'w')
    # write ini file
    write_config.write(cfgfile)
    # save and close ini file
    cfgfile.close()
    # notify user that settings have been updated successfully
    ctypes.windll.user32.MessageBoxW(0,
                                     'Your DataSearch settings have been updated successfully.',
                                     'Settings Updated', 0)
    return


def quiet_update_datasearch_ini():
    """Update INI configuration file to initialize DataSearch settings"""
    # check file dependencies
    datasearch_source_checker()
    # initialize configparser object
    write_config = configparser.ConfigParser()
    # add section
    write_config.add_section('Products_Settings')
    # set values for app settings
    write_config.set('Products_Settings', 'plant', str(plant_intvar.get()))
    write_config.set('Products_Settings', 'osku', str(osku_intvar.get()))
    write_config.set('Products_Settings', 'msku', str(msku_intvar.get()))
    write_config.set('Products_Settings', 'segment', str(segment_intvar.get()))
    write_config.set('Products_Settings', 'description', str(description_intvar.get()))
    write_config.set('Products_Settings', 'run_freq', str(run_freq_intvar.get()))
    write_config.set('Products_Settings', 'production', str(production_intvar.get()))
    write_config.set('Products_Settings', 'shelf_life', str(shelf_life_intvar.get()))
    write_config.set('Products_Settings', 'ship_policy', str(ship_policy_intvar.get()))
    write_config.set('Products_Settings', 'bbl_conversion', str(bbl_conversion_intvar.get()))
    write_config.set('Products_Settings', 'units_p_pallet', str(units_p_pallet_intvar.get()))
    write_config.set('Products_Settings', 'search_stacking', str(p_search_stacking_intvar.get()))
    write_config.set('Products_Settings', 'search_exact', str(p_search_exact_intvar.get()))
    # add section
    write_config.add_section('Customers_Settings')
    # set values for app settings
    write_config.set('Customers_Settings', 'shipto', str(shipto_intvar.get()))
    write_config.set('Customers_Settings', 'shipto_name', str(shipto_name_intvar.get()))
    write_config.set('Customers_Settings', 'csa', str(csa_intvar.get()))
    write_config.set('Customers_Settings', 'state', str(state_intvar.get()))
    write_config.set('Customers_Settings', 'state_code', str(state_code_intvar.get()))
    write_config.set('Customers_Settings', 'district', str(region_intvar.get()))
    write_config.set('Customers_Settings', 'city', str(city_intvar.get()))
    write_config.set('Customers_Settings', 'postal_code', str(postal_code_intvar.get()))
    write_config.set('Customers_Settings', 'search_stacking', str(c_search_stacking_intvar.get()))
    write_config.set('Customers_Settings', 'search_exact', str(c_search_exact_intvar.get()))
    # add section
    write_config.add_section('Plants_Settings')
    # set values for app settings
    write_config.set('Plants_Settings', 'plant', str(lplant_intvar.get()))
    write_config.set('Plants_Settings', 'plant_name', str(plant_name_intvar.get()))
    write_config.set('Plants_Settings', 'plant_address', str(plant_address_intvar.get()))
    write_config.set('Plants_Settings', 'plant_city', str(plant_city_intvar.get()))
    write_config.set('Plants_Settings', 'plant_type', str(plant_type_intvar.get()))
    write_config.set('Plants_Settings', 'search_stacking', str(l_search_stacking_intvar.get()))
    write_config.set('Plants_Settings', 'search_exact', str(l_search_exact_intvar.get()))
    # add section
    write_config.add_section('Copy_Settings')
    # set values for app settings based on combobox
    write_config.set('Copy_Settings', 'delimiter', str(delimiter_var.get()))
    # create ini file object
    cfgfile = open('DataSearch\\DataSearch.ini', 'w')
    # write ini file
    write_config.write(cfgfile)
    # save and close ini file
    cfgfile.close()
    # check if products_stack should be reset
    if p_search_stacking_intvar.get() == 0:
        # delete products_stack.db
        unstack_products()
        # reset treeview
        query_products()
    # check if customers_stack should be reset
    if c_search_stacking_intvar.get() == 0:
        # delete customers_stack.db
        unstack_customers()
        # reset treeview
        query_customers()
    # check if plants_stack should be reset
    if l_search_stacking_intvar.get() == 0:
        # delete customers_stack.db
        unstack_plants()
        # reset treeview
        query_plants()
    return


def datasearch_source_checker():
    """Check if all dependency files exist otherwise close the application"""
    if not os.path.isdir('DataSearch') \
            or not os.path.exists('DataSearch\\DataSearch.ini') \
            or not os.path.exists('DataSearch\\products.db') \
            or not os.path.exists('DataSearch\\customers.db') \
            or not os.path.exists('DataSearch\\plants.db'):
        # notify user that an error has occurred
        ctypes.windll.user32.MessageBoxW(0,
                                         'DataSearch has encountered a fatal error. Source files were either moved or deleted and cannot be found. Please restart DataSearch to continue.',
                                         'Fatal Error Encountered', 0)
        # close DataSearch
        try:
            # check if data window exists and destroy
            if data.winfo_exists():
                data.destroy()
        except:
            pass
        return


def datasearch_import():
    """Initialize datasearch source data and create dependency files where necessary"""
    # check if DataSearch directory does not exist
    if not os.path.isdir('DataSearch'):
        # create DataSearch directory
        uniq_ds_dir_maker('DataSearch')
    # check if initializing configuration file exists
    if not os.path.exists('DataSearch\\DataSearch.ini'):
        # create initializing configuration file
        write_datasearch_ini()
    # check if database files exists already
    if not os.path.exists('DataSearch\\customers.db') \
            or not os.path.exists('DataSearch\\products.db') \
            or not os.path.exists('DataSearch\\plants.db'):
        # loop until a file is selected or the user exits
        while True:
            # file destination select dialogue
            Tk().withdraw()  # prevent root window
            # open file explorer folder select window
            dirspath = askopenfilename(title='Select a DataSearch - Data Source file',
                                       filetypes=[('Excel files', '.xlsx')])
            if not dirspath:
                ctypes.windll.user32.MessageBoxW(0, 'File select cancelled. Closing the application',
                                                 'Process Cancelled', 0)
                # close the application
                # sys.exit(0)
                # break while loop
                break
                # return
            try:
                # create dataframes
                df = pd.read_excel(dirspath, sheet_name='Product').fillna(0)
                cf = pd.read_excel(dirspath, sheet_name='Customer').fillna('-')
                lf = pd.read_excel(dirspath, sheet_name='Plant').fillna('-')
                # reindex dataframes
                df = df[['Plant', 'OSKU', 'MSKU', 'Segment', 'Description', 'Run_Freq',
                         'Production', 'Shelf_Life', 'Ship_Policy', 'BBL_Conversion', 'Units_p_Pallet']]
                cf = cf[['ShipTo', 'ShipTo_Name', 'CSA', 'State', 'State_Code', 'Region', 'City', 'Postal_Code']]
                lf = lf[['Plant', 'Plant_Name', 'Plant_Address', 'Plant_City', 'Plant_Type']]
                # end while loop
                break
            # exit script if no dataframes were created
            except UnboundLocalError:
                ctypes.windll.user32.MessageBoxW(0,
                                                 'The file was loaded incorrectly. Please try again.',
                                                 'File Load Error', 0)
                return False
            # if file is currently open
            except PermissionError:
                ctypes.windll.user32.MessageBoxW(0,
                                                 'The file you have selected is locked by the user. Please close the file and try again.',
                                                 'File Locked', 0)
                return False
            # if dataframe is wrong
            except (KeyError, ValueError, TypeError):
                ctypes.windll.user32.MessageBoxW(0,
                                                 'Selected file has missing or incomplete data. Please select a new file and try again.',
                                                 'Incompatible File', 0)
                return False
            # if the dataframe is wrong for any reason
            except:
                ctypes.windll.user32.MessageBoxW(0,
                                                 'The selected file has missing or incomplete data. Please select a new file and try again.',
                                                 'Incompatible File', 0)
                return False
        try:
            # format columns
            df['Plant'] = df['Plant'].astype(int)
            cf['Postal_Code'] = cf['Postal_Code'].astype(str)
            # identify lines that are all blanks
            cf['Remove'] = np.where((cf['ShipTo_Name'] == '-') & (cf['CSA'] == '-') & (cf['State'] == '-') &
                                    (cf['State_Code'] == '-') & (cf['Region'] == '-') & (cf['City'] == '-') &
                                    (cf['Postal_Code'] == '-'), 1, 0)
            # only keep rows that have nonzero values
            cf = cf[~(cf.Remove != 0)]
            # create database objects
            prod_conn1 = sqlite3.connect('DataSearch\\products.db')
            cust_conn1 = sqlite3.connect('DataSearch\\customers.db')
            plnt_conn1 = sqlite3.connect('DataSearch\\plants.db')
            # create a cursor instances
            pc1 = prod_conn1.cursor()
            cc1 = cust_conn1.cursor()
            lc1 = plnt_conn1.cursor()
            # create tables
            pc1.execute("""CREATE TABLE if not exists products (
                Plant INTEGER,
                OSKU INTEGER,
                MSKU INTEGER,
                Segment TEXT,
                Description TEXT,
                Run_Freq TEXT,
                Production TEXT,
                Shelf_Life INTEGER,
                Ship_Policy INTEGER,
                BBL_Conversion REAL,
                Units_p_Pallet INTEGER)
                """)
            cc1.execute("""CREATE TABLE if not exists customers (
                ShipTo INTEGER,
                ShipTo_Name TEXT,
                CSA TEXT,
                State TEXT,
                State_Code TEXT,
                Region TEXT,
                City TEXT,
                Postal_Code TEXT)
                """)
            lc1.execute("""CREATE TABLE if not exists plants (
                Plant INTEGER,
                Plant_Name TEXT,
                Plant_Address TEXT,
                Plant_City TEXT,
                Plant_Type TEXT)
                """)
            # import dataframe data to sql database
            df.to_sql('products', con=prod_conn1, if_exists='replace')
            cf.to_sql('customers', con=cust_conn1, if_exists='replace')
            lf.to_sql('plants', con=plnt_conn1, if_exists='replace')
            # commit changes to database
            prod_conn1.commit()
            cust_conn1.commit()
            plnt_conn1.commit()
            # close connections
            prod_conn1.close()
            cust_conn1.close()
            plnt_conn1.close()
            return True
        # if the dataframe is wrong for any reason
        except:
            ctypes.windll.user32.MessageBoxW(0,
                                             'The selected file has missing or incomplete data. Please select a new file and try again.',
                                             'Incompatible File', 0)
            return False
    # if source files already exist
    return True


def datasearch_upload():
    """Initialize datasearch source data and create dependency files where necessary"""
    # check if DataSearch directory does not exist
    if not os.path.isdir('DataSearch'):
        # create DataSearch directory
        uniq_ds_dir_maker('DataSearch')
    # check if initializing configuration file exists
    if os.path.exists('DataSearch\\DataSearch.ini'):
        # delete datasearch.ini
        os.remove('DataSearch\\DataSearch.ini')
        # create initializing configuration file
        write_datasearch_ini()
    # file destination select dialogue
    Tk().withdraw()  # prevent root window
    # open file explorer folder select window
    dirspath = askopenfilename(title='Select a DataSearch - Data Source file',
                               filetypes=[('Excel files', '.xlsx')])
    if not dirspath:
        ctypes.windll.user32.MessageBoxW(0, 'Update data file select cancelled.',
                                         'Update Cancelled', 0)
        return
    try:
        # create dataframes
        df = pd.read_excel(dirspath, sheet_name='Product').fillna(0)
        cf = pd.read_excel(dirspath, sheet_name='Customer').fillna('-')
        lf = pd.read_excel(dirspath, sheet_name='Plant').fillna('-')
        # reindex dataframes
        df = df[['Plant', 'OSKU', 'MSKU', 'Segment', 'Description', 'Run_Freq',
                 'Production', 'Shelf_Life', 'Ship_Policy', 'BBL_Conversion', 'Units_p_Pallet']]
        cf = cf[['ShipTo', 'ShipTo_Name', 'CSA', 'State', 'State_Code', 'Region', 'City', 'Postal_Code']]
        lf = lf[['Plant', 'Plant_Name', 'Plant_Address', 'Plant_City', 'Plant_Type']]
        # format columns
        df['Plant'] = df['Plant'].astype(int)
        cf['Postal_Code'] = cf['Postal_Code'].astype(str)
        # identify lines that are all blanks
        cf['Remove'] = np.where((cf['ShipTo_Name'] == '-') & (cf['CSA'] == '-') & (cf['State'] == '-') &
                                (cf['State_Code'] == '-') & (cf['Region'] == '-') & (cf['City'] == '-') &
                                (cf['Postal_Code'] == '-'), 1, 0)
        # only keep rows that have nonzero values
        cf = cf[~(cf.Remove != 0)]
        # create database objects
        prod_conn1 = sqlite3.connect('DataSearch\\products.db')
        cust_conn1 = sqlite3.connect('DataSearch\\customers.db')
        plnt_conn1 = sqlite3.connect('DataSearch\\plants.db')
        # create a cursor instances
        pc1 = prod_conn1.cursor()
        cc1 = cust_conn1.cursor()
        lc1 = plnt_conn1.cursor()
        # create tables
        pc1.execute("""CREATE TABLE if not exists products (
            Plant INTEGER,
            OSKU INTEGER,
            MSKU INTEGER,
            Segment TEXT,
            Description TEXT,
            Run_Freq TEXT,
            Production TEXT,
            Shelf_Life INTEGER,
            Ship_Policy INTEGER,
            BBL_Conversion REAL,
            Units_p_Pallet INTEGER)
            """)
        cc1.execute("""CREATE TABLE if not exists customers (
            ShipTo INTEGER,
            ShipTo_Name TEXT,
            CSA TEXT,
            State TEXT,
            State_Code TEXT,
            Region TEXT,
            City TEXT,
            Postal_Code TEXT)
            """)
        lc1.execute("""CREATE TABLE if not exists plants (
            Plant INTEGER,
            Plant_Name TEXT,
            Plant_Address TEXT,
            Plant_City TEXT,
            Plant_Type TEXT)
            """)
        # import dataframe data to sql database
        df.to_sql('products', con=prod_conn1, if_exists='replace')
        cf.to_sql('customers', con=cust_conn1, if_exists='replace')
        lf.to_sql('plants', con=plnt_conn1, if_exists='replace')
        # commit changes to database
        prod_conn1.commit()
        cust_conn1.commit()
        plnt_conn1.commit()
        # close connections
        prod_conn1.close()
        cust_conn1.close()
        plnt_conn1.close()
        # notify user that the source data has been updated successfully
        ctypes.windll.user32.MessageBoxW(0,
                                         'Your DataSearch data has been updated successfully.',
                                         'Data Updated', 0)
    # exit script if no dataframes were created
    except UnboundLocalError:
        ctypes.windll.user32.MessageBoxW(0,
                                         'The file was loaded incorrectly. Please try again.',
                                         'File Load Error', 0)
    # if file is currently open
    except PermissionError:
        ctypes.windll.user32.MessageBoxW(0,
                                         'The file you have selected is locked by the user. Please close the file and try again.',
                                         'File Locked', 0)
    # if dataframe is wrong
    except (KeyError, ValueError, TypeError):
        ctypes.windll.user32.MessageBoxW(0,
                                         'Selected file has missing or incomplete data. Please select a new file and try again.',
                                         'Incompatible File', 0)
    # if the dataframe is wrong for any reason
    except:
        ctypes.windll.user32.MessageBoxW(0,
                                         'The selected file has missing or incomplete data. Please select a new file and try again.',
                                         'Incompatible File', 0)
    return


def query_products():
    """Import product data from dataframe into database"""
    # check file dependencies
    datasearch_source_checker()
    # check if products_stack should be reset
    if p_search_stacking_intvar.get() == 0:
        # delete products_stack.db
        unstack_products()
    # clear the treeview
    for record in datatree1.get_children():
        datatree1.delete(record)
    # create database object
    prod_conn1 = sqlite3.connect('DataSearch\\products.db')
    # create a cursor instance
    pc1 = prod_conn1.cursor()
    # execute query
    pc1.execute("SELECT * FROM products")
    # get results
    prod_rec = pc1.fetchall()
    # add data to treeview
    for prod_count1, record in enumerate(prod_rec):
        # determine row count for tag formatting
        if prod_count1 % 2 == 0:
            # insert data from database skipping index column record[0]
            datatree1.insert(parent='', index='end', iid=prod_count1, text='', values=(record[1], record[2], record[3],
                                                                                       record[4], record[5], record[6],
                                                                                       record[7], record[8], record[9],
                                                                                       record[10], record[11]),
                             tags=('everow',))
        else:
            datatree1.insert(parent='', index='end', iid=prod_count1, text='', values=(record[1], record[2], record[3],
                                                                                       record[4], record[5], record[6],
                                                                                       record[7], record[8], record[9],
                                                                                       record[10], record[11]),
                             tags=('oddrow',))
    # commit changes to database
    prod_conn1.commit()
    # close connections
    prod_conn1.close()


def query_customers():
    """Import customer data from dataframe into database"""
    # check file dependencies
    datasearch_source_checker()
    # check if customers_stack should be reset
    if c_search_stacking_intvar.get() == 0:
        # delete customers_stack.db
        unstack_customers()
    # create database object
    cust_conn1 = sqlite3.connect('DataSearch\\customers.db')
    # create a cursor instance
    cc1 = cust_conn1.cursor()
    # execute query
    cc1.execute("SELECT * FROM customers")
    # get results
    cust_rec = cc1.fetchall()
    # clear the treeview
    for record in datatree2.get_children():
        datatree2.delete(record)
    # add data to treeview
    for cust_count1, record in enumerate(cust_rec):
        # determine row count for tag formatting
        if cust_count1 % 2 == 0:
            # insert data from database skipping index column record[0]
            datatree2.insert(parent='', index='end', iid=cust_count1, text='', values=(str(record[1]).zfill(6),
                                                                                       record[2], record[3],
                                                                                       record[4], record[5],
                                                                                       record[6], record[7],
                                                                                       str(record[8]).zfill(5)),
                             tags=('everow',))
        else:
            datatree2.insert(parent='', index='end', iid=cust_count1, text='', values=(str(record[1]).zfill(6),
                                                                                       record[2], record[3],
                                                                                       record[4], record[5],
                                                                                       record[6], record[7],
                                                                                       str(record[8]).zfill(5)),
                             tags=('oddrow',))
    # commit changes to database
    cust_conn1.commit()
    # close connections
    cust_conn1.close()


def query_plants():
    """Import plant data from dataframe into database"""
    # check file dependencies
    datasearch_source_checker()
    # check if products_stack should be reset
    if l_search_stacking_intvar.get() == 0:
        # delete products_stack.db
        unstack_plants()
    # clear the treeview
    for record in datatree3.get_children():
        datatree3.delete(record)
    # create database object
    plnt_conn1 = sqlite3.connect('DataSearch\\plants.db')
    # create a cursor instance
    lc1 = plnt_conn1.cursor()
    # execute query
    lc1.execute("SELECT * FROM plants")
    # get results
    plnt_rec = lc1.fetchall()
    # add data to treeview
    for plnt_count1, record in enumerate(plnt_rec):
        # determine row count for tag formatting
        if plnt_count1 % 2 == 0:
            # insert data from database skipping index column record[0]
            datatree3.insert(parent='', index='end', iid=plnt_count1, text='', values=(record[1], record[2], record[3],
                                                                                       record[4], record[5]),
                             tags=('everow',))
        else:
            datatree3.insert(parent='', index='end', iid=plnt_count1, text='', values=(record[1], record[2], record[3],
                                                                                       record[4], record[5]),
                             tags=('oddrow',))
    # commit changes to database
    plnt_conn1.commit()
    # close connections
    plnt_conn1.close()


def search_products_picker():
    """determine which function to use for searching the products database"""
    # check file dependencies
    datasearch_source_checker()
    # check if products search stacking is enabled
    if p_search_stacking_intvar.get() == 1:
        stack_search_products()
    else:
        search_products()


def search_customers_picker():
    """determine which function to use for searching the customers database"""
    # check file dependencies
    datasearch_source_checker()
    # check if customers search stacking is enabled
    if c_search_stacking_intvar.get() == 1:
        stack_search_customers()
    else:
        search_customers()


def search_plants_picker():
    """determine which function to use for searching the plants database"""
    # check file dependencies
    datasearch_source_checker()
    # check if products search stacking is enabled
    if l_search_stacking_intvar.get() == 1:
        stack_search_plants()
    else:
        search_plants()


def search_products():
    """Get results in products table based on entry box search"""
    # check file dependencies
    datasearch_source_checker()
    # get value in combobox
    prod_select = prod_search.get()
    # get value in entry box
    prod_entered = prod_entry.get()
    # clear the treeview
    for record in datatree1.get_children():
        datatree1.delete(record)
    # create database object
    prod_conn2 = sqlite3.connect('DataSearch\\products.db')
    # create a cursor instance
    pc2 = prod_conn2.cursor()
    # query with wildcards if search exact is disabled
    if p_search_exact_intvar.get() == 0:
        # execute query based on combobox for column selection
        if prod_select == 'Plant':
            pc2.execute("SELECT * FROM products WHERE Plant like ?", (f"%{prod_entered}%",))
        elif prod_select == 'OSKU':
            pc2.execute("SELECT * FROM products WHERE OSKU like ?", (f"%{prod_entered}%",))
        elif prod_select == 'MSKU':
            pc2.execute("SELECT * FROM products WHERE MSKU like ?", (f"%{prod_entered}%",))
        elif prod_select == 'Segment':
            pc2.execute("SELECT * FROM products WHERE Segment like ?", (f"%{prod_entered}%",))
        elif prod_select == 'Description':
            pc2.execute("SELECT * FROM products WHERE Description like ?", (f"%{prod_entered}%",))
        elif prod_select == 'Run Frequency':
            pc2.execute("SELECT * FROM products WHERE Run_Freq like ?", (f"%{prod_entered}%",))
        elif prod_select == 'Production':
            pc2.execute("SELECT * FROM products WHERE Production like ?", (f"%{prod_entered}%",))
        elif prod_select == 'Shelf Life':
            pc2.execute("SELECT * FROM products WHERE Shelf_Life like ?", (f"%{prod_entered}%",))
        elif prod_select == 'Ship Policy':
            pc2.execute("SELECT * FROM products WHERE Ship_Policy like ?", (f"%{prod_entered}%",))
        elif prod_select == 'BBL Conversion':
            pc2.execute("SELECT * FROM products WHERE BBL_Conversion like ?", (f"%{prod_entered}%",))
        elif prod_select == 'Units per Pallet':
            pc2.execute("SELECT * FROM products WHERE Units_p_Pallet like ?", (f"%{prod_entered}%",))
    # query without wildcards if search exact is enabled
    else:
        # execute query based on combobox for column selection
        if prod_select == 'Plant':
            pc2.execute("SELECT * FROM products WHERE Plant like ?", (f"{prod_entered}",))
        elif prod_select == 'OSKU':
            pc2.execute("SELECT * FROM products WHERE OSKU like ?", (f"{prod_entered}",))
        elif prod_select == 'MSKU':
            pc2.execute("SELECT * FROM products WHERE MSKU like ?", (f"{prod_entered}",))
        elif prod_select == 'Segment':
            pc2.execute("SELECT * FROM products WHERE Segment like ?", (f"{prod_entered}",))
        elif prod_select == 'Description':
            pc2.execute("SELECT * FROM products WHERE Description like ?", (f"{prod_entered}",))
        elif prod_select == 'Run Frequency':
            pc2.execute("SELECT * FROM products WHERE Run_Freq like ?", (f"{prod_entered}",))
        elif prod_select == 'Production':
            pc2.execute("SELECT * FROM products WHERE Production like ?", (f"{prod_entered}",))
        elif prod_select == 'Shelf Life':
            pc2.execute("SELECT * FROM products WHERE Shelf_Life like ?", (f"{prod_entered}",))
        elif prod_select == 'Ship Policy':
            pc2.execute("SELECT * FROM products WHERE Ship_Policy like ?", (f"{prod_entered}",))
        elif prod_select == 'BBL Conversion':
            pc2.execute("SELECT * FROM products WHERE BBL_Conversion like ?", (f"{prod_entered}",))
        elif prod_select == 'Units per Pallet':
            pc2.execute("SELECT * FROM products WHERE Units_p_Pallet like ?", (f"{prod_entered}",))
    # get results
    prod_rec = pc2.fetchall()
    # add data to treeview
    for prod_count2, record in enumerate(prod_rec):
        # determine row count for tag formatting
        if prod_count2 % 2 == 0:
            # insert data from database skipping index column record[0]
            datatree1.insert(parent='', index='end', iid=prod_count2, text='', values=(record[1], record[2], record[3],
                                                                                       record[4], record[5], record[6],
                                                                                       record[7], record[8], record[9],
                                                                                       record[10], record[11]),
                             tags=('everow',))
        else:
            datatree1.insert(parent='', index='end', iid=prod_count2, text='', values=(record[1], record[2], record[3],
                                                                                       record[4], record[5], record[6],
                                                                                       record[7], record[8], record[9],
                                                                                       record[10], record[11]),
                             tags=('oddrow',))
    # commit changes to database
    prod_conn2.commit()
    # close connections
    prod_conn2.close()


def search_customers():
    """get results in customers table based on entry box search"""
    # check file dependencies
    datasearch_source_checker()
    # get value in combobox
    cust_select = cust_search.get()
    # get value in entry box
    cust_entered = cust_entry.get()
    # clear the treeview
    for record in datatree2.get_children():
        datatree2.delete(record)
    # create database object
    cust_conn2 = sqlite3.connect('DataSearch\\customers.db')
    # create a cursor instance
    cc2 = cust_conn2.cursor()
    # query with wildcards if search exact is disabled
    if c_search_exact_intvar.get() == 0:
        # execute query based on combobox for column selection
        if cust_select == 'ShipTo':
            cc2.execute("SELECT * FROM customers WHERE ShipTo like ?", (f"%{cust_entered}%",))
        elif cust_select == 'ShipTo Name':
            cc2.execute("SELECT * FROM customers WHERE ShipTo_Name like ?", (f"%{cust_entered}%",))
        elif cust_select == 'CSA':
            cc2.execute("SELECT * FROM customers WHERE CSA like ?", (f"%{cust_entered}%",))
        elif cust_select == 'State':
            cc2.execute("SELECT * FROM customers WHERE State like ?", (f"%{cust_entered}%",))
        elif cust_select == 'State Code':
            cc2.execute("SELECT * FROM customers WHERE State_Code like ?", (f"%{cust_entered}%",))
        elif cust_select == 'Region':
            cc2.execute("SELECT * FROM customers WHERE Region like ?", (f"%{cust_entered}%",))
        elif cust_select == 'City':
            cc2.execute("SELECT * FROM customers WHERE City like ?", (f"%{cust_entered}%",))
        elif cust_select == 'Postal Code':
            cc2.execute("SELECT * FROM customers WHERE Postal_Code like ?", (f"%{cust_entered}%",))
    # query without wildcards if search exact is enabled
    else:
        # execute query based on combobox for column selection
        if cust_select == 'ShipTo':
            cc2.execute("SELECT * FROM customers WHERE ShipTo like ?", (f"{cust_entered}",))
        elif cust_select == 'ShipTo Name':
            cc2.execute("SELECT * FROM customers WHERE ShipTo_Name like ?", (f"{cust_entered}",))
        elif cust_select == 'CSA':
            cc2.execute("SELECT * FROM customers WHERE CSA like ?", (f"{cust_entered}",))
        elif cust_select == 'State':
            cc2.execute("SELECT * FROM customers WHERE State like ?", (f"{cust_entered}",))
        elif cust_select == 'State Code':
            cc2.execute("SELECT * FROM customers WHERE State_Code like ?", (f"{cust_entered}",))
        elif cust_select == 'Region':
            cc2.execute("SELECT * FROM customers WHERE Region like ?", (f"{cust_entered}",))
        elif cust_select == 'City':
            cc2.execute("SELECT * FROM customers WHERE City like ?", (f"{cust_entered}",))
        elif cust_select == 'Postal Code':
            cc2.execute("SELECT * FROM customers WHERE Postal_Code like ?", (f"{cust_entered}",))
    # get results
    cust_rec = cc2.fetchall()
    # add data to treeview
    for cust_count2, record in enumerate(cust_rec):
        # determine row count for tag formatting
        if cust_count2 % 2 == 0:
            # insert data from database skipping index column record[0]
            datatree2.insert(parent='', index='end', iid=cust_count2, text='', values=(str(record[1]).zfill(6),
                                                                                       record[2], record[3],
                                                                                       record[4], record[5],
                                                                                       record[6], record[7],
                                                                                       str(record[8]).zfill(5)),
                             tags=('everow',))
        else:
            datatree2.insert(parent='', index='end', iid=cust_count2, text='', values=(str(record[1]).zfill(6),
                                                                                       record[2], record[3],
                                                                                       record[4], record[5],
                                                                                       record[6], record[7],
                                                                                       str(record[8]).zfill(5)),
                             tags=('oddrow',))
    # commit changes to database
    cust_conn2.commit()
    # close connections
    cust_conn2.close()


def search_plants():
    """Get results in plants table based on entry box search"""
    # check file dependencies
    datasearch_source_checker()
    # get value in combobox
    plnt_select = plnt_search.get()
    # get value in entry box
    plnt_entered = plnt_entry.get()
    # clear the treeview
    for record in datatree3.get_children():
        datatree3.delete(record)
    # create database object
    plnt_conn2 = sqlite3.connect('DataSearch\\plants.db')
    # create a cursor instance
    pc3 = plnt_conn2.cursor()
    # query with wildcards if search exact is disabled
    if l_search_exact_intvar.get() == 0:
        # execute query based on combobox for column selection
        if plnt_select == 'Plant':
            pc3.execute("SELECT * FROM plants WHERE Plant like ?", (f"%{plnt_entered}%",))
        elif plnt_select == 'Plant Name':
            pc3.execute("SELECT * FROM plants WHERE Plant_Name like ?", (f"%{plnt_entered}%",))
        elif plnt_select == 'Plant Address':
            pc3.execute("SELECT * FROM plants WHERE Plant_Address like ?", (f"%{plnt_entered}%",))
        elif plnt_select == 'Plant City':
            pc3.execute("SELECT * FROM plants WHERE Plant_City like ?", (f"%{plnt_entered}%",))
        elif plnt_select == 'Plant Type':
            pc3.execute("SELECT * FROM plants WHERE Plant_Type like ?", (f"%{plnt_entered}%",))
    # query without wildcards if search exact is enabled
    else:
        # execute query based on combobox for column selection
        if plnt_select == 'Plant':
            pc3.execute("SELECT * FROM plants WHERE Plant like ?", (f"{plnt_entered}",))
        elif plnt_select == 'Plant Name':
            pc3.execute("SELECT * FROM plants WHERE Plant_Name like ?", (f"{plnt_entered}",))
        elif plnt_select == 'Plant Address':
            pc3.execute("SELECT * FROM plants WHERE Plant_Address like ?", (f"{plnt_entered}",))
        elif plnt_select == 'Plant City':
            pc3.execute("SELECT * FROM plants WHERE Plant_City like ?", (f"{plnt_entered}",))
        elif plnt_select == 'Plant Type':
            pc3.execute("SELECT * FROM plants WHERE Plant_Type like ?", (f"{plnt_entered}",))
    # get results
    plnt_rec = pc3.fetchall()
    # add data to treeview
    for plnt_count2, record in enumerate(plnt_rec):
        # determine row count for tag formatting
        if plnt_count2 % 2 == 0:
            # insert data from database skipping index column record[0]
            datatree3.insert(parent='', index='end', iid=plnt_count2, text='', values=(record[1], record[2], record[3],
                                                                                       record[4], record[5]),
                             tags=('everow',))
        else:
            datatree3.insert(parent='', index='end', iid=plnt_count2, text='', values=(record[1], record[2], record[3],
                                                                                       record[4], record[5]),
                             tags=('oddrow',))
    # commit changes to database
    plnt_conn2.commit()
    # close connections
    plnt_conn2.close()


def stack_search_products():
    """Get results in products table based on entry box search with stacking enabled"""
    # check file dependencies
    datasearch_source_checker()
    # create list to create dataframe
    products_res_list = []
    # get treeview values into list of lists
    for prod_record in datatree1.get_children():
        # append values to list
        products_res_list.append(datatree1.item(prod_record)['values'])
    # create dataframe with list of lists
    psf = pd.DataFrame(products_res_list, columns=['Plant', 'OSKU', 'MSKU', 'Segment',
                                                   'Description', 'Run_Freq', 'Production',
                                                   'Shelf_Life', 'Ship_Policy', 'BBL_Conversion',
                                                   'Units_p_Pallet'])
    # create database object
    prod_conn_s = sqlite3.connect('DataSearch\\products_stack.db')
    # create a cursor instance
    pcs = prod_conn_s.cursor()
    # create table
    pcs.execute("""CREATE TABLE if not exists products_stack (
            Plant INTEGER,
            OSKU INTEGER,
            MSKU INTEGER,
            Segment TEXT,
            Description TEXT,
            Run_Freq TEXT,
            Production TEXT,
            Shelf_Life INTEGER,
            Ship_Policy INTEGER,
            BBL_Conversion REAL,
            Units_p_Pallet INTEGER)
            """)
    # replace previous products_stack database with new one to continuing stacking searches
    psf.to_sql('products_stack', con=prod_conn_s, if_exists='replace')
    # get value in combobox
    prod_select = prod_search.get()
    # get value in entry box
    prod_entered = prod_entry.get()
    # clear the treeview
    for record in datatree1.get_children():
        datatree1.delete(record)
    # query with wildcards if search exact is disabled
    if p_search_exact_intvar.get() == 0:
        # execute query based on combobox for column selection
        if prod_select == 'Plant':
            pcs.execute("SELECT * FROM products_stack WHERE Plant like ?", (f"%{prod_entered}%",))
        elif prod_select == 'OSKU':
            pcs.execute("SELECT * FROM products_stack WHERE OSKU like ?", (f"%{prod_entered}%",))
        elif prod_select == 'MSKU':
            pcs.execute("SELECT * FROM products_stack WHERE MSKU like ?", (f"%{prod_entered}%",))
        elif prod_select == 'Segment':
            pcs.execute("SELECT * FROM products_stack WHERE Segment like ?", (f"%{prod_entered}%",))
        elif prod_select == 'Description':
            pcs.execute("SELECT * FROM products_stack WHERE Description like ?", (f"%{prod_entered}%",))
        elif prod_select == 'Run Frequency':
            pcs.execute("SELECT * FROM products_stack WHERE Run_Freq like ?", (f"%{prod_entered}%",))
        elif prod_select == 'Production':
            pcs.execute("SELECT * FROM products_stack WHERE Production like ?", (f"%{prod_entered}%",))
        elif prod_select == 'Shelf Life':
            pcs.execute("SELECT * FROM products_stack WHERE Shelf_Life like ?", (f"%{prod_entered}%",))
        elif prod_select == 'Ship Policy':
            pcs.execute("SELECT * FROM products_stack WHERE Ship_Policy like ?", (f"%{prod_entered}%",))
        elif prod_select == 'BBL Conversion':
            pcs.execute("SELECT * FROM products_stack WHERE BBL_Conversion like ?", (f"%{prod_entered}%",))
        elif prod_select == 'Units per Pallet':
            pcs.execute("SELECT * FROM products_stack WHERE Units_p_Pallet like ?", (f"%{prod_entered}%",))
    # query without wildcards if search exact is enabled
    else:
        # execute query based on combobox for column selection
        if prod_select == 'Plant':
            pcs.execute("SELECT * FROM products WHERE Plant like ?", (f"{prod_entered}",))
        elif prod_select == 'OSKU':
            pcs.execute("SELECT * FROM products WHERE OSKU like ?", (f"{prod_entered}",))
        elif prod_select == 'MSKU':
            pcs.execute("SELECT * FROM products WHERE MSKU like ?", (f"{prod_entered}",))
        elif prod_select == 'Segment':
            pcs.execute("SELECT * FROM products WHERE Segment like ?", (f"{prod_entered}",))
        elif prod_select == 'Description':
            pcs.execute("SELECT * FROM products WHERE Description like ?", (f"{prod_entered}",))
        elif prod_select == 'Run Frequency':
            pcs.execute("SELECT * FROM products WHERE Run_Freq like ?", (f"{prod_entered}",))
        elif prod_select == 'Production':
            pcs.execute("SELECT * FROM products WHERE Production like ?", (f"{prod_entered}",))
        elif prod_select == 'Shelf Life':
            pcs.execute("SELECT * FROM products WHERE Shelf_Life like ?", (f"{prod_entered}",))
        elif prod_select == 'Ship Policy':
            pcs.execute("SELECT * FROM products WHERE Ship_Policy like ?", (f"{prod_entered}",))
        elif prod_select == 'BBL Conversion':
            pcs.execute("SELECT * FROM products WHERE BBL_Conversion like ?", (f"{prod_entered}",))
        elif prod_select == 'Units per Pallet':
            pcs.execute("SELECT * FROM products WHERE Units_p_Pallet like ?", (f"{prod_entered}",))
    # get pcs
    prod_rec = pcs.fetchall()
    # add data to treeview
    for prod_count2, record in enumerate(prod_rec):
        # determine row count for tag formatting
        if prod_count2 % 2 == 0:
            # insert data from database skipping index column record[0]
            datatree1.insert(parent='', index='end', iid=prod_count2, text='', values=(record[1], record[2], record[3],
                                                                                       record[4], record[5], record[6],
                                                                                       record[7], record[8], record[9],
                                                                                       record[10], record[11]),
                             tags=('everow',))
        else:
            datatree1.insert(parent='', index='end', iid=prod_count2, text='', values=(record[1], record[2], record[3],
                                                                                       record[4], record[5], record[6],
                                                                                       record[7], record[8], record[9],
                                                                                       record[10], record[11]),
                             tags=('oddrow',))

    # commit changes to database
    prod_conn_s.commit()
    # close connection
    prod_conn_s.close()


def stack_search_customers():
    """Get results in customers table based on entry box search with stacking enabled"""
    # check file dependencies
    datasearch_source_checker()
    # create list to create dataframe
    customers_res_list = []
    # get treeview values into list of lists
    for cust_record in datatree2.get_children():
        # append values to list
        customers_res_list.append(datatree2.item(cust_record)['values'])
    # create dataframe with list of lists
    csf = pd.DataFrame(customers_res_list, columns=['ShipTo', 'ShipTo_Name', 'CSA', 'State',
                                                    'State_Code', 'Region', 'City', 'Postal_Code'])
    # create database object
    cust_conn_s = sqlite3.connect('DataSearch\\customers_stack.db')
    # create a cursor instance
    ccs = cust_conn_s.cursor()
    # create table
    ccs.execute("""CREATE TABLE if not exists customers_stack (
        ShipTo INTEGER,
        ShipTo_Name TEXT,
        CSA TEXT,
        State TEXT,
        State_Code TEXT,
        Region TEXT,
        City TEXT,
        Postal_Code TEXT)
        """)
    # import dataframe data to sql database
    csf.to_sql('customers_stack', con=cust_conn_s, if_exists='replace')
    # get value in combobox
    cust_select = cust_search.get()
    # get value in entry box
    cust_entered = cust_entry.get()
    # clear the treeview
    for record in datatree2.get_children():
        datatree2.delete(record)
    # query with wildcards if search exact is disabled
    if c_search_exact_intvar.get() == 0:
        # execute query based on combobox for column selection
        if cust_select == 'ShipTo':
            ccs.execute("SELECT * FROM customers_stack WHERE ShipTo like ?", (f"%{cust_entered}%",))
        elif cust_select == 'ShipTo Name':
            ccs.execute("SELECT * FROM customers_stack WHERE ShipTo_Name like ?", (f"%{cust_entered}%",))
        elif cust_select == 'CSA':
            ccs.execute("SELECT * FROM customers_stack WHERE CSA like ?", (f"%{cust_entered}%",))
        elif cust_select == 'State':
            ccs.execute("SELECT * FROM customers_stack WHERE State like ?", (f"%{cust_entered}%",))
        elif cust_select == 'State Code':
            ccs.execute("SELECT * FROM customers_stack WHERE State_Code like ?", (f"%{cust_entered}%",))
        elif cust_select == 'Region':
            ccs.execute("SELECT * FROM customers_stack WHERE Region like ?", (f"%{cust_entered}%",))
        elif cust_select == 'City':
            ccs.execute("SELECT * FROM customers_stack WHERE City like ?", (f"%{cust_entered}%",))
        elif cust_select == 'Postal Code':
            ccs.execute("SELECT * FROM customers_stack WHERE Postal_Code like ?", (f"%{cust_entered}%",))
    # query without wildcards if search exact is enabled
    else:
        # execute query based on combobox for column selection
        if cust_select == 'ShipTo':
            ccs.execute("SELECT * FROM customers_stack WHERE ShipTo like ?", (f"{cust_entered}",))
        elif cust_select == 'ShipTo Name':
            ccs.execute("SELECT * FROM customers_stack WHERE ShipTo_Name like ?", (f"{cust_entered}",))
        elif cust_select == 'CSA':
            ccs.execute("SELECT * FROM customers_stack WHERE CSA like ?", (f"{cust_entered}",))
        elif cust_select == 'State':
            ccs.execute("SELECT * FROM customers_stack WHERE State like ?", (f"{cust_entered}",))
        elif cust_select == 'State Code':
            ccs.execute("SELECT * FROM customers_stack WHERE State_Code like ?", (f"{cust_entered}",))
        elif cust_select == 'Region':
            ccs.execute("SELECT * FROM customers_stack WHERE Region like ?", (f"{cust_entered}",))
        elif cust_select == 'City':
            ccs.execute("SELECT * FROM customers_stack WHERE City like ?", (f"{cust_entered}",))
        elif cust_select == 'Postal Code':
            ccs.execute("SELECT * FROM customers_stack WHERE Postal_Code like ?", (f"{cust_entered}",))
    # get results
    cust_rec = ccs.fetchall()
    # add data to treeview
    for cust_count2, record in enumerate(cust_rec):
        # determine row count for tag formatting
        if cust_count2 % 2 == 0:
            # insert data from database skipping index column record[0]
            datatree2.insert(parent='', index='end', iid=cust_count2, text='', values=(str(record[1]).zfill(6),
                                                                                       record[2], record[3],
                                                                                       record[4], record[5],
                                                                                       record[6], record[7],
                                                                                       str(record[8]).zfill(5)),
                             tags=('everow',))
        else:
            datatree2.insert(parent='', index='end', iid=cust_count2, text='', values=(str(record[1]).zfill(6),
                                                                                       record[2], record[3],
                                                                                       record[4], record[5],
                                                                                       record[6], record[7],
                                                                                       str(record[8]).zfill(5)),
                             tags=('oddrow',))

    # commit changes to database
    cust_conn_s.commit()
    # close connection
    cust_conn_s.close()


def stack_search_plants():
    """Get results in plants table based on entry box search with stacking enabled"""
    # check file dependencies
    datasearch_source_checker()
    # create list to create dataframe
    plants_res_list = []
    # get treeview values into list of lists
    for plnt_record in datatree3.get_children():
        # append values to list
        plants_res_list.append(datatree3.item(plnt_record)['values'])
    # create dataframe with list of lists
    lsf = pd.DataFrame(plants_res_list, columns=['Plant', 'Plant_Name', 'Plant_Address', 'Plant_City', 'Plant_Type'])
    # create database object
    plnt_conn_s = sqlite3.connect('DataSearch\\plants_stack.db')
    # create a cursor instance
    lcs = plnt_conn_s.cursor()
    # create table
    lcs.execute("""CREATE TABLE if not exists plants (
            Plant INTEGER,
            Plant_Name TEXT,
            Plant_Address TEXT,
            Plant_City TEXT,
            Plant_Type TEXT)
            """)
    # import dataframe data to sql database
    lsf.to_sql('plants_stack', con=plnt_conn_s, if_exists='replace')
    # get value in combobox
    plnt_select = plnt_search.get()
    # get value in entry box
    plnt_entered = plnt_entry.get()
    # clear the treeview
    for record in datatree3.get_children():
        datatree3.delete(record)
    # query with wildcards if search exact is disabled
    if l_search_exact_intvar.get() == 0:
        # execute query based on combobox for column selection
        if plnt_select == 'Plant':
            lcs.execute("SELECT * FROM plants_stack WHERE Plant like ?", (f"%{plnt_entered}%",))
        elif plnt_select == 'Plant Name':
            lcs.execute("SELECT * FROM plants_stack WHERE Plant_Name like ?", (f"%{plnt_entered}%",))
        elif plnt_select == 'Plant Address':
            lcs.execute("SELECT * FROM plants_stack WHERE Plant_Address like ?", (f"%{plnt_entered}%",))
        elif plnt_select == 'Plant City':
            lcs.execute("SELECT * FROM plants_stack WHERE Plant_City like ?", (f"%{plnt_entered}%",))
        elif plnt_select == 'Plant Type':
            lcs.execute("SELECT * FROM plants_stack WHERE Plant_Type like ?", (f"%{plnt_entered}%",))
    # query without wildcards if search exact is enabled
    else:
        # execute query based on combobox for column selection
        if plnt_select == 'Plant':
            lcs.execute("SELECT * FROM plants_stack WHERE Plant like ?", (f"{plnt_entered}",))
        elif plnt_select == 'Plant Name':
            lcs.execute("SELECT * FROM plants_stack WHERE Plant_Name like ?", (f"{plnt_entered}",))
        elif plnt_select == 'Plant Address':
            lcs.execute("SELECT * FROM plants_stack WHERE Plant_Address like ?", (f"{plnt_entered}",))
        elif plnt_select == 'Plant City':
            lcs.execute("SELECT * FROM plants_stack WHERE Plant_City like ?", (f"{plnt_entered}",))
        elif plnt_select == 'Plant Type':
            lcs.execute("SELECT * FROM plants_stack WHERE Plant_Type like ?", (f"{plnt_entered}",))
    # get results
    plnt_rec = lcs.fetchall()
    # add data to treeview
    for plnt_count2, record in enumerate(plnt_rec):
        # determine row count for tag formatting
        if plnt_count2 % 2 == 0:
            # insert data from database skipping index column record[0]
            datatree3.insert(parent='', index='end', iid=plnt_count2, text='', values=(record[1], record[2], record[3],
                                                                                       record[4], record[5]),
                             tags=('everow',))
        else:
            datatree3.insert(parent='', index='end', iid=plnt_count2, text='', values=(record[1], record[2], record[3],
                                                                                       record[4], record[5]),
                             tags=('oddrow',))

    # commit changes to database
    plnt_conn_s.commit()
    # close connection
    plnt_conn_s.close()


def unstack_products():
    """Delete products_stack database to reset searching"""
    # check file dependencies
    datasearch_source_checker()
    # check if file exists
    if os.path.exists('DataSearch\\products_stack.db'):
        # delete file
        os.remove('DataSearch\\products_stack.db')


def unstack_customers():
    """Delete customers_stack database to reset searching"""
    # check file dependencies
    datasearch_source_checker()
    # check if file exists
    if os.path.exists('DataSearch\\customers_stack.db'):
        # delete file
        os.remove('DataSearch\\customers_stack.db')


def unstack_plants():
    """Delete plants_stack database to reset searching"""
    # check file dependencies
    datasearch_source_checker()
    # check if file exists
    if os.path.exists('DataSearch\\plants_stack.db'):
        # delete file
        os.remove('DataSearch\\plants_stack.db')


def export_products():
    """Export data from products treeview to excel"""
    # prevent root window
    Tk().withdraw()
    # file destination select dialogue
    dirspath = filedialog.askdirectory(title='Select the output file save destination')
    # get current timestamp
    full_timestamp = ds_get_timestamps()[9]
    # if folder select cancelled
    if not dirspath:
        ctypes.windll.user32.MessageBoxW(0, 'Folder select cancelled.', 'Process Cancelled', 0)
        return
    # create file name
    filename = f"Products Export - {full_timestamp}.xlsx"
    # create unique file in destination directory
    fname = uniq_ds_file_maker(f"{dirspath}{os.sep}{filename}")
    # create list to store treeview data
    products_export_list = []
    # get treeview into list
    for record in datatree1.get_children():
        products_export_list.append(datatree1.item(record)['values'])
    # create list of products table headers
    products_headers_list = ['Plant', 'OSKU', 'MSKU', 'Segment', 'Description', 'Run_Freq',
                             'Production', 'Shelf_Life', 'Ship_Policy', 'BBL_Conversion', 'Units_p_Pallet']
    # create dataframe with list of headers and data
    products_df = pd.DataFrame(products_export_list, columns=products_headers_list)
    # create a Pandas Excel writer using XlsxWriter as the engine
    filewriter = pd.ExcelWriter(fname, engine='xlsxwriter')
    # create workbook object
    products_wb = filewriter.book
    # convert the dataframe to an XlsxWriter Excel object
    products_df.to_excel(filewriter, sheet_name='Products', index=False)
    # create worksheet objects
    prods = filewriter.sheets['Products']
    # create table formatting for each worksheet
    ds_format_excel(products_df, 'Products', 'PROD', 'Table Style Medium 2', True, filewriter)
    # create formatting methods for workbook
    center_format = products_wb.add_format({'align': 'center'})
    left_format = products_wb.add_format({'align': 'left'})
    nodec_format = products_wb.add_format({'num_format': '#,##0', 'align': 'right'})
    bconv_format = products_wb.add_format({'num_format': '#,##0.00000', 'align': 'right'})
    # hide gridlines
    prods.hide_gridlines(2)
    # freeze first row
    prods.freeze_panes(1, 0)
    # Set the column width and format
    prods.set_column('A:A', 8.43, center_format)
    prods.set_column('B:B', 8.43, center_format)
    prods.set_column('C:C', 8.43, center_format)
    prods.set_column('D:D', 12.71, left_format)
    prods.set_column('E:E', 65.43, left_format)
    prods.set_column('F:F', 15, left_format)
    prods.set_column('G:G', 12.29, left_format)
    prods.set_column('H:H', 11.43, nodec_format)
    prods.set_column('I:I', 12.71, nodec_format)
    prods.set_column('J:J', 16.86, bconv_format)
    prods.set_column('K:K', 15.71, nodec_format)
    # correct header formatting for each worksheet
    prods.set_row(0, None, left_format)
    # Turn off some of the warnings:
    prods.ignore_errors({'number_stored_as_text': 'J:J'})
    # save and close workbook
    products_wb.close()
    # create popup messagebox
    ctypes.windll.user32.MessageBoxW(0, 'Product Export file created successfully.', 'File created', 0)
    return


def export_customers():
    """Export data from customers treeview to excel"""
    # prevent root window
    Tk().withdraw()
    # file destination select dialogue
    dirspath = filedialog.askdirectory(title='Select the output file save destination')
    # get current timestamp
    full_timestamp = ds_get_timestamps()[9]
    # if folder select cancelled
    if not dirspath:
        ctypes.windll.user32.MessageBoxW(0, 'Folder select cancelled.', 'Process Cancelled', 0)
        return
    # create file name
    filename = f"Customers Export - {full_timestamp}.xlsx"
    # create unique file in destination directory
    fname = uniq_ds_file_maker(f"{dirspath}{os.sep}{filename}")
    # create list to store treeview data
    customers_export_list = []
    # get treeview into list
    for record in datatree2.get_children():
        customers_export_list.append(datatree2.item(record)['values'])
    # create list of products table headers
    customers_headers_list = ['ShipTo', 'ShipTo_Name', 'CSA', 'State', 'State_Code',
                              'Region', 'City', 'Postal_Code']
    # create dataframe with list of headers and data
    customers_df = pd.DataFrame(customers_export_list, columns=customers_headers_list)
    # create a Pandas Excel writer using XlsxWriter as the engine
    filewriter = pd.ExcelWriter(fname, engine='xlsxwriter')
    # create workbook object
    customers_wb = filewriter.book
    # convert the dataframe to an XlsxWriter Excel object
    customers_df.to_excel(filewriter, sheet_name='Customers', index=False)
    # create worksheet objects
    custs = filewriter.sheets['Customers']
    # create table formatting for each worksheet
    ds_format_excel(customers_df, 'Customers', 'CUST', 'Table Style Medium 2', True, filewriter)
    # create formatting methods for workbook
    center_format = customers_wb.add_format({'align': 'center'})
    left_format = customers_wb.add_format({'align': 'left'})
    # hide gridlines
    custs.hide_gridlines(2)
    # freeze first row
    custs.freeze_panes(1, 0)
    # Set the column width and format
    custs.set_column('A:A', 8.57, center_format)
    custs.set_column('B:B', 39.57, left_format)
    custs.set_column('C:C', 30, left_format)
    custs.set_column('D:D', 18, left_format)
    custs.set_column('E:E', 12.71, center_format)
    custs.set_column('F:F', 25, left_format)
    custs.set_column('G:G', 25, left_format)
    custs.set_column('H:H', 13.57, left_format)
    # correct header formatting for each worksheet
    custs.set_row(0, None, left_format)
    # save and close workbook
    customers_wb.close()
    # create popup messagebox
    ctypes.windll.user32.MessageBoxW(0, 'Customer Export file created successfully.', 'File created', 0)
    return


def export_plants():
    """Export data from customers treeview to excel"""
    # prevent root window
    Tk().withdraw()
    # file destination select dialogue
    dirspath = filedialog.askdirectory(title='Select the output file save destination')
    # get current timestamp
    full_timestamp = ds_get_timestamps()[9]
    # if folder select cancelled
    if not dirspath:
        ctypes.windll.user32.MessageBoxW(0, 'Folder select cancelled.', 'Process Cancelled', 0)
        return
    # create file name
    filename = f"Plants Export - {full_timestamp}.xlsx"
    # create unique file in destination directory
    fname = uniq_ds_file_maker(f"{dirspath}{os.sep}{filename}")
    # create list to store treeview data
    plants_export_list = []
    # get treeview into list
    for record in datatree3.get_children():
        plants_export_list.append(datatree3.item(record)['values'])
    # create list of products table headers
    plants_headers_list = ['Plant', 'Plant_Name', 'Plant_Address', 'Plant_City', 'Plant_Type']
    # create dataframe with list of headers and data
    customers_df = pd.DataFrame(plants_export_list, columns=plants_headers_list)
    # create a Pandas Excel writer using XlsxWriter as the engine
    filewriter = pd.ExcelWriter(fname, engine='xlsxwriter')
    # create workbook object
    customers_wb = filewriter.book
    # convert the dataframe to an XlsxWriter Excel object
    customers_df.to_excel(filewriter, sheet_name='Plants', index=False)
    # create worksheet objects
    custs = filewriter.sheets['Plants']
    # create table formatting for each worksheet
    ds_format_excel(customers_df, 'Plants', 'PLNT', 'Table Style Medium 2', True, filewriter)
    # create formatting methods for workbook
    center_format = customers_wb.add_format({'align': 'center'})
    left_format = customers_wb.add_format({'align': 'left'})
    # hide gridlines
    custs.hide_gridlines(2)
    # freeze first row
    custs.freeze_panes(1, 0)
    # Set the column width and format
    custs.set_column('A:A', 10, center_format)
    custs.set_column('B:B', 35, left_format)
    custs.set_column('C:C', 35, left_format)
    custs.set_column('D:D', 20, left_format)
    custs.set_column('E:E', 20, left_format)
    # correct header formatting for each worksheet
    custs.set_row(0, None, left_format)
    # save and close workbook
    customers_wb.close()
    # create popup messagebox
    ctypes.windll.user32.MessageBoxW(0, 'Plant Export file created successfully.', 'File created', 0)
    return


def prod_copy_selected():
    """Copy selected fields in focus treeview row to clipboard"""
    # create lists to store treeview data
    products_copy_list1 = []
    products_copy_list2 = []
    # get treeview values into list of lists
    for prod_record in datatree1.selection():
        # append values to list
        products_copy_list1.append(datatree1.item(prod_record)['values'])
    # get only the products fields in list
    products_copy_list1 = products_copy_list1[:11]
    # flatten list of lists
    flat_list = [item for sublist in products_copy_list1 for item in sublist]
    # correct the flattened list that had been double
    flat_list = flat_list[:11]
    # get current settings
    curr_settings_list = get_datasearch_ini()
    # check and remove delimiter setting
    delim_option = curr_settings_list.pop()
    # create list based on field copy settings
    for count, item in enumerate(flat_list):
        # check if setting is selected
        if curr_settings_list[count] == '1':
            # construct string by adding selected fields separated by chosen delimiter
            products_copy_list2.append(str(item))
    # join items in list by delimiter
    if delim_option == 'Space':
        prod_constructed_str = ' '.join(products_copy_list2)
    elif delim_option == 'Comma':
        prod_constructed_str = ', '.join(products_copy_list2)
    elif delim_option == 'Tab':
        prod_constructed_str = '    '.join(products_copy_list2)
    elif delim_option == 'Semicolon':
        prod_constructed_str = '; '.join(products_copy_list2)
    elif delim_option == 'Colon':
        prod_constructed_str = ': '.join(products_copy_list2)
    elif delim_option == 'Vertical Slash':
        prod_constructed_str = ' | '.join(products_copy_list2)
    elif delim_option == 'Underscore':
        prod_constructed_str = '_'.join(products_copy_list2)
    elif delim_option == 'Double Underscore':
        prod_constructed_str = '__'.join(products_copy_list2)
    elif delim_option == 'Hyphen':
        prod_constructed_str = '-'.join(products_copy_list2)
    elif delim_option == 'Dash':
        prod_constructed_str = '--'.join(products_copy_list2)
    # copy data to clipboard
    pc.copy(prod_constructed_str)
    return prod_constructed_str


def cust_copy_selected():
    """Copy selected fields in focus treeview row to clipboard"""
    # create lists to store treeview data
    customers_copy_list1 = []
    customers_copy_list2 = []
    # get treeview values into list of lists
    for cust_record in datatree2.selection():
        # append values to list
        customers_copy_list1.append(datatree2.item(cust_record)['values'])
    c_flat_list = [item for sublist in customers_copy_list1 for item in sublist]
    # correct the flattened list that had been double
    c_flat_list = c_flat_list[:8]
    # get current settings
    curr_settings_list = get_datasearch_ini()
    # check and remove delimiter setting
    delim_option = curr_settings_list.pop()
    # slice list to get options for customers table
    curr_settings_list = curr_settings_list[13:21]
    # create list based on field copy settings
    for count, item in enumerate(c_flat_list):
        # check if setting is selected
        if curr_settings_list[count] == '1':
            # construct string by adding selected fields separated by chosen delimiter
            customers_copy_list2.append(str(item))
    # join items in list by delimiter
    if delim_option == 'Space':
        cust_constructed_str = ' '.join(customers_copy_list2)
    elif delim_option == 'Comma':
        cust_constructed_str = ', '.join(customers_copy_list2)
    elif delim_option == 'Tab':
        cust_constructed_str = '    '.join(customers_copy_list2)
    elif delim_option == 'Semicolon':
        cust_constructed_str = '; '.join(customers_copy_list2)
    elif delim_option == 'Colon':
        cust_constructed_str = ': '.join(customers_copy_list2)
    elif delim_option == 'Vertical Slash':
        cust_constructed_str = ' | '.join(customers_copy_list2)
    elif delim_option == 'Underscore':
        cust_constructed_str = '_'.join(customers_copy_list2)
    elif delim_option == 'Double Underscore':
        cust_constructed_str = '__'.join(customers_copy_list2)
    elif delim_option == 'Hyphen':
        cust_constructed_str = '-'.join(customers_copy_list2)
    elif delim_option == 'Dash':
        cust_constructed_str = '--'.join(customers_copy_list2)
    # copy data to clipboard
    pc.copy(cust_constructed_str)
    return cust_constructed_str


def plnt_copy_selected():
    """Copy selected fields in focus treeview row to clipboard"""
    # create lists to store treeview data
    plants_copy_list1 = []
    plants_copy_list2 = []
    # get treeview values into list of lists
    for plnt_record in datatree3.selection():
        # append values to list
        plants_copy_list1.append(datatree3.item(plnt_record)['values'])
    # get only the plants fields in list
    plants_copy_list1 = plants_copy_list1[:6]
    # flatten list of lists
    flat_list = [item for sublist in plants_copy_list1 for item in sublist]
    # correct the flattened list that had been double
    flat_list = flat_list[:6]
    # get current settings
    curr_settings_list = get_datasearch_ini()
    # check and remove delimiter setting
    delim_option = curr_settings_list.pop()
    # slice list to get options for customers table
    curr_settings_list = curr_settings_list[23:28]
    # create list based on field copy settings
    for count, item in enumerate(flat_list):
        # check if setting is selected
        if curr_settings_list[count] == '1':
            # construct string by adding selected fields separated by chosen delimiter
            plants_copy_list2.append(str(item))
    # join items in list by delimiter
    if delim_option == 'Space':
        plnt_constructed_str = ' '.join(plants_copy_list2)
    elif delim_option == 'Comma':
        plnt_constructed_str = ', '.join(plants_copy_list2)
    elif delim_option == 'Tab':
        plnt_constructed_str = '    '.join(plants_copy_list2)
    elif delim_option == 'Semicolon':
        plnt_constructed_str = '; '.join(plants_copy_list2)
    elif delim_option == 'Colon':
        plnt_constructed_str = ': '.join(plants_copy_list2)
    elif delim_option == 'Vertical Slash':
        plnt_constructed_str = ' | '.join(plants_copy_list2)
    elif delim_option == 'Underscore':
        plnt_constructed_str = '_'.join(plants_copy_list2)
    elif delim_option == 'Double Underscore':
        plnt_constructed_str = '__'.join(plants_copy_list2)
    elif delim_option == 'Hyphen':
        plnt_constructed_str = '-'.join(plants_copy_list2)
    elif delim_option == 'Dash':
        plnt_constructed_str = '--'.join(plants_copy_list2)
    # copy data to clipboard
    pc.copy(plnt_constructed_str)
    return plnt_constructed_str


def cycle_checker():
    """Update date values on cycle tab"""
    # check file dependencies
    datasearch_source_checker()
    # get entry box values
    cycle_date = cycl_entry_var1.get()
    cycle_plnt = cycl_entry_var2.get()
    cycle_msku = cycl_entry_var3.get()
    # get convert date to datetime
    try:
        # convert string formatted date to datetime object
        dateobj = datetime.datetime.strptime(cycle_date, '%m/%d/%Y')
    except ValueError:
        try:
            # convert string formatted date to datetime object
            dateobj = datetime.datetime.strptime(cycle_date, '%m-%d-%Y')
        except ValueError:
            # create popup messagebox if neither correct format options are used
            ctypes.windll.user32.MessageBoxW(0, 'Please use either mm/dd/yyy or mm-dd-yyyy date formatting.',
                                             'Incorrect Date Format', 0)
            return
    # update entry boxes
    cycl_entry_var4.set(str(ds_get_past_timestamps(dateobj)[2]))  # week number
    cycl_entry_var6.set(str(ds_get_past_timestamps(dateobj)[9]))  # week day
    # calculate cycle week number
    week_mod = ds_get_past_timestamps(dateobj)[2] % 4
    if week_mod == 0:
        week_mod = 4
    # update entry box
    cycl_entry_var5.set(str(week_mod))  # cycle week number
    # create database object
    cycle_conn1 = sqlite3.connect('DataSearch\\products.db')
    # create a cursor instance
    yc1 = cycle_conn1.cursor()
    # execute query for ship policy
    yc1.execute(
        "SELECT Ship_Policy, Shelf_Life, OSKU, Segment, Description, Run_Freq, Production, BBL_Conversion, Units_p_Pallet FROM products WHERE Plant like ? And MSKU like ?",
        (f"{cycle_plnt}", f"{cycle_msku}"))
    # get results from query
    try:
        # get first result from query - convert list of one tuple to single tuple
        ship_pol_wks, expr_pol_wks, osku_v, segment_v, desc_v, run_freq_v, prod_v, bbl_conv_v, units_p_pallet_v = \
            yc1.fetchall()[0]
        # calculate production date + ship policy weeks
        ship_policy_week = dateobj + datetime.timedelta(days=ship_pol_wks * 7)
        expr_policy_week = dateobj + datetime.timedelta(days=expr_pol_wks * 7)
        # roll back to previous monday
        ship_policy_date = ship_policy_week - datetime.timedelta(days=ship_policy_week.weekday())
        expr_policy_date = expr_policy_week - datetime.timedelta(days=expr_policy_week.weekday())
        # format date
        ship_policy_date = ship_policy_date.strftime('%m-%d-%Y')
        expr_policy_date = expr_policy_date.strftime('%m-%d-%Y')
    except:
        # create popup messagebox if no results are found
        ctypes.windll.user32.MessageBoxW(0, 'No results found. Please adjust your search and try again', 'No Results',
                                         0)
        # blank out entry boxes
        cycl_entry_var4.set('')  # production weekday
        cycl_entry_var5.set('')  # cycle week
        cycl_entry_var6.set('')  # week number
        cycl_entry_var7.set('')  # ship policy weeks
        cycl_entry_var8.set('')  # ship policy date
        cycl_entry_var9.set('')  # expiration date
        cycl_entry_var10.set('')  # OSKU
        cycl_entry_var11.set('')  # segment
        cycl_entry_var12.set('')  # description
        cycl_entry_var13.set('')  # run frequency
        cycl_entry_var14.set('')  # production
        cycl_entry_var15.set('')  # BBL conversion
        cycl_entry_var16.set('')  # units per pallet
        return
    # update entry boxes
    cycl_entry_var7.set(str(ship_pol_wks))  # ship policy weeks
    cycl_entry_var8.set(str(ship_policy_date))  # ship policy date
    cycl_entry_var9.set(str(expr_policy_date))  # expiration date
    cycl_entry_var10.set(str(osku_v))  # OSKU
    cycl_entry_var11.set(str(segment_v))  # segment
    cycl_entry_var12.set(str(desc_v))  # description
    cycl_entry_var13.set(str(run_freq_v))  # run frequency
    cycl_entry_var14.set(str(prod_v))  # production
    cycl_entry_var15.set(str(bbl_conv_v))  # BBL conversion
    cycl_entry_var16.set(str(units_p_pallet_v))  # units per pallet
    # close database connection
    cycle_conn1.close()
    return


def reset_checker():
    """Reset all fields on the cycle tab"""
    cycl_entry_var1.set('')  # plant
    cycl_entry_var2.set('')  # msku
    cycl_entry_var3.set('')  # date
    cycl_entry_var4.set('')  # production weekday
    cycl_entry_var5.set('')  # cycle week
    cycl_entry_var6.set('')  # week number
    cycl_entry_var7.set('')  # ship policy weeks
    cycl_entry_var8.set('')  # ship policy date
    cycl_entry_var9.set('')  # expiration date
    cycl_entry_var10.set('')  # OSKU
    cycl_entry_var11.set('')  # segment
    cycl_entry_var12.set('')  # description
    cycl_entry_var13.set('')  # run frequency
    cycl_entry_var14.set('')  # production
    cycl_entry_var15.set('')  # BBL conversion
    cycl_entry_var16.set('')  # units per pallet
    return


def datasearch_gui():
    """create DataSearch gui"""
    # create global variables
    global data
    global datatree1, datatree2, datatree3, prod_entry, prod_search, cust_entry, cust_search, plnt_entry, plnt_search, \
        cycl_entry1, cycl_entry2, cycl_entry3, cycl_entry4, cycl_entry_var1, cycl_entry_var2, cycl_entry_var3, \
        cycl_entry_var4, cycl_entry_var5, cycl_entry_var6, plant_intvar, \
        osku_intvar, msku_intvar, segment_intvar, description_intvar, run_freq_intvar, production_intvar, \
        shelf_life_intvar, ship_policy_intvar, bbl_conversion_intvar, units_p_pallet_intvar, p_search_stacking_intvar, \
        p_search_exact_intvar, shipto_intvar, shipto_name_intvar, csa_intvar, state_intvar, state_code_intvar, \
        region_intvar, city_intvar, postal_code_intvar, c_search_stacking_intvar, c_search_exact_intvar, \
        settings_list, delimiter_var, lplant_intvar, plant_name_intvar, plant_address_intvar, plant_city_intvar, \
        plant_type_intvar, l_search_stacking_intvar, l_search_exact_intvar, cycl_entry_var7, cycl_entry_var8, \
        cycl_entry_var9, cycl_entry_var10, cycl_entry_var11, cycl_entry_var12, cycl_entry_var13, cycl_entry_var14, \
        cycl_entry_var14, cycl_entry_var15, cycl_entry_var16
    # set search widget x values
    frame_x = 15
    column1_x = 30
    button1_x = 370
    frame_lbl_x = 290
    settings_x1 = 60
    # set search widget y values
    widget_y = 506
    button_y = 485
    label_y = 472
    # cycle_y1 = 75
    frame_y = 465
    frame_lbl_y = 450
    settings_y1 = 75
    # set standard button sizes
    button_w = 12
    button_h = 2
    entry_w = 30
    cycle_entry_w = 20
    frame_h = 75
    frame_w = 875
    # calculate widget offsets
    settings_y2 = settings_y1 + 25
    settings_y3 = settings_y2 + 25
    settings_y4 = settings_y3 + 25
    settings_y5 = settings_y4 + 25
    settings_y6 = settings_y5 + 25
    settings_y7 = settings_y6 + 25
    settings_y8 = settings_y7 + 25
    settings_y9 = settings_y8 + 25
    settings_y10 = settings_y9 + 25
    settings_y11 = settings_y10 + 25
    delim_y = settings_y11 + 2
    settings_x2 = settings_x1 + 190
    settings_x3 = settings_x2 + 190
    column2_x = column1_x + 150
    column3_x = column2_x + 150
    column4_x = column3_x + 150
    # column5_x = column4_x + 150
    button2_x = button1_x + 100
    button3_x = button2_x + 100
    button4_x = button3_x + 100
    button5_x = button4_x + 100
    delim_x = settings_x1 + 180
    # set widget font
    elbel_font = ('Tuno', 11)
    widget_font = ('Tuno', 13)
    label_font = ('Tuno', 15, 'italic')
    # create tkinter gui
    data = tk.Tk()
    # set as active window
    data.focus_force()
    # set gui title
    data.title(datasearch_apptitle)
    # make gui resizable
    data.resizable(True, True)
    # set gui size
    data.geometry('1070x600')
    # create application icon
    # mc_icon = resource_path('C:\\Users\\JSVAR\\PycharmProjects\\Language\\mc_icon.ico')
    mc_icon = resource_path('mc_icon.ico')
    data.iconbitmap(mc_icon)
    # create tab control
    tabcontrol = ttk.Notebook(data)
    # create tab frame widgets
    prod_tab = ttk.Frame(tabcontrol)
    cust_tab = ttk.Frame(tabcontrol)
    plnt_tab = ttk.Frame(tabcontrol)
    cycl_tab = ttk.Frame(tabcontrol)
    sett_tab = ttk.Frame(tabcontrol)
    # create tab widgets
    tabcontrol.add(prod_tab, text='Products')
    tabcontrol.add(cust_tab, text='Customers')
    tabcontrol.add(plnt_tab, text='Plants')
    tabcontrol.add(cycl_tab, text='Cycles')
    tabcontrol.add(sett_tab, text='Settings')
    tabcontrol.pack(expand=1, fill='both')

    """create products tab"""
    # create treeview frame
    viewframe1 = Frame(prod_tab)
    viewframe1.pack(padx=15, pady=15, expand=True, anchor='nw')
    viewframe1.columnconfigure(0, weight=1)
    viewframe1.rowconfigure(1, weight=1)
    # create scrollbars
    datayscroll1 = Scrollbar(viewframe1, orient='vertical')
    datayscroll1.pack(side=RIGHT, fill=Y)
    # apply formatting to treeview
    style = ttk.Style()
    # choose style
    style.theme_use('default')
    # define style
    style.configure('Treeview', background='#FFFFFF',
                    foreground='#000000', rowheight=20, fieldbackground='#FFFFFF')
    # apply style
    style.map('Treeview', background=[('selected', '#FFB000')])
    # create treeview
    datatree1 = ttk.Treeview(viewframe1, show='headings', height=20,
                             selectmode='extended', yscrollcommand=datayscroll1.set)
    # create list to define column widths
    colwids1 = [50, 60, 60, 90, 340, 70, 75, 60, 70, 65, 75]
    # define treeview columns
    datatree1['columns'] = ('Plant', 'OSKU', 'MSKU', 'Segment', 'Description', 'Run_Freq',
                            'Production', 'Shelf_Life', 'Ship_Policy', 'BBL_Conversion', 'Units_p_Pallet')
    # create columns
    datatree1.column('#0', width=0, minwidth=0, stretch=NO)
    datatree1.column('Plant', width=colwids1[0], minwidth=5, anchor=CENTER)
    datatree1.column('OSKU', width=colwids1[1], minwidth=5, anchor=CENTER)
    datatree1.column('MSKU', width=colwids1[2], minwidth=5, anchor=CENTER)
    datatree1.column('Segment', width=colwids1[3], minwidth=5, anchor=W)
    datatree1.column('Description', width=colwids1[4], minwidth=5, anchor=W)
    datatree1.column('Run_Freq', width=colwids1[5], minwidth=5, anchor=W)
    datatree1.column('Production', width=colwids1[6], minwidth=5, anchor=W)
    datatree1.column('Shelf_Life', width=colwids1[7], minwidth=5, anchor=E)
    datatree1.column('Ship_Policy', width=colwids1[8], minwidth=5, anchor=E)
    datatree1.column('BBL_Conversion', width=colwids1[9], minwidth=5, anchor=E)
    datatree1.column('Units_p_Pallet', width=colwids1[10], minwidth=5, anchor=E)
    # create treeview headings
    datatree1.heading('#0', text='', anchor=W)
    datatree1.heading('Plant', text='Plant', anchor=W)
    datatree1.heading('OSKU', text='OSKU', anchor=W)
    datatree1.heading('MSKU', text='MSKU', anchor=W)
    datatree1.heading('Segment', text='Segment', anchor=W)
    datatree1.heading('Description', text='Description', anchor=W)
    datatree1.heading('Run_Freq', text='Run Freq', anchor=W)
    datatree1.heading('Production', text='Production', anchor=W)
    datatree1.heading('Shelf_Life', text='Shelf Life', anchor=W)
    datatree1.heading('Ship_Policy', text='Ship Policy', anchor=W)
    datatree1.heading('BBL_Conversion', text='BBL Conv', anchor=W)
    datatree1.heading('Units_p_Pallet', text='Units / Pallet', anchor=W)
    # place treeview
    datatree1.pack(expand=True)
    # configure scrollbars
    datayscroll1.config(command=datatree1.yview)
    # add sizegrip widget
    grip1 = ttk.Sizegrip(data)
    grip1.pack(side='right', anchor=SE)
    # create striped row tags
    datatree1.tag_configure('oddrow', background='#FFFFFF', foreground='#000000')
    datatree1.tag_configure('everow', background='#D9E1F2', foreground='#FFFFFF')
    # add frame
    psearch_frame = Frame(prod_tab, bd=0, height=frame_h, width=frame_w, highlightbackground='#091F3F',
                          highlightcolor='#1496FF', highlightthickness=1, relief=FLAT)
    psearch_frame.place(x=frame_x, y=frame_y)
    # create buttons
    search_button1 = tk.Button(prod_tab, text='Search', height=button_h, width=button_w, command=search_products_picker)
    search_button1.place(x=button1_x, y=button_y)
    reset_button1 = tk.Button(prod_tab, text='Reset', height=button_h, width=button_w, command=query_products)
    reset_button1.place(x=button2_x, y=button_y)
    export_button1 = tk.Button(prod_tab, text='Export', height=button_h, width=button_w, command=export_products)
    export_button1.place(x=button3_x, y=button_y)
    copy_button1 = tk.Button(prod_tab, text='Copy', height=button_h, width=button_w, command=prod_copy_selected)
    copy_button1.place(x=button4_x, y=button_y)
    # create integer variables and set values according to ini file
    p_search_stacking_intvar = IntVar(master=prod_tab, value=settings_list[11])
    p_search_exact_intvar = IntVar(master=prod_tab, value=settings_list[12])
    # create checkboxes
    p_search_stacking_chkbox = Checkbutton(prod_tab, text='Search Stacking', variable=p_search_stacking_intvar,
                                           onvalue=1, offvalue=0, command=quiet_update_datasearch_ini)
    p_search_exact_chkbox = Checkbutton(prod_tab, text='Search Exact', variable=p_search_exact_intvar,
                                        onvalue=1, offvalue=0, command=quiet_update_datasearch_ini)
    # place checkbuttons
    p_search_stacking_chkbox.place(x=button5_x, y=button_y - 4)
    p_search_exact_chkbox.place(x=button5_x, y=button_y + 21)
    # create variable for dropdown menu
    prod_search_var = StringVar(prod_tab)
    # set default value for dropdown menu
    prod_search_var.set('MSKU')
    # create list for options menu
    prod_options = ['Plant', 'OSKU', 'MSKU', 'Segment', 'Description', 'Run Frequency',
                    'Production', 'Shelf Life', 'Ship Policy', 'BBL Conversion', 'Units per Pallet']
    # create dropdown menu widget
    prod_search = ttk.Combobox(prod_tab, textvariable=prod_search_var, values=prod_options, height=25)
    prod_search.place(x=column1_x, y=widget_y)
    # create variable for dropdown menu
    prod_entry_var = StringVar(prod_tab)
    # set default value for entry box
    prod_entry_var.set('Enter search term')
    # create entry box
    prod_entry = Entry(prod_tab, textvariable=prod_entry_var, width=entry_w, selectborderwidth=4)
    prod_entry.place(x=column2_x, y=widget_y)
    # create labels
    prod_lbl1 = Label(prod_tab, text='Column Select', justify=LEFT, fg='#091F3F', font=widget_font)
    prod_lbl1.place(x=column1_x, y=label_y)
    prod_lbl2 = Label(prod_tab, text='Search', justify=LEFT, fg='#091F3F', font=widget_font)
    prod_lbl2.place(x=column2_x, y=label_y)
    prod_lbl3 = Label(prod_tab, text='PRODUCTS', justify=LEFT, fg='#091F3F', font=label_font)
    prod_lbl3.place(x=frame_lbl_x, y=frame_lbl_y)

    """create customer tab"""
    # create treeview frame
    viewframe2 = Frame(cust_tab)
    viewframe2.pack(padx=15, pady=15, expand=True, anchor='nw')
    viewframe2.columnconfigure(0, weight=1)
    viewframe2.rowconfigure(1, weight=1)
    # create scrollbars
    datayscroll2 = Scrollbar(viewframe2, orient='vertical')
    datayscroll2.pack(side=RIGHT, fill=Y)
    # create treeview
    datatree2 = ttk.Treeview(viewframe2, show='headings', height=20,
                             selectmode='extended', yscrollcommand=datayscroll2.set)
    # define treeview columns
    datatree2['columns'] = ('ShipTo', 'ShipTo_Name', 'CSA', 'State', 'State_Code',
                            'Region', 'City', 'Postal_Code')
    # create columns
    datatree2.column('ShipTo', width=70, minwidth=5, anchor=CENTER)
    datatree2.column('ShipTo_Name', width=275, minwidth=5, anchor=W)
    datatree2.column('CSA', width=180, minwidth=5, anchor=W)
    datatree2.column('State', width=85, minwidth=5, anchor=W)
    datatree2.column('State_Code', width=70, minwidth=5, anchor=CENTER)
    datatree2.column('Region', width=115, minwidth=5, anchor=W)
    datatree2.column('City', width=125, minwidth=5, anchor=W)
    datatree2.column('Postal_Code', width=97, minwidth=5, anchor=W)
    # create treeview headings
    datatree2.heading('ShipTo', text='ShipTo', anchor=W)
    datatree2.heading('ShipTo_Name', text='ShipTo Name', anchor=W)
    datatree2.heading('CSA', text='CSA', anchor=W)
    datatree2.heading('State', text='State', anchor=W)
    datatree2.heading('State_Code', text='State Code', anchor=W)
    datatree2.heading('Region', text='Region', anchor=W)
    datatree2.heading('City', text='City', anchor=W)
    datatree2.heading('Postal_Code', text='Postal Code', anchor=W)
    # place treeview
    datatree2.pack(expand=True)
    # configure scrollbars
    datayscroll2.config(command=datatree2.yview)
    # create striped row tags
    datatree2.tag_configure('oddrow', background='#FFFFFF', foreground='#000000')
    datatree2.tag_configure('everow', background='#D9E1F2', foreground='#FFFFFF')
    # add frame
    csearch_frame = Frame(cust_tab, bd=0, height=frame_h, width=frame_w, highlightbackground='#091F3F',
                          highlightcolor='#1496FF', highlightthickness=1, relief=FLAT)
    csearch_frame.place(x=frame_x, y=frame_y)
    # create buttons
    search_button2 = tk.Button(cust_tab, text='Search', height=button_h, width=button_w,
                               command=search_customers_picker)
    search_button2.place(x=button1_x, y=button_y)
    reset_button2 = tk.Button(cust_tab, text='Reset', height=button_h, width=button_w, command=query_customers)
    reset_button2.place(x=button2_x, y=button_y)
    export_button2 = tk.Button(cust_tab, text='Export', height=button_h, width=button_w, command=export_customers)
    export_button2.place(x=button3_x, y=button_y)
    copy_button2 = tk.Button(cust_tab, text='Copy', height=button_h, width=button_w, command=cust_copy_selected)
    copy_button2.place(x=button4_x, y=button_y)
    # create integer variables and set values according to ini file
    c_search_stacking_intvar = IntVar(master=cust_tab, value=settings_list[21])
    c_search_exact_intvar = IntVar(master=cust_tab, value=settings_list[22])
    # create checkboxes
    c_search_stacking_chkbox = Checkbutton(cust_tab, text='Search Stacking', variable=c_search_stacking_intvar,
                                           onvalue=1, offvalue=0, command=quiet_update_datasearch_ini)
    c_search_exact_chkbox = Checkbutton(cust_tab, text='Search Exact', variable=c_search_exact_intvar,
                                        onvalue=1, offvalue=0, command=quiet_update_datasearch_ini)
    # place checkbuttons
    c_search_stacking_chkbox.place(x=button5_x, y=button_y - 4)
    c_search_exact_chkbox.place(x=button5_x, y=button_y + 21)
    # create variable for dropdown menu
    cust_search_var = StringVar(cust_tab)
    # set default value for dropdown menu
    cust_search_var.set('ShipTo')
    # create list for options menu
    cust_options = ['ShipTo', 'ShipTo Name', 'CSA', 'State', 'State Code', 'Region', 'City', 'Postal Code']
    # create dropdown menu widget
    cust_search = ttk.Combobox(cust_tab, textvariable=cust_search_var, values=cust_options, height=25)
    cust_search.place(x=column1_x, y=widget_y)
    # create variable
    cust_entry_var = StringVar(cust_tab)
    # set default value for entry box
    cust_entry_var.set('Enter search term')
    # create entry box
    cust_entry = Entry(cust_tab, textvariable=cust_entry_var, width=entry_w)
    cust_entry.place(x=column2_x, y=widget_y)
    # create labels
    cust_lbl1 = Label(cust_tab, text='Column Select', justify=LEFT, fg='#091F3F', font=widget_font)
    cust_lbl1.place(x=column1_x, y=label_y)
    cust_lbl2 = Label(cust_tab, text='Search', justify=LEFT, fg='#091F3F', font=widget_font)
    cust_lbl2.place(x=column2_x, y=label_y)
    cust_lbl3 = Label(cust_tab, text='CUSTOMERS', justify=LEFT, fg='#091F3F', font=label_font)
    cust_lbl3.place(x=frame_lbl_x, y=frame_lbl_y)

    """create plants tab"""
    # create treeview frame
    viewframe3 = Frame(plnt_tab)
    viewframe3.pack(padx=15, pady=15, expand=True, anchor='nw')
    viewframe3.columnconfigure(0, weight=1)
    viewframe3.rowconfigure(1, weight=1)
    # create scrollbars
    datayscroll3 = Scrollbar(viewframe3, orient='vertical')
    datayscroll3.pack(side=RIGHT, fill=Y)
    # apply formatting to treeview
    style = ttk.Style()
    # define style
    style.configure('Treeview', background='#FFFFFF',
                    foreground='#000000', rowheight=20, fieldbackground='#FFFFFF')
    # apply style
    style.map('Treeview', background=[('selected', '#FFB000')])
    # create treeview
    datatree3 = ttk.Treeview(viewframe3, show='headings', height=20,
                             selectmode='extended', yscrollcommand=datayscroll3.set)
    # create list to define column widths
    colwids3 = [80, 260, 230, 150, 153]
    # define treeview columns
    datatree3['columns'] = ('Plant', 'Plant_Name', 'Plant_Address', 'Plant_City', 'Plant_Type')
    # create columns
    datatree3.column('#0', width=0, minwidth=0, stretch=NO)
    datatree3.column('Plant', width=colwids3[0], minwidth=5, anchor=CENTER)
    datatree3.column('Plant_Name', width=colwids3[1], minwidth=5, anchor=W)
    datatree3.column('Plant_Address', width=colwids3[2], minwidth=5, anchor=W)
    datatree3.column('Plant_City', width=colwids3[3], minwidth=5, anchor=W)
    datatree3.column('Plant_Type', width=colwids3[4], minwidth=5, anchor=W)
    # create treeview headings
    datatree3.heading('#0', text='', anchor=W)
    datatree3.heading('Plant', text='Plant', anchor=W)
    datatree3.heading('Plant_Name', text='Plant Name', anchor=W)
    datatree3.heading('Plant_Address', text='Plant Address', anchor=W)
    datatree3.heading('Plant_City', text='Plant City', anchor=W)
    datatree3.heading('Plant_Type', text='Plant Type', anchor=W)
    # place treeview
    datatree3.pack(expand=True)
    # configure scrollbars
    datayscroll3.config(command=datatree3.yview)
    # add sizegrip widget
    # grip3 = ttk.Sizegrip(data)
    # grip3.pack(side='right', anchor=SE)
    # create striped row tags
    datatree3.tag_configure('oddrow', background='#FFFFFF', foreground='#000000')
    datatree3.tag_configure('everow', background='#D9E1F2', foreground='#FFFFFF')
    # add frame
    lsearch_frame = Frame(plnt_tab, bd=0, height=frame_h, width=frame_w, highlightbackground='#091F3F',
                          highlightcolor='#1496FF', highlightthickness=1, relief=FLAT)
    lsearch_frame.place(x=frame_x, y=frame_y)
    # create buttons
    search_button3 = tk.Button(plnt_tab, text='Search', height=button_h, width=button_w, command=search_plants_picker)
    search_button3.place(x=button1_x, y=button_y)
    reset_button3 = tk.Button(plnt_tab, text='Reset', height=button_h, width=button_w, command=query_plants)
    reset_button3.place(x=button2_x, y=button_y)
    export_button3 = tk.Button(plnt_tab, text='Export', height=button_h, width=button_w, command=export_plants)
    export_button3.place(x=button3_x, y=button_y)
    copy_button3 = tk.Button(plnt_tab, text='Copy', height=button_h, width=button_w, command=plnt_copy_selected)
    copy_button3.place(x=button4_x, y=button_y)
    # create integer variables and set values according to ini file
    l_search_stacking_intvar = IntVar(master=plnt_tab, value=settings_list[28])
    l_search_exact_intvar = IntVar(master=plnt_tab, value=settings_list[29])
    # create checkboxes
    l_search_stacking_chkbox = Checkbutton(plnt_tab, text='Search Stacking', variable=l_search_stacking_intvar,
                                           onvalue=1, offvalue=0, command=quiet_update_datasearch_ini)
    l_search_exact_chkbox = Checkbutton(plnt_tab, text='Search Exact', variable=l_search_exact_intvar,
                                        onvalue=1, offvalue=0, command=quiet_update_datasearch_ini)
    # place checkbuttons
    l_search_stacking_chkbox.place(x=button5_x, y=button_y - 4)
    l_search_exact_chkbox.place(x=button5_x, y=button_y + 21)
    # create variable for dropdown menu
    plnt_search_var = StringVar(plnt_tab)
    # set default value for dropdown menu
    plnt_search_var.set('Plant')
    # create list for options menu
    plnt_options = ['Plant', 'Plant Name', 'Plant Address', 'Plant City', 'Plant Type']
    # create dropdown menu widget
    plnt_search = ttk.Combobox(plnt_tab, textvariable=plnt_search_var, values=plnt_options, height=25)
    plnt_search.place(x=column1_x, y=widget_y)
    # create variable for dropdown menu
    plnt_entry_var = StringVar(plnt_tab)
    # set default value for entry box
    plnt_entry_var.set('Enter search term')
    # create entry box
    plnt_entry = Entry(plnt_tab, textvariable=plnt_entry_var, width=entry_w, selectborderwidth=4)
    plnt_entry.place(x=column2_x, y=widget_y)
    # create labels
    plnt_lbl1 = Label(plnt_tab, text='Column Select', justify=LEFT, fg='#091F3F', font=widget_font)
    plnt_lbl1.place(x=column1_x, y=label_y)
    plnt_lbl2 = Label(plnt_tab, text='Search', justify=LEFT, fg='#091F3F', font=widget_font)
    plnt_lbl2.place(x=column2_x, y=label_y)
    plnt_lbl3 = Label(plnt_tab, text='PLANTS', justify=LEFT, fg='#091F3F', font=label_font)
    plnt_lbl3.place(x=frame_lbl_x, y=frame_lbl_y)

    """cycle tab"""
    # create frame
    cycl_frame1 = Frame(cycl_tab, bd=0, height=120, width=290, highlightbackground='#091F3F',
                        highlightcolor='#1496FF', highlightthickness=1, relief=FLAT)
    cycl_frame1.place(x=column1_x, y=settings_y1 - 15)
    cycl_frame2 = Frame(cycl_tab, bd=0, height=225, width=295, highlightbackground='#091F3F',
                        highlightcolor='#1496FF', highlightthickness=1, relief=FLAT)
    cycl_frame2.place(x=column3_x + 10, y=settings_y1 - 15)
    cycl_frame3 = Frame(cycl_tab, bd=0, height=65, width=290, highlightbackground='#091F3F',
                        highlightcolor='#1496FF', highlightthickness=1, relief=FLAT)
    cycl_frame3.place(x=column1_x, y=settings_y1 + 145)
    cycl_frame4 = Frame(cycl_tab, bd=0, height=140, width=605, highlightbackground='#091F3F',
                        highlightcolor='#1496FF', highlightthickness=1, relief=FLAT)
    cycl_frame4.place(x=column1_x, y=305)

    # create labels
    cycl_frame_lbl1 = Label(cycl_tab, text='Production Search', justify=LEFT, fg='#091F3F', font=label_font)
    cycl_frame_lbl1.place(x=column1_x + 50, y=settings_y1 - 33)
    cycl_frame_lbl2 = Label(cycl_tab, text='Production Results', justify=LEFT, fg='#091F3F', font=label_font)
    cycl_frame_lbl2.place(x=column3_x + 60, y=settings_y1 - 33)
    cycl_frame_lbl3 = Label(cycl_tab, text='Product Data', justify=LEFT, fg='#091F3F', font=label_font)
    cycl_frame_lbl3.place(x=column3_x + 60, y=288)
    cycl_frame_lbl4 = Label(cycl_tab, text='Search', justify=LEFT, fg='#091F3F', font=label_font)
    cycl_frame_lbl4.place(x=column1_x + 50, y=settings_y1 + 128)

    # create buttons
    cycl_button1 = tk.Button(cycl_tab, text='Submit', height=button_h, width=button_w + 4, command=cycle_checker)
    cycl_button1.place(x=column2_x, y=settings_y1 + 160)
    cycl_button2 = tk.Button(cycl_tab, text='Reset', height=button_h, width=button_w + 4, command=reset_checker)
    cycl_button2.place(x=column1_x + 15, y=settings_y1 + 160)

    # create variable
    cycl_entry_var1 = StringVar(cycl_tab)
    cycl_entry_var2 = StringVar(cycl_tab)
    cycl_entry_var3 = StringVar(cycl_tab)
    cycl_entry_var4 = StringVar(cycl_tab)
    cycl_entry_var5 = StringVar(cycl_tab)
    cycl_entry_var6 = StringVar(cycl_tab)
    cycl_entry_var7 = StringVar(cycl_tab)
    cycl_entry_var8 = StringVar(cycl_tab)
    cycl_entry_var9 = StringVar(cycl_tab)
    cycl_entry_var10 = StringVar(cycl_tab)
    cycl_entry_var11 = StringVar(cycl_tab)
    cycl_entry_var12 = StringVar(cycl_tab)
    cycl_entry_var13 = StringVar(cycl_tab)
    cycl_entry_var14 = StringVar(cycl_tab)
    cycl_entry_var15 = StringVar(cycl_tab)
    cycl_entry_var16 = StringVar(cycl_tab)
    # set default value for entry box
    cycl_entry_var1.set('Enter production date')
    cycl_entry_var2.set('Enter plant number')
    cycl_entry_var3.set('Enter MSKU number')
    cycl_entry_var4.set('')
    cycl_entry_var5.set('')
    cycl_entry_var6.set('')
    cycl_entry_var7.set('')
    cycl_entry_var8.set('')
    cycl_entry_var9.set('')
    cycl_entry_var10.set('')
    cycl_entry_var11.set('')
    cycl_entry_var12.set('')
    cycl_entry_var13.set('')
    cycl_entry_var14.set('')
    cycl_entry_var15.set('')
    cycl_entry_var16.set('')
    # create entry boxes
    cycl_entry1 = Entry(cycl_tab, textvariable=cycl_entry_var1, width=cycle_entry_w)
    cycl_entry1.place(x=column2_x - 5, y=settings_y1)
    cycl_entry2 = Entry(cycl_tab, textvariable=cycl_entry_var2, width=cycle_entry_w)
    cycl_entry2.place(x=column2_x - 5, y=settings_y2 + 10)
    cycl_entry3 = Entry(cycl_tab, textvariable=cycl_entry_var3, width=cycle_entry_w)
    cycl_entry3.place(x=column2_x - 5, y=settings_y3 + 20)

    cycl_entry4 = Entry(cycl_tab, textvariable=cycl_entry_var6, width=cycle_entry_w - 3)
    cycl_entry4.place(x=column4_x + 30, y=settings_y1)
    cycl_entry5 = Entry(cycl_tab, textvariable=cycl_entry_var5, width=cycle_entry_w - 3)
    cycl_entry5.place(x=column4_x + 30, y=settings_y2 + 10)
    cycl_entry6 = Entry(cycl_tab, textvariable=cycl_entry_var4, width=cycle_entry_w - 3)
    cycl_entry6.place(x=column4_x + 30, y=settings_y3 + 20)

    cycl_entry7 = Entry(cycl_tab, textvariable=cycl_entry_var7, width=cycle_entry_w - 3)
    cycl_entry7.place(x=column4_x + 30, y=settings_y4 + 30)
    cycl_entry8 = Entry(cycl_tab, textvariable=cycl_entry_var8, width=cycle_entry_w - 3)
    cycl_entry8.place(x=column4_x + 30, y=settings_y5 + 40)
    cycl_entry9 = Entry(cycl_tab, textvariable=cycl_entry_var9, width=cycle_entry_w - 3)
    cycl_entry9.place(x=column4_x + 30, y=settings_y6 + 50)

    # create entry boxes for product data
    cycl_entry10 = Entry(cycl_tab, textvariable=cycl_entry_var10, width=cycle_entry_w - 6)
    cycl_entry10.place(x=column1_x + 25, y=350)
    cycl_entry11 = Entry(cycl_tab, textvariable=cycl_entry_var11, width=cycle_entry_w - 2)
    cycl_entry11.place(x=column2_x - 18, y=350)
    cycl_entry12 = Entry(cycl_tab, textvariable=cycl_entry_var12, width=cycle_entry_w + 33)
    cycl_entry12.place(x=column3_x - 37, y=350)

    cycl_entry13 = Entry(cycl_tab, textvariable=cycl_entry_var13, width=cycle_entry_w)
    cycl_entry13.place(x=column1_x + 25, y=410)
    cycl_entry14 = Entry(cycl_tab, textvariable=cycl_entry_var14, width=cycle_entry_w)
    cycl_entry14.place(x=column2_x + 20, y=410)
    cycl_entry15 = Entry(cycl_tab, textvariable=cycl_entry_var15, width=cycle_entry_w)
    cycl_entry15.place(x=column3_x + 15, y=410)
    cycl_entry16 = Entry(cycl_tab, textvariable=cycl_entry_var16, width=cycle_entry_w)
    cycl_entry16.place(x=column4_x + 10, y=410)

    # create labels for entry boxes
    cycl_lbl1 = Label(cycl_tab, text='Production Date', justify=LEFT, fg='#091F3F', font=elbel_font)
    cycl_lbl1.place(x=column1_x + 15, y=settings_y1 - 5)
    cycl_lbl2 = Label(cycl_tab, text='Plant Number', justify=LEFT, fg='#091F3F', font=elbel_font)
    cycl_lbl2.place(x=column1_x + 15, y=settings_y2 + 5)
    cycl_lbl3 = Label(cycl_tab, text='MSKU Number', justify=LEFT, fg='#091F3F', font=elbel_font)
    cycl_lbl3.place(x=column1_x + 15, y=settings_y3 + 15)

    cycl_lbl4 = Label(cycl_tab, text='Production Weekday', justify=LEFT, fg='#091F3F', font=elbel_font)
    cycl_lbl4.place(x=column3_x + 20, y=settings_y1 - 5)
    cycl_lbl5 = Label(cycl_tab, text='Cycle Week', justify=LEFT, fg='#091F3F', font=elbel_font)
    cycl_lbl5.place(x=column3_x + 20, y=settings_y2 + 5)
    cycl_lbl6 = Label(cycl_tab, text='Production Week', justify=LEFT, fg='#091F3F', font=elbel_font)
    cycl_lbl6.place(x=column3_x + 20, y=settings_y3 + 15)
    cycl_lbl7 = Label(cycl_tab, text='Ship Policy (Weeks)', justify=LEFT, fg='#091F3F', font=elbel_font)
    cycl_lbl7.place(x=column3_x + 20, y=settings_y4 + 25)
    cycl_lbl8 = Label(cycl_tab, text='Ship Policy Date', justify=LEFT, fg='#091F3F', font=elbel_font)
    cycl_lbl8.place(x=column3_x + 20, y=settings_y5 + 35)
    cycl_lbl8 = Label(cycl_tab, text='Expiration Date', justify=LEFT, fg='#091F3F', font=elbel_font)
    cycl_lbl8.place(x=column3_x + 20, y=settings_y6 + 45)

    cycl_lbl10 = Label(cycl_tab, text='OSKU', justify=LEFT, fg='#091F3F', font=elbel_font)
    cycl_lbl10.place(x=column1_x + 25, y=320)
    cycl_lbl11 = Label(cycl_tab, text='Segment', justify=LEFT, fg='#091F3F', font=elbel_font)
    cycl_lbl11.place(x=column2_x - 18, y=320)
    cycl_lbl12 = Label(cycl_tab, text='Description', justify=LEFT, fg='#091F3F', font=elbel_font)
    cycl_lbl12.place(x=column3_x - 37, y=320)
    cycl_lbl13 = Label(cycl_tab, text='Run Freq', justify=LEFT, fg='#091F3F', font=elbel_font)
    cycl_lbl13.place(x=column1_x + 25, y=380)
    cycl_lbl14 = Label(cycl_tab, text='Production', justify=LEFT, fg='#091F3F', font=elbel_font)
    cycl_lbl14.place(x=column2_x + 20, y=380)
    cycl_lbl15 = Label(cycl_tab, text='BBL Conversion', justify=LEFT, fg='#091F3F', font=elbel_font)
    cycl_lbl15.place(x=column3_x + 15, y=380)
    cycl_lbl16 = Label(cycl_tab, text='Units / Pallet', justify=LEFT, fg='#091F3F', font=elbel_font)
    cycl_lbl16.place(x=column4_x + 10, y=380)

    """settings tab"""
    # add frames
    setting_frame1 = Frame(sett_tab, bd=0, height=310, width=175, highlightbackground='#091F3F',
                           highlightcolor='#1496FF', highlightthickness=1, relief=FLAT)
    setting_frame1.place(x=settings_x1 - 25, y=settings_y1 - 20)
    setting_frame2 = Frame(sett_tab, bd=0, height=310, width=175, highlightbackground='#091F3F',
                           highlightcolor='#1496FF', highlightthickness=1, relief=FLAT)
    setting_frame2.place(x=settings_x2 - 25, y=settings_y1 - 20)
    setting_frame3 = Frame(sett_tab, bd=0, height=60, width=175, highlightbackground='#091F3F',
                           highlightcolor='#1496FF', highlightthickness=1, relief=FLAT)
    setting_frame3.place(x=settings_x2 - 25, y=settings_y1 + 305)
    setting_frame4 = Frame(sett_tab, bd=0, height=60, width=175, highlightbackground='#091F3F',
                           highlightcolor='#1496FF', highlightthickness=1, relief=FLAT)
    setting_frame4.place(x=settings_x1 - 25, y=settings_y1 + 305)
    setting_frame5 = Frame(sett_tab, bd=0, height=60, width=175, highlightbackground='#091F3F',
                           highlightcolor='#1496FF', highlightthickness=1, relief=FLAT)
    setting_frame5.place(x=settings_x3 - 25, y=settings_y2 + 280)
    setting_frame6 = Frame(sett_tab, bd=0, height=310, width=175, highlightbackground='#091F3F',
                           highlightcolor='#1496FF', highlightthickness=1, relief=FLAT)
    setting_frame6.place(x=settings_x3 - 25, y=settings_y1 - 20)

    # create labels
    settings_lbl1 = Label(sett_tab, text='Products', justify=LEFT, fg='#091F3F', font=label_font)
    settings_lbl1.place(x=settings_x1, y=settings_y1 - 36)
    settings_lbl2 = Label(sett_tab, text='Customers', justify=LEFT, fg='#091F3F', font=label_font)
    settings_lbl2.place(x=settings_x2, y=settings_y1 - 36)
    settings_lbl3 = Label(sett_tab, text='Delimiters', justify=LEFT, fg='#091F3F', font=label_font)
    settings_lbl3.place(x=settings_x2, y=settings_y11 + 40)
    settings_lbl4 = Label(sett_tab, text='Plants', justify=LEFT, fg='#091F3F', font=label_font)
    settings_lbl4.place(x=settings_x3, y=settings_y1 - 36)

    # create integer variables and set values according to ini file
    plant_intvar = IntVar(master=sett_tab, value=settings_list[0])
    osku_intvar = IntVar(master=sett_tab, value=settings_list[1])
    msku_intvar = IntVar(master=sett_tab, value=settings_list[2])
    segment_intvar = IntVar(master=sett_tab, value=settings_list[3])
    description_intvar = IntVar(master=sett_tab, value=settings_list[4])
    run_freq_intvar = IntVar(master=sett_tab, value=settings_list[5])
    production_intvar = IntVar(master=sett_tab, value=settings_list[6])
    shelf_life_intvar = IntVar(master=sett_tab, value=settings_list[7])
    ship_policy_intvar = IntVar(master=sett_tab, value=settings_list[8])
    bbl_conversion_intvar = IntVar(master=sett_tab, value=settings_list[9])
    units_p_pallet_intvar = IntVar(master=sett_tab, value=settings_list[10])
    shipto_intvar = IntVar(master=sett_tab, value=settings_list[13])
    shipto_name_intvar = IntVar(master=sett_tab, value=settings_list[14])
    csa_intvar = IntVar(master=sett_tab, value=settings_list[15])
    state_intvar = IntVar(master=sett_tab, value=settings_list[16])
    state_code_intvar = IntVar(master=sett_tab, value=settings_list[17])
    region_intvar = IntVar(master=sett_tab, value=settings_list[18])
    city_intvar = IntVar(master=sett_tab, value=settings_list[19])
    postal_code_intvar = IntVar(master=sett_tab, value=settings_list[20])
    lplant_intvar = IntVar(master=sett_tab, value=settings_list[23])
    plant_name_intvar = IntVar(master=sett_tab, value=settings_list[24])
    plant_address_intvar = IntVar(master=sett_tab, value=settings_list[25])
    plant_city_intvar = IntVar(master=sett_tab, value=settings_list[26])
    plant_type_intvar = IntVar(master=sett_tab, value=settings_list[27])

    # create checkboxes
    plant_chkbox = Checkbutton(sett_tab, text='Plant', variable=plant_intvar, onvalue=1, offvalue=0)
    osku_chkbox = Checkbutton(sett_tab, text='OSKU', variable=osku_intvar, onvalue=1, offvalue=0)
    msku_chkbox = Checkbutton(sett_tab, text='MSKU', variable=msku_intvar, onvalue=1, offvalue=0)
    segment_chkbox = Checkbutton(sett_tab, text='Segment', variable=segment_intvar, onvalue=1, offvalue=0)
    description_chkbox = Checkbutton(sett_tab, text='Description', variable=description_intvar, onvalue=1, offvalue=0)
    run_freq_chkbox = Checkbutton(sett_tab, text='Run Frequency', variable=run_freq_intvar, onvalue=1, offvalue=0)
    production_chkbox = Checkbutton(sett_tab, text='Production', variable=production_intvar, onvalue=1, offvalue=0)
    shelf_life_chkbox = Checkbutton(sett_tab, text='Shelf Life', variable=shelf_life_intvar, onvalue=1, offvalue=0)
    ship_policy_chkbox = Checkbutton(sett_tab, text='Ship Policy', variable=ship_policy_intvar, onvalue=1, offvalue=0)
    bbl_conversion_chkbox = Checkbutton(sett_tab, text='BBL Conversion', variable=bbl_conversion_intvar, onvalue=1,
                                        offvalue=0)
    units_p_pallet_chkbox = Checkbutton(sett_tab, text='Units / Pallet', variable=units_p_pallet_intvar, onvalue=1,
                                        offvalue=0)
    shipto_chkbox = Checkbutton(sett_tab, text='ShipTo', variable=shipto_intvar, onvalue=1, offvalue=0)
    shipto_name_chkbox = Checkbutton(sett_tab, text='ShipTo Name', variable=shipto_name_intvar, onvalue=1, offvalue=0)
    csa_chkbox = Checkbutton(sett_tab, text='CSA', variable=csa_intvar, onvalue=1, offvalue=0)
    state_chkbox = Checkbutton(sett_tab, text='State', variable=state_intvar, onvalue=1, offvalue=0)
    state_code_chkbox = Checkbutton(sett_tab, text='State Code', variable=state_code_intvar, onvalue=1, offvalue=0)
    region_chkbox = Checkbutton(sett_tab, text='Region', variable=region_intvar, onvalue=1, offvalue=0)
    city_chkbox = Checkbutton(sett_tab, text='City', variable=city_intvar, onvalue=1, offvalue=0)
    postal_code_chkbox = Checkbutton(sett_tab, text='Postal Code', variable=postal_code_intvar, onvalue=1, offvalue=0)
    lplant_chkbox = Checkbutton(sett_tab, text='Plant', variable=lplant_intvar, onvalue=1, offvalue=0)
    plant_name = Checkbutton(sett_tab, text='Plant Name', variable=plant_name_intvar, onvalue=1, offvalue=0)
    plant_address = Checkbutton(sett_tab, text='Plant Address', variable=plant_address_intvar, onvalue=1, offvalue=0)
    plant_city = Checkbutton(sett_tab, text='Plant City', variable=plant_city_intvar, onvalue=1, offvalue=0)
    plant_type = Checkbutton(sett_tab, text='Plant Type', variable=plant_type_intvar, onvalue=1, offvalue=0)

    # place checkbuttons
    plant_chkbox.place(x=settings_x1, y=settings_y1)
    osku_chkbox.place(x=settings_x1, y=settings_y2)
    msku_chkbox.place(x=settings_x1, y=settings_y3)
    segment_chkbox.place(x=settings_x1, y=settings_y4)
    description_chkbox.place(x=settings_x1, y=settings_y5)
    run_freq_chkbox.place(x=settings_x1, y=settings_y6)
    production_chkbox.place(x=settings_x1, y=settings_y7)
    shelf_life_chkbox.place(x=settings_x1, y=settings_y8)
    ship_policy_chkbox.place(x=settings_x1, y=settings_y9)
    bbl_conversion_chkbox.place(x=settings_x1, y=settings_y10)
    units_p_pallet_chkbox.place(x=settings_x1, y=settings_y11)
    shipto_chkbox.place(x=settings_x2, y=settings_y1)
    shipto_name_chkbox.place(x=settings_x2, y=settings_y2)
    csa_chkbox.place(x=settings_x2, y=settings_y3)
    state_chkbox.place(x=settings_x2, y=settings_y4)
    state_code_chkbox.place(x=settings_x2, y=settings_y5)
    region_chkbox.place(x=settings_x2, y=settings_y6)
    city_chkbox.place(x=settings_x2, y=settings_y7)
    postal_code_chkbox.place(x=settings_x2, y=settings_y8)
    lplant_chkbox.place(x=settings_x3, y=settings_y1)
    plant_name.place(x=settings_x3, y=settings_y2)
    plant_address.place(x=settings_x3, y=settings_y3)
    plant_city.place(x=settings_x3, y=settings_y4)
    plant_type.place(x=settings_x3, y=settings_y5)

    # create variable for dropdown menu
    delimiter_var = StringVar(sett_tab)
    # set default value for dropdown menu
    delimiter_var.set(settings_list[30])
    # create list for options menu
    delim_options = ['Space', 'Comma', 'Tab', 'Semicolon', 'Colon', 'Vertical Slash', 'Underscore',
                     'Double Underscore', 'Hyphen', 'Dash']
    # create dropdown menu widget
    delim_combo = ttk.Combobox(sett_tab, textvariable=delimiter_var, values=delim_options, height=25)
    delim_combo.place(x=settings_x2 - 10, y=settings_y11 + 80)
    # create button
    settings_submit = tk.Button(sett_tab, text='Submit', height=button_h, width=button_w + 8,
                                command=loud_update_datasearch_ini)
    settings_submit.place(x=settings_x1 - 13, y=settings_y11 + 65)
    newdata_submit = tk.Button(sett_tab, text='Update Data', height=button_h, width=button_w + 8,
                               command=datasearch_upload)
    newdata_submit.place(x=settings_x3 - 13, y=settings_y11 + 65)

    """get data"""
    # import data for treeviews on start
    query_products()
    query_customers()
    query_plants()
    # update settings variables
    quiet_update_datasearch_ini()
    # run gui
    data.mainloop()


def launch_datasearch():
    global settings_list, data
    # check if data window is open
    try:
        data.destroy()
    except:
        pass
    # initialize databases
    setup_process = datasearch_import()
    # check if setup should be continued
    if setup_process:
        # initialize settings
        settings_list = get_datasearch_ini()
        # launch app
        datasearch_gui()
    return


"""Distributor Scorecard Functions"""


def str_splitter(split_string: str, delimiter: str) -> list:
    """Dynamically split a string by a delimiter"""
    # create list of strings
    str_list = split_string.split(delimiter)
    # create list to store final strings
    new_list = []
    # remove any spaces for string in list and add to list
    for s in str_list:
        new_list.append(s.strip())
    return new_list


def get_avg_min1(results: list) -> float:
    """Get average result from list of numbers after removing the last item in the list"""
    # remove last result from list
    results.pop()
    # get average of remaining list
    avgresult = mean(results)
    return avgresult


def used_range(time_obj: str) -> list:
    """Calculate execution scorecard week number range"""
    # convert string formatted date to datetime object
    dateobj = datetime.datetime.strptime(time_obj, '%m/%d/%Y')
    # calculate week number with offsets
    date_wk_m6 = str(get_past_timestamps(dateobj + datetime.timedelta(days=-42))[2]).zfill(2)
    date_wk_m5 = str(get_past_timestamps(dateobj + datetime.timedelta(days=-35))[2]).zfill(2)
    date_wk_m4 = str(get_past_timestamps(dateobj + datetime.timedelta(days=-28))[2]).zfill(2)
    date_wk_m3 = str(get_past_timestamps(dateobj + datetime.timedelta(days=-21))[2]).zfill(2)
    date_wk_m2 = str(get_past_timestamps(dateobj + datetime.timedelta(days=-14))[2]).zfill(2)
    date_wk_m1 = str(get_past_timestamps(dateobj + datetime.timedelta(days=-7))[2]).zfill(2)
    date_wk_p0 = str(get_past_timestamps(dateobj + datetime.timedelta(days=0))[2]).zfill(2)
    date_wk_p1 = str(get_past_timestamps(dateobj + datetime.timedelta(days=7))[2]).zfill(2)
    date_wk_p2 = str(get_past_timestamps(dateobj + datetime.timedelta(days=14))[2]).zfill(2)
    # calculate year number with offsets
    date_yr_m6 = str(get_past_timestamps(dateobj + datetime.timedelta(days=-42))[0])
    date_yr_m5 = str(get_past_timestamps(dateobj + datetime.timedelta(days=-35))[0])
    date_yr_m4 = str(get_past_timestamps(dateobj + datetime.timedelta(days=-28))[0])
    date_yr_m3 = str(get_past_timestamps(dateobj + datetime.timedelta(days=-21))[0])
    date_yr_m2 = str(get_past_timestamps(dateobj + datetime.timedelta(days=-14))[0])
    date_yr_m1 = str(get_past_timestamps(dateobj + datetime.timedelta(days=-7))[0])
    date_yr_p0 = str(get_past_timestamps(dateobj + datetime.timedelta(days=0))[0])
    date_yr_p1 = str(get_past_timestamps(dateobj + datetime.timedelta(days=7))[0])
    date_yr_p2 = str(get_past_timestamps(dateobj + datetime.timedelta(days=14))[0])
    # calculate year week number with offsets
    yr_wk_m6 = str(date_yr_m6) + '-' + str(date_wk_m6)
    yr_wk_m5 = str(date_yr_m5) + '-' + str(date_wk_m5)
    yr_wk_m4 = str(date_yr_m4) + '-' + str(date_wk_m4)
    yr_wk_m3 = str(date_yr_m3) + '-' + str(date_wk_m3)
    yr_wk_m2 = str(date_yr_m2) + '-' + str(date_wk_m2)
    yr_wk_m1 = str(date_yr_m1) + '-' + str(date_wk_m1)
    yr_wk_p0 = str(date_yr_p0) + '-' + str(date_wk_p0)
    yr_wk_p1 = str(date_yr_p1) + '-' + str(date_wk_p1)
    yr_wk_p2 = str(date_yr_p2) + '-' + str(date_wk_p2)
    return [yr_wk_m6, yr_wk_m5, yr_wk_m4, yr_wk_m3, yr_wk_m2, yr_wk_m1, yr_wk_p0, yr_wk_p1, yr_wk_p2]


def calc_offset_range(time_obj: str) -> list:
    """Calculate execution scorecard week number range"""
    # convert string formatted date to datetime object
    dateobj = datetime.datetime.strptime(time_obj, '%m/%d/%Y')
    # calculate week number with offsets
    date_wk_m6 = str(get_past_timestamps(dateobj + datetime.timedelta(days=-42))[2]).zfill(2)
    date_wk_m5 = str(get_past_timestamps(dateobj + datetime.timedelta(days=-35))[2]).zfill(2)
    date_wk_m4 = str(get_past_timestamps(dateobj + datetime.timedelta(days=-28))[2]).zfill(2)
    date_wk_m3 = str(get_past_timestamps(dateobj + datetime.timedelta(days=-21))[2]).zfill(2)
    date_wk_m2 = str(get_past_timestamps(dateobj + datetime.timedelta(days=-14))[2]).zfill(2)
    date_wk_m1 = str(get_past_timestamps(dateobj + datetime.timedelta(days=-7))[2]).zfill(2)
    date_wk_p0 = str(get_past_timestamps(dateobj)[2]).zfill(2)
    date_wk_p1 = str(get_past_timestamps(dateobj + datetime.timedelta(days=7))[2]).zfill(2)
    date_wk_p2 = str(get_past_timestamps(dateobj + datetime.timedelta(days=14))[2]).zfill(2)
    date_wk_p3 = str(get_past_timestamps(dateobj + datetime.timedelta(days=21))[2]).zfill(2)
    date_wk_p4 = str(get_past_timestamps(dateobj + datetime.timedelta(days=28))[2]).zfill(2)
    date_wk_p5 = str(get_past_timestamps(dateobj + datetime.timedelta(days=35))[2]).zfill(2)
    date_wk_p6 = str(get_past_timestamps(dateobj + datetime.timedelta(days=42))[2]).zfill(2)
    date_wk_p7 = str(get_past_timestamps(dateobj + datetime.timedelta(days=49))[2]).zfill(2)
    date_wk_p8 = str(get_past_timestamps(dateobj + datetime.timedelta(days=56))[2]).zfill(2)
    date_wk_p9 = str(get_past_timestamps(dateobj + datetime.timedelta(days=63))[2]).zfill(2)
    # calculate year number with offsets
    date_yr_m6 = str(get_past_timestamps(dateobj + datetime.timedelta(days=-42))[0])
    date_yr_m5 = str(get_past_timestamps(dateobj + datetime.timedelta(days=-35))[0])
    date_yr_m4 = str(get_past_timestamps(dateobj + datetime.timedelta(days=-28))[0])
    date_yr_m3 = str(get_past_timestamps(dateobj + datetime.timedelta(days=-21))[0])
    date_yr_m2 = str(get_past_timestamps(dateobj + datetime.timedelta(days=-14))[0])
    date_yr_m1 = str(get_past_timestamps(dateobj + datetime.timedelta(days=-7))[0])
    date_yr_p0 = get_past_timestamps(dateobj)[0]
    date_yr_p1 = get_past_timestamps(dateobj + datetime.timedelta(days=7))[0]
    date_yr_p2 = get_past_timestamps(dateobj + datetime.timedelta(days=14))[0]
    date_yr_p3 = get_past_timestamps(dateobj + datetime.timedelta(days=21))[0]
    date_yr_p4 = get_past_timestamps(dateobj + datetime.timedelta(days=28))[0]
    date_yr_p5 = get_past_timestamps(dateobj + datetime.timedelta(days=35))[0]
    date_yr_p6 = get_past_timestamps(dateobj + datetime.timedelta(days=42))[0]
    date_yr_p7 = get_past_timestamps(dateobj + datetime.timedelta(days=49))[0]
    date_yr_p8 = get_past_timestamps(dateobj + datetime.timedelta(days=56))[0]
    date_yr_p9 = get_past_timestamps(dateobj + datetime.timedelta(days=63))[0]
    # calculate year week number with offsets
    yr_wk_m6 = str(date_yr_m6) + '-' + str(date_wk_m6)
    yr_wk_m5 = str(date_yr_m5) + '-' + str(date_wk_m5)
    yr_wk_m4 = str(date_yr_m4) + '-' + str(date_wk_m4)
    yr_wk_m3 = str(date_yr_m3) + '-' + str(date_wk_m3)
    yr_wk_m2 = str(date_yr_m2) + '-' + str(date_wk_m2)
    yr_wk_m1 = str(date_yr_m1) + '-' + str(date_wk_m1)
    yr_wk_p0 = str(date_yr_p0) + '-' + str(date_wk_p0)
    yr_wk_p1 = str(date_yr_p1) + '-' + str(date_wk_p1)
    yr_wk_p2 = str(date_yr_p2) + '-' + str(date_wk_p2)
    yr_wk_p3 = str(date_yr_p3) + '-' + str(date_wk_p3)
    yr_wk_p4 = str(date_yr_p4) + '-' + str(date_wk_p4)
    yr_wk_p5 = str(date_yr_p5) + '-' + str(date_wk_p5)
    yr_wk_p6 = str(date_yr_p6) + '-' + str(date_wk_p6)
    yr_wk_p7 = str(date_yr_p7) + '-' + str(date_wk_p7)
    yr_wk_p8 = str(date_yr_p8) + '-' + str(date_wk_p8)
    yr_wk_p9 = str(date_yr_p9) + '-' + str(date_wk_p9)
    return [yr_wk_m6, yr_wk_m5, yr_wk_m4, yr_wk_m3, yr_wk_m2, yr_wk_m1, yr_wk_p0, yr_wk_p1,
            yr_wk_p2, yr_wk_p3, yr_wk_p4, yr_wk_p5, yr_wk_p6, yr_wk_p7, yr_wk_p8, yr_wk_p9]


def pic_scaler(pic: str, scale: float):
    """Resize image file preserving aspect ratio"""
    png = Image.open(pic)  # open image
    pic_wt = png.size[0] * scale  # calc max scaled width
    pic_ht = png.size[1] * scale  # calc max scaled height
    png.thumbnail((pic_wt, pic_ht))  # scale pic
    png.save(pic, verbose=False)  # overwrite original with resized image
    return png


def oosip_barplot(plot_data: pd.DataFrame, savepath, avg: float):
    """Create plot of the oos incident percentages for each week"""
    # get today's date
    today = datetime.datetime.today().strftime("%m-%d-%Y")  # mm-dd-yyyy
    plottitle = 'OOS Incidents % by Week - ' + str('{:.1%}'.format(float(avg))) + ' Average'
    png = 'OOSi Percentages by Week - ' + today + '.png'  # name file
    plot = (ggplot(data=plot_data)
            + geom_bar(mapping=aes(x='Yr-Wk', y='OOSi_%'), alpha=.25, color='#1496FF', fill='#1496FF', stat='identity')
            + geom_label(mapping=aes(label='OOSi_%', x='Yr-Wk', y='OOSi_%'), va='top',
                         format_string='{:.1%}', nudge_y=-.004, label_size=.25, data=plot_data,
                         alpha=1, position='identity', color='#091F3F', size=11)
            + labs(x='', y='OOS Incidents %', title=plottitle)
            + geom_hline(yintercept=avg, color='#FFB000', size=1, alpha=.5)
            + theme_gray()
            + theme(
                plot_title=element_text(style='italic', size=12, color='#091F3F', ha='center', weight='light'),
                figure_size=(8, 4.944),
                legend_position=(.8, .8),
                legend_title=element_text(text='', style='oblique', size=10, color='#091F3F', ha='center',
                                          weight='light'),
                legend_title_align='right',
                legend_background=element_rect(color='', size=1, fill='none'),
                legend_box_background=element_rect(color='#091F3F', size=9, fill='none', weight='light'),
                legend_direction='vertical',
                axis_line_x=element_line(size=1, color='#091F3F'),
                axis_line_y=element_line(size=1, color='#091F3F'),
                axis_text_x=element_text(text='', style='italic', size=9, color='#091F3F',
                                         ha='center', weight='light'),  # rotation=0, hjust=1
                axis_text_y=element_text(style='italic', size=8, color='#091F3F', ha='right', weight='light'),
                axis_title_x=element_text(size=10, color='#091F3F', ha='center', weight='light'),
                axis_title_y=element_text(size=10, color='#091F3F', ha='center', weight='light')))
    png = path.join(savepath, png)  # join file name to directory path
    png = os_split_fixer(png)  # correct operating system separators
    plot.save(png, height=6, width=8, verbose=False)  # save new file to directory
    return png


def oosic_barplot(plot_data: pd.DataFrame, savepath, avg: float):
    """Create plot of the oos incidents for each week"""
    # get today's date
    today = datetime.datetime.today().strftime("%m-%d-%Y")  # mm-dd-yyyy
    plottitle = 'OOS Incidents by Week - ' + str('{:.1f}'.format(float(avg))) + ' Average'
    png = 'OOS Incidents by Week - ' + today + '.png'  # name file
    plot = (ggplot(data=plot_data)
            + geom_bar(mapping=aes(x='Yr-Wk', y='OOSi'), alpha=.25, color='#091F3F', fill='#091F3F', stat='identity')
            + geom_label(mapping=aes(label='OOSi', x='Yr-Wk', y='OOSi'), va='top',
                         format_string='{:,}', nudge_y=-1, label_size=.25, data=plot_data,
                         alpha=1, position='identity', color='#091F3F', size=11)
            + labs(x='', y='OOS Incidents', title=plottitle)
            + geom_hline(yintercept=avg, color='#FFB000', size=1, alpha=.5)
            + theme_gray()
            + theme(
                plot_title=element_text(style='italic', size=12, color='#091F3F', ha='center', weight='light'),
                figure_size=(8, 4.944),
                legend_position=(.8, .8),
                legend_title=element_text(text='', style='oblique', size=10, color='#091F3F', ha='center',
                                          weight='light'),
                legend_title_align='right',
                legend_background=element_rect(color='', size=1, fill='none'),
                legend_box_background=element_rect(color='#091F3F', size=9, fill='none', weight='light'),
                legend_direction='vertical',
                axis_line_x=element_line(size=1, color='#091F3F'),
                axis_line_y=element_line(size=1, color='#091F3F'),
                axis_text_x=element_text(text='', style='italic', size=9, color='#091F3F',
                                         ha='center', weight='light'),
                axis_text_y=element_text(style='italic', size=8, color='#091F3F', ha='right', weight='light'),
                axis_title_x=element_text(size=10, color='#091F3F', ha='center', weight='light'),
                axis_title_y=element_text(size=10, color='#091F3F', ha='center', weight='light')))
    png = path.join(savepath, png)  # join file name to directory path
    png = os_split_fixer(png)  # correct operating system separators
    plot.save(png, height=6, width=8, verbose=False)  # save new file to directory
    return png


def oosvp_barplot(plot_data: pd.DataFrame, savepath: str, avg: float):
    """Create plot of the oos volume percentages for each week"""
    # get today's date
    today = datetime.datetime.today().strftime("%m-%d-%Y")  # mm-dd-yyyy
    plottitle = 'OOS Volume % by Week - ' + str('{:.1%}'.format(float(avg))) + ' Average'
    png = 'OOSv Percentages by Week - ' + today + '.png'  # name file
    plot = (ggplot(data=plot_data)
            + geom_bar(mapping=aes(x='Yr-Wk', y='OOSv_%'), alpha=.25, color='#1496FF', fill='#1496FF', stat='identity')
            + geom_label(mapping=aes(label='OOSv_%', x='Yr-Wk', y='OOSv_%'), va='top',
                         format_string='{:.1%}', nudge_y=-.001, label_size=.25, data=plot_data,
                         alpha=1, position='identity', color='#091F3F', size=11)
            + labs(x='', y='OOS Volume %', title=plottitle)
            + geom_hline(yintercept=avg, color='#FFB000', size=1, alpha=.5)
            + theme_gray()
            + theme(
                plot_title=element_text(style='italic', size=12, color='#091F3F', ha='center', weight='light'),
                figure_size=(8, 4.944),
                legend_position=(.8, .8),
                legend_title=element_text(text='', style='oblique', size=10, color='#091F3F', ha='center',
                                          weight='light'),
                legend_title_align='right',
                legend_background=element_rect(color='', size=1, fill='none'),
                legend_box_background=element_rect(color='#091F3F', size=9, fill='none', weight='light'),
                legend_direction='vertical',
                axis_line_x=element_line(size=1, color='#091F3F'),
                axis_line_y=element_line(size=1, color='#091F3F'),
                axis_text_x=element_text(text='', style='italic', size=9, color='#091F3F',
                                         ha='center', weight='light'),
                axis_text_y=element_text(style='italic', size=8, color='#091F3F', ha='right', weight='light'),
                axis_title_x=element_text(size=10, color='#091F3F', ha='center', weight='light'),
                axis_title_y=element_text(size=10, color='#091F3F', ha='center', weight='light')))
    png = path.join(savepath, png)  # join file name to directory path
    png = os_split_fixer(png)  # correct operating system separators
    plot.save(png, height=6, width=8, verbose=False)  # save new file to directory
    return png


def oosvc_barplot(plot_data: pd.DataFrame, savepath: str, avg: float):
    """Create plot of the oos volume for each week"""
    # get today's date
    today = datetime.datetime.today().strftime("%m-%d-%Y")  # mm-dd-yyyy
    plottitle = 'OOS Volume by Week - ' + str('{:,.1f}'.format(float(avg))) + ' Average'
    png = 'OOS Volume by Week - ' + today + '.png'  # name file
    plot = (ggplot(data=plot_data)
            + geom_bar(mapping=aes(x='Yr-Wk', y='OOSv'), alpha=.25, color='#091F3F', fill='#091F3F', stat='identity')
            + geom_label(mapping=aes(label='OOSv', x='Yr-Wk', y='OOSv'), va='top',
                         format_string='{:,.1f}', nudge_y=-8, label_size=.25, data=plot_data,
                         alpha=1, position='identity', color='#091F3F', size=10)
            + labs(x='', y='OOS Volume (BBLs)', title=plottitle)
            + geom_hline(yintercept=avg, color='#FFB000', size=1, alpha=.5)
            + theme_gray()
            + theme(
                plot_title=element_text(style='italic', size=12, color='#091F3F', ha='center', weight='light'),
                figure_size=(8, 4.944),
                legend_position=(.8, .8),
                legend_title=element_text(text='', style='oblique', size=10, color='#091F3F', ha='center',
                                          weight='light'),
                legend_title_align='right',
                legend_background=element_rect(color='', size=1, fill='none'),
                legend_box_background=element_rect(color='#091F3F', size=9, fill='none', weight='light'),
                legend_direction='vertical',
                axis_line_x=element_line(size=1, color='#091F3F'),
                axis_line_y=element_line(size=1, color='#091F3F'),
                axis_text_x=element_text(text='', style='italic', size=9, color='#091F3F',
                                         ha='center', weight='light'),
                axis_text_y=element_text(style='italic', size=8, color='#091F3F', ha='right', weight='light'),
                axis_title_x=element_text(size=10, color='#091F3F', ha='center', weight='light'),
                axis_title_y=element_text(size=10, color='#091F3F', ha='center', weight='light')))
    png = path.join(savepath, png)  # join file name to directory path
    png = os_split_fixer(png)  # correct operating system separators
    plot.save(png, height=6, width=8, verbose=False)  # save new file to directory
    return png


def doip_barplot(plot_data: pd.DataFrame, savepath: str, avgdoi: float):
    """Create plot of the DOI results for each week"""
    # get today's date
    today = datetime.datetime.today().strftime("%m-%d-%Y")  # mm-dd-yyyy
    plottitle = 'Days of Inventory by Week - ' + str('{:.1f}'.format(avgdoi)) + ' Average'
    png = 'Days of Inventory by Week - ' + today + '.png'  # name file
    png = path.join(savepath, png)  # join file name to directory path
    plot = (ggplot(data=plot_data, mapping=aes(x='Yr-Wk', color='Category', fill='Category'))
            + geom_bar(mapping=aes(x='Yr-Wk', y='DOI'), stat='identity', alpha=.25)
            + geom_label(mapping=aes(label='DOI', x='Yr-Wk', y='DOI'), va='top',
                         format_string='{:,.1f}', nudge_y=-1, label_size=.25, data=plot_data,
                         alpha=.85, position='identity', color='#FFFFFF', size=11, show_legend=False)
            + labs(x='', y='Starting Days of Inventory', title=plottitle)
            + geom_hline(yintercept=avgdoi, color='#FFB000', size=1, alpha=.75)
            + scale_color_manual(values=['#1496FF', '#091F3F'], limits=['Reported', 'Projected'])
            + scale_fill_manual(values=['#1496FF', '#091F3F'], limits=['Reported', 'Projected'])
            + theme_gray()
            + theme(
                plot_title=element_text(style='italic', size=12, color='#091F3F', ha='center', weight='light'),
                figure_size=(8, 4.944),
                legend_position=(.2, .225),
                legend_title=element_text(text='', style='oblique', size=10,
                                          color='#091F3F', ha='center', weight='light'),
                legend_title_align='right',
                legend_background=element_rect(color='', size=1, fill='none'),
                legend_box_background=element_rect(color='#091F3F', size=9, fill='none', weight='light'),
                legend_direction='vertical',
                axis_line_x=element_line(size=1, color='#091F3F'),
                axis_line_y=element_line(size=1, color='#091F3F'),
                axis_text_x=element_text(text='', style='italic', size=9, color='#091F3F',
                                         ha='center', weight='light'),
                axis_text_y=element_text(style='italic', size=8, color='#091F3F', ha='right', weight='light'),
                axis_title_x=element_text(size=10, color='#091F3F', ha='center', weight='light'),
                axis_title_y=element_text(size=10, color='#091F3F', ha='center', weight='light')))
    png = os_split_fixer(png)  # correct operating system separators
    plot.save(png, height=6, width=9, verbose=False)  # save new file to directory
    return png


def socc_barplot(plot_data: pd.DataFrame, savepath: str, avgimp: float):
    """Create plot of the SOC results for each week"""
    # get today's date
    today = datetime.datetime.today().strftime("%m-%d-%Y")  # mm-dd-yyyy
    plottitle = 'Execution Impact by Week - Net ' + str('{:+,.1f}'.format(avgimp)) + ' Average'
    png = 'Execution Impact by Week - ' + today + '.png'  # name file
    png = path.join(savepath, png)  # join file name to directory path
    plot = (ggplot(data=plot_data, mapping=aes(x='Yr-Wk', color='Category', fill='Category'))
            + geom_col(mapping=aes(x='Yr-Wk', y='Impact'), stat='identity', position='dodge', alpha=.25)
            + labs(x='', y='Impact (Units)', title=plottitle)
            + geom_hline(yintercept=avgimp, color='#FFB000', size=1, alpha=.75)
            + scale_color_manual(values=['#1496FF', '#091F3F',
                                         '#FF6912', '#FFB000'], limits=['1. Order', '2. Capacity',
                                                                        '3. Execution', '4. Net'])
            + scale_fill_manual(values=['#1496FF', '#091F3F',
                                        '#FF6912', '#FFB000'], limits=['1. Order', '2. Capacity',
                                                                       '3. Execution', '4. Net'])
            + theme_gray()
            + theme(
                plot_title=element_text(style='italic', size=12, color='#091F3F', ha='center', weight='light'),
                figure_size=(8, 4.944),
                legend_position=(.975, .55),
                legend_title=element_text(text='', style='oblique', size=10,
                                          color='#091F3F', ha='center', weight='light'),
                legend_title_align='right',
                # legend_key=None,
                legend_background=element_rect(color='', size=1, fill='none'),
                legend_box_background=element_rect(color='#091F3F', size=9, fill='none', weight='light'),
                legend_direction='vertical',
                axis_line_x=element_line(size=1, color='#091F3F'),
                axis_line_y=element_line(size=1, color='#091F3F'),
                axis_text_x=element_text(text='', style='italic', size=9, color='#091F3F',
                                         ha='center', weight='light'),
                axis_text_y=element_text(style='italic', size=8, color='#091F3F', ha='right', weight='light'),
                axis_title_x=element_text(size=10, color='#091F3F', ha='center', weight='light'),
                axis_title_y=element_text(size=10, color='#091F3F', ha='center', weight='light'))
            )
    png = os_split_fixer(png)  # correct operating system separators
    plot.save(png, height=6, width=9, verbose=False)  # save new file to directory
    return png


def fap_barplot(plot_data: pd.DataFrame, savepath: str, avg: float):
    """Create plot of the forecast accuracy percentages for each week"""
    # get today's date
    today = datetime.datetime.today().strftime("%m-%d-%Y")  # mm-dd-yyyy
    plottitle = 'Forecast Accuracy % by Week - ' + str('{:.1%}'.format(float(avg))) + ' Average'
    png = 'Forecast Accuracy % by Week - ' + today + '.png'  # name file
    plot = (ggplot(data=plot_data)
            + geom_bar(mapping=aes(x='Yr-Wk', y='Fcst_Acc'), alpha=.25, color='#1496FF', fill='#1496FF',
                       stat='identity')
            + geom_label(mapping=aes(label='Fcst_Acc', x='Yr-Wk', y='Fcst_Acc'), va='top',
                         format_string='{:.1%}', nudge_y=-.02, label_size=.25, data=plot_data,
                         alpha=1, position='identity', color='#091F3F', size=10)
            + labs(x='', y='Forecast Accuracy %', title=plottitle)
            + geom_hline(yintercept=avg, color='#FFB000', size=1, alpha=.5)
            + theme_gray()
            + theme(
                plot_title=element_text(style='italic', size=12, color='#091F3F', ha='center', weight='light'),
                figure_size=(8, 4.944),
                legend_position=(.8, .8),
                legend_title=element_text(text='', style='oblique', size=10, color='#091F3F', ha='center',
                                          weight='light'),
                legend_title_align='right',
                legend_background=element_rect(color='', size=1, fill='none'),
                legend_box_background=element_rect(color='#091F3F', size=9, fill='none', weight='light'),
                legend_direction='vertical',
                axis_line_x=element_line(size=1, color='#091F3F'),
                axis_line_y=element_line(size=1, color='#091F3F'),
                axis_text_x=element_text(text='', style='italic', size=9, color='#091F3F',
                                         ha='center', weight='light'),  # , rotation=45, hjust=.5
                axis_text_y=element_text(style='italic', size=8, color='#091F3F', ha='right', weight='light'),
                axis_title_x=element_text(size=10, color='#091F3F', ha='center', weight='light'),
                axis_title_y=element_text(size=10, color='#091F3F', ha='center', weight='light')))
    png = path.join(savepath, png)  # join file name to directory path
    png = os_split_fixer(png)  # correct operating system separators
    plot.save(png, height=6, width=12, verbose=False)  # save new file to directory
    return png


def oosplot_finalizer(im1, im2, im3, im4, savepath: str, dirname: str):
    # jalogo = resource_path('jalogo.png')
    # jalogo = Image.open(jalogo)
    topplot = get_concat_h_resize(im1, im2, resize_big_image=False)  # concat two pngs horizontally
    botplot = get_concat_h_resize(im3, im4, resize_big_image=False)  # concat two pngs horizontally
    fpicname = savepath + sep + 'OOS Results - ' + dirname + '.png'  # name graph file of results
    png = path.join(savepath, fpicname)  # join file name to directory path
    png = os_split_fixer(png)  # correct operating system separators
    plot = get_concat_v_resize(topplot, botplot)  # concat both pngs vertically
    # plot = get_concat_v_resize(plot, jalogo)  # concat both pngs vertically
    plot.save(png, verbose=False)  # save new file to directory
    # Image.open(png).show()  # open the final plot result
    return png


def fniplot_finalizer(im1, im2, savepath: str, dirname: str):
    """Finalize the forecast accuracy and days of inventory plots"""
    plot = get_concat_v_resize(im1, im2)  # concat two pngs vertically
    fpicname = savepath + sep + 'DOI and Forecast Results - ' + dirname + '.png'  # name graph file of results
    png = path.join(savepath, fpicname)  # join file name to directory path
    png = os_split_fixer(png)  # correct operating system separators
    plot.save(png, verbose=False)  # save new file to directory
    return png


def plotplot_finalizer(im1, im2, savepath: str, dirname: str):
    """Finalize the forecast accuracy and days of inventory combined plot with the OOS combined plot"""
    plot = get_concat_h_resize(im1, im2)  # concat two pngs horizontally
    fpicname = savepath + sep + 'Scorecard Plots - ' + dirname + '.png'  # name graph file of results
    png = path.join(savepath, fpicname)  # join file name to directory path
    png = os_split_fixer(png)  # correct operating system separators
    plot.save(png, verbose=False)  # save new file to directory
    return png


def plotplotplot_finalizer(im1, im2, savepath: str, dirname: str):
    """Finalize all combined plots into full metrics review dashboard"""
    jalogo = resource_path('jalogo_tall.png')
    jalogo = Image.open(jalogo)
    socplot = get_concat_v_blank(im2, jalogo)  # concat two pngs vertically
    plot = get_concat_h_blank(im1, socplot)  # concat two pngs vertically
    fpicname = savepath + sep + 'Scorecard Dashboard - ' + dirname + '.png'  # name graph file of results
    png = path.join(savepath, fpicname)  # join file name to directory path
    png = os_split_fixer(png)  # correct operating system separators
    # plot = get_concat_v_blank(plot, jalogo)  # concat two pngs vertically
    plot.save(png, verbose=False)  # save new file to directory
    return png


def plot2i_finalizer(im1, im2, savepath: str, dirname: str):
    # jalogo = resource_path('jalogo.png')
    # jalogo = Image.open(jalogo)
    plot = get_concat_h_resize(im1, im2, resize_big_image=False)  # concat two pngs horizontally
    fpicname = savepath + sep + 'OOS Incidents - ' + dirname + '.png'  # name graph file of results
    png = path.join(savepath, fpicname)  # join file name to directory path
    png = os_split_fixer(png)  # correct operating system separators
    # plot = get_concat_v_resize(plot, jalogo)  # concat both pngs vertically
    plot.save(png, verbose=False)  # save new file to directory
    # Image.open(png).show()  # open the final plot result
    return


def plot2v_finalizer(im1, im2, savepath: str, dirname: str):
    # jalogo = resource_path('jalogo.png')
    # jalogo = Image.open(jalogo)
    plot = get_concat_h_resize(im1, im2, resize_big_image=False)  # concat two pngs horizontally
    fpicname = savepath + sep + 'OOS Volume - ' + dirname + '.png'  # name graph file of results
    png = path.join(savepath, fpicname)  # join file name to directory path
    png = os_split_fixer(png)  # correct operating system separators
    # plot = get_concat_v_resize(plot, jalogo)  # concat both pngs vertically
    plot.save(png, verbose=False)  # save new file to directory
    # Image.open(png).show()  # open the final plot result
    return


def disco_oos(oosfile: str) -> list:
    """Get source data and create OOS results and graphs"""
    # turn off chained assignment warning
    pd.options.mode.chained_assignment = None  # default='warn'
    # turn off user warnings for creating a dictionary of OSKUs with duplicate IDs
    filterwarnings('ignore', category=UserWarning)
    # read excel to dataframe
    df = pd.read_excel(oosfile, sheet_name='OOS')
    # rename columns
    df = df.rename(columns={'Valid_Count': 'OOSc', 'ALL_STR': 'STR'})
    # reindex dataframe
    df = df[['Yr-Wk', 'ShipTo', 'ShipTo_Name', 'OSKU', 'Description', 'OOSi', 'OOSc', 'OOSv', 'STR']]
    # filling all missing values in dataframe with 0
    df = df.fillna(0)
    # create new columns
    df['OOSv_%'] = round(df['OOSv'] / df['STR'], 2)
    # reindex dataframe
    df = df[['Yr-Wk', 'ShipTo', 'ShipTo_Name', 'OSKU', 'Description', 'OOSi', 'OOSc', 'OOSv', 'STR']]
    # format dataframe
    df.style.format({'OOSi': '{:,}', 'OOSc': '{:,}', 'OOSv': '{:,.2f}', 'STR': '{:,.2f}'})
    # sort dataframe by oos sales to retailers - largest to smallest
    df = df.sort_values(by=['OOSv', 'OOSi'], ascending=[False, False])
    # reset index without creating a new column
    df.reset_index(drop=True, inplace=True)
    # get total results by CSA for UPD pallets and units
    rf1 = pd.DataFrame(df.groupby('Yr-Wk')['OOSv'].sum())
    rf2 = pd.DataFrame(df.groupby('Yr-Wk')['STR'].sum())
    rf3 = pd.DataFrame(df.groupby('Yr-Wk')['OOSi'].sum())
    rf4 = pd.DataFrame(df.groupby('Yr-Wk')['OOSc'].sum())
    # create a series with the index
    rf1.reset_index(inplace=True)
    rf2.reset_index(inplace=True)
    rf3.reset_index(inplace=True)
    rf4.reset_index(inplace=True)
    # create list of dataframes
    rflist = [rf1, rf2, rf3, rf4]
    # reduce dataframes into combined frame
    rf = reduce(lambda left, right: pd.merge(left, right, on='Yr-Wk'), rflist)
    # create new series
    rf['OOSv_%'] = rf['OOSv'] / rf['STR']
    rf['OOSi_%'] = rf['OOSi'] / rf['OOSc']
    # reindex dataframe
    rf = rf[['Yr-Wk', 'OOSv', 'STR', 'OOSv_%', 'OOSi', 'OOSc', 'OOSi_%']]
    # format full results
    df = df[df.OOSi == 1]  # only include OOS incidents
    # reset index
    df.reset_index(drop=True, inplace=True)
    df.reset_index(inplace=True)
    # create new series
    df['Rank'] = df['index'] + 1
    df['Total_OOSv_%'] = df['OOSv'] / df['OOSv'].sum()
    # reindex dataframe
    df = df[['Rank', 'Yr-Wk', 'ShipTo', 'ShipTo_Name', 'OSKU', 'Description', 'OOSv', 'Total_OOSv_%']]
    # rename columns
    df = df.rename(columns={'ShipTo_Name': 'Distributor Name',
                            'OOSv': 'OOS Vol (BBLs)', 'Total_OOSv_%': 'Total OOS Prop %'})
    return [rf, df]


def disco_fca(fafile: str) -> list:
    """Get source data and create doi results and graphs"""
    # turn off chained assignment warning
    pd.options.mode.chained_assignment = None  # default='warn'
    # turn off user warnings for creating a dictionary of OSKUs with duplicate IDs
    filterwarnings('ignore', category=UserWarning)
    # read excel worksheets to dataframes
    df = pd.read_excel(fafile, sheet_name='Forecast Accuracy')
    # filling all missing values in dataframes with 0
    df = df.fillna(0)
    # rename columns
    df = df.rename(columns={'Dist Nbr': 'ShipTo', 'Frcst Accuracy-RollUp': 'Fcst_Acc'})
    # create series
    df['Pair'] = df.agg('{0[ShipTo]}-{0[OSKU]}'.format, axis=1)
    # count weeks of forecasts included in the report
    fca_wk_count = len(df['Yr-Wk'].unique())
    # calculate average str volume across all weeks
    avg_wk_str = df['STR'].sum() / fca_wk_count
    # create pivot tables
    rf = pd.pivot_table(df, values=['STR', 'Fcst_Acc'], index='Pair', columns='Yr-Wk', aggfunc=np.sum).fillna(value=0)
    # convert multi-index dataframe to single index
    rf.reset_index(drop=False, inplace=True)
    # create list to add column names to
    pair_list = ['Pair']
    fca_fcs_list = []
    fca_wks_list = []
    # dynamically create column list based on number of columns
    for wk in range(1, fca_wk_count + 1):
        fca_fcs_list.append('Fcst' + str(wk).zfill(2))
        fca_wks_list.append('Week' + str(wk).zfill(2))
    # combine lists into dataframe column name list
    fca_col_list = pair_list + fca_fcs_list + fca_wks_list
    # rename results dataframe columns from list
    rf.columns = fca_col_list
    # create list to add volume projection results to
    vol_proj_list = []
    # dynamically calculate total str results for each week in list of weeks
    for wk in fca_wks_list:
        vol_proj_list.append(np.where(rf[wk].sum() == 0, avg_wk_str, rf[wk].sum()))
    # create list to add series names to
    vol_prop_list = []
    fca_prop_list = []
    # dynamically create list for naming series based on number of weeks of data
    for wk in range(1, fca_wk_count + 1):
        vol_prop_list.append('Vol_Prop_WK' + str(wk).zfill(2))
        fca_prop_list.append('FCA_Prop_WK' + str(wk).zfill(2))
    # create index iterating variable
    vproj_n = 0
    # dynamically create series from names in list
    for vprop in vol_prop_list:
        # create volume proportion series from list
        rf[vprop] = np.where(rf[fca_wks_list[vproj_n]] == 0,
                             avg_wk_str / vol_proj_list[vproj_n],
                             rf[fca_wks_list[vproj_n]] / vol_proj_list[vproj_n])
        # create forecast accuracy proportion from list
        rf[fca_prop_list[vproj_n]] = rf[vprop] * rf[fca_fcs_list[vproj_n]]
        # iterate the variable
        vproj_n += 1
    # create list of average forecast accuracy
    avg_fca_list = []
    # get total of each forecast accuracy proportion series
    for fprop in fca_prop_list:
        avg_fca_list.append(round(rf[fprop].sum(), 2))
    # get list of weeks
    fca_wk_list = df['Yr-Wk'].unique()
    # zip list of weeks and fca accuracy
    fca_tuples = list(zip(fca_wk_list, avg_fca_list))
    # create dataframe from zipped list of weeks and fca accuracy
    rf = pd.DataFrame(fca_tuples, columns=['Yr-Wk', 'Fcst_Acc'])
    # reindex dataframe
    df = df[['Yr-Wk', 'Pair', 'ShipTo', 'OSKU', 'Description',
             'Distr Fcst STR Daily Units', 'STR', 'Variance-RollUp', 'MAPE-RollUp', 'Fcst_Acc']]
    # rename columns
    df = df.rename(columns={'Ship To Dist DBA Name': 'Distributor Name',
                            'Distr Fcst STR Daily Units': 'STR Fcst (Units)',
                            'STR': 'STR (Units)', 'Variance-RollUp': 'Variance',
                            'MAPE-RollUp': 'MAPE', 'Fcst_Acc': 'Fcst Accuracy'})
    return [rf, df]


def disco_exc(excfile: str) -> list:
    """Get source data and create doi results and graphs"""
    # turn off chained assignment warning
    pd.options.mode.chained_assignment = None  # default='warn'
    # turn off user warnings for creating a dictionary of OSKUs with duplicate IDs
    filterwarnings('ignore', category=UserWarning)
    # get today's date as string in format %m/%d/%Y
    todaystrdate = str(get_timestamps()[8])
    # calculate used week range offset
    weekrange = calc_offset_range(todaystrdate)
    # slice list to only include DOI framed weeks - week-6 to week+2
    doiwkrng = weekrange[:9]
    # read excel worksheets to dataframes
    orddf = pd.read_excel(excfile, sheet_name='Orders').fillna(value=0)
    invdf = pd.read_excel(excfile, sheet_name='Inventory').fillna(value=0)
    frcdf = pd.read_excel(excfile, sheet_name='Forecast (Dist)').fillna(value=0)
    strdf = pd.read_excel(excfile, sheet_name='Sales').fillna(value=0)
    dscdf = pd.read_excel(excfile, sheet_name='Sales').fillna(value=0)
    dnpdf = pd.read_excel(excfile, sheet_name='DNP').fillna(value=0)
    # get list of weeks in orders dataframe
    order_week_list = orddf['Yr-Wk'].tolist()
    missing_weeks = []
    # check if the execution data has any missing weeks and add to list
    for week in doiwkrng:
        if week not in order_week_list:
            missing_weeks.append(week)
    # create list of 0s based on items in missing weeks list
    zero_wk_list = [0] * len(missing_weeks)
    # create dataframe with missing weeks fill with 0s
    mwkdf = pd.DataFrame({
        'Pair': zero_wk_list,
        'Year': zero_wk_list,
        'Week': zero_wk_list,
        'Yr-Wk': missing_weeks,
        'ShipTo': zero_wk_list,
        'OSKU': zero_wk_list,
        'Description': zero_wk_list,
        'Suggest': zero_wk_list,
        'Order': zero_wk_list,
        'Original_Confirm': zero_wk_list,
        'Current_Confirm': zero_wk_list
    })
    # add any missing weeks dataframe to execution dataframe
    orddf = pd.concat([orddf, mwkdf], ignore_index=True, axis=0)
    # reindex dataframe for OSKU description merging later
    dscdf = dscdf[['Pair', 'OSKU', 'Description']]
    # create pivot tables
    orddf = pd.pivot_table(orddf, values=['Suggest', 'Order', 'Original_Confirm', 'Current_Confirm'],
                           index='Pair', columns='Yr-Wk', aggfunc=np.sum).fillna(value=0)
    strdf = pd.pivot_table(strdf, values='STRs', index='Pair', columns='Yr-Wk', aggfunc=np.sum).fillna(value=0)
    # convert multi-index dataframe to single index
    orddf.reset_index(drop=False, inplace=True)
    # rename dataframe headers from list
    orddf.columns = ['Pair', 'Curr_Conf_WK-6', 'Curr_Conf_WK-5', 'Curr_Conf_WK-4', 'Curr_Conf_WK-3',
                     'Curr_Conf_WK-2', 'Curr_Conf_WK-1', 'Curr_Conf_WK+0', 'Curr_Conf_WK+1',
                     'Curr_Conf_WK+2', 'Orders_WK-6', 'Orders_WK-5', 'Orders_WK-4', 'Orders_WK-3',
                     'Orders_WK-2', 'Orders_WK-1', 'Orders_WK+0', 'Orders_WK+1', 'Orders_WK+2',
                     'Orig_Conf_WK-6', 'Orig_Conf_WK-5', 'Orig_Conf_WK-4', 'Orig_Conf_WK-3',
                     'Orig_Conf_WK-2', 'Orig_Conf_WK-1', 'Orig_Conf_WK+0', 'Orig_Conf_WK+1',
                     'Orig_Conf_WK+2', 'Suggest_WK-6', 'Suggest_WK-5', 'Suggest_WK-4', 'Suggest_WK-3',
                     'Suggest_WK-2', 'Suggest_WK-1', 'Suggest_WK+0', 'Suggest_WK+1', 'Suggest_WK+2']
    # remove wk-1 current confirm from Suggest to correct
    orddf['Suggest_WK-2'] = orddf['Suggest_WK-2'] - orddf['Curr_Conf_WK-2']
    # rename columns
    strdf = strdf.rename(columns={strdf.columns[0]: 'STRs_WK-6', strdf.columns[1]: 'STRs_WK-5',
                                  strdf.columns[2]: 'STRs_WK-4', strdf.columns[3]: 'STRs_WK-3',
                                  strdf.columns[4]: 'STRs_WK-2', strdf.columns[5]: 'STRs_WK-1'})
    # create list of dataframes to merge
    dfdoilist = [invdf, strdf, frcdf, orddf, dnpdf]
    # reduce dataframes into combined frame
    doidf = reduce(lambda left, right: pd.merge(left, right, how='outer', on='Pair'), dfdoilist)
    # fill blank values with zeros
    doidf = doidf.fillna(value=0)
    # remove unnecessary rows
    doidf = doidf[doidf.Pair != '#ERROR']
    # reindex the dataframe
    doidf = doidf[['Pair', 'Year', 'Week', 'Yr-Wk', 'ShipTo', 'OSKU_x', 'Description_x', 'DNP_Flag',
                   'Beg_Inv_WK+0', 'Beg_Inv_WK+1', 'Suggest_WK-6', 'Suggest_WK-5',
                   'Suggest_WK-4', 'Suggest_WK-3', 'Suggest_WK-2', 'Suggest_WK-1',
                   'Suggest_WK+0', 'Suggest_WK+1', 'Suggest_WK+2', 'Orders_WK-6', 'Orders_WK-5', 'Orders_WK-4',
                   'Orders_WK-3', 'Orders_WK-2', 'Orders_WK-1', 'Orders_WK+0', 'Orders_WK+1', 'Orders_WK+2',
                   'Orig_Conf_WK-6', 'Orig_Conf_WK-5', 'Orig_Conf_WK-4', 'Orig_Conf_WK-3', 'Orig_Conf_WK-2',
                   'Orig_Conf_WK-1', 'Orig_Conf_WK+0', 'Orig_Conf_WK+1', 'Orig_Conf_WK+2',
                   'Curr_Conf_WK-6', 'Curr_Conf_WK-5', 'Curr_Conf_WK-4', 'Curr_Conf_WK-3', 'Curr_Conf_WK-2',
                   'Curr_Conf_WK-1', 'Curr_Conf_WK+0', 'Curr_Conf_WK+1', 'Curr_Conf_WK+2',
                   'STRs_WK-6', 'STRs_WK-5', 'STRs_WK-4', 'STRs_WK-3', 'STRs_WK-2', 'STRs_WK-1',
                   'DFcst_WK+0', 'DFcst_WK+1', 'DFcst_WK+2', 'DFcst_WK+3', 'DFcst_WK+4', 'DFcst_WK+5',
                   'DFcst_WK+6', 'DFcst_WK+7', 'DFcst_WK+8', 'DFcst_WK+9']]
    # rename columns
    doidf = doidf.rename(columns={'OSKU_x': 'OSKU', 'Description_x': 'Description'})
    # create new series to remove by
    doidf['Remove'] = np.where(
        (doidf['Beg_Inv_WK+0'] == 0) & (doidf['STRs_WK-6'] == 0) & (doidf['STRs_WK-5'] == 0) &
        (doidf['STRs_WK-4'] == 0) & (doidf['STRs_WK-3'] == 0) & (doidf['STRs_WK-2'] == 0) &
        (doidf['STRs_WK-1'] == 0) & (doidf['Curr_Conf_WK+0'] == 0) & (doidf['Curr_Conf_WK+1'] == 0) &
        (doidf['Curr_Conf_WK+2'] == 0) & (doidf['Orders_WK+0'] == 0) & (doidf['Orders_WK+1'] == 0) &
        (doidf['Orders_WK+2'] == 0) & (doidf['Orig_Conf_WK+0'] == 0) & (doidf['Orig_Conf_WK+1'] == 0) &
        (doidf['Orig_Conf_WK+2'] == 0) & (doidf['Suggest_WK+0'] == 0) & (doidf['Suggest_WK+1'] == 0) &
        (doidf['Suggest_WK+2'] == 0) & (doidf['DFcst_WK+0'] == 0) & (doidf['DFcst_WK+1'] == 0) &
        (doidf['DFcst_WK+2'] == 0) & (doidf['DFcst_WK+3'] == 0) & (doidf['DFcst_WK+4'] == 0) &
        (doidf['DFcst_WK+5'] == 0) & (doidf['DFcst_WK+6'] == 0) & (doidf['DFcst_WK+7'] == 0) &
        (doidf['DFcst_WK+8'] == 0) & (doidf['DFcst_WK+9'] == 0), 1, 0)
    # only keep rows that have nonzero values
    doidf = doidf[~(doidf.Remove != 0)]
    # format series in dataframe
    doidf.astype({'Pair': 'string', 'ShipTo': 'int', 'OSKU': 'int', 'Description': 'string',
                  'DNP_Flag': 'string', 'Beg_Inv_WK+0': 'int', 'Beg_Inv_WK+1': 'int',
                  'STRs_WK-6': 'int', 'STRs_WK-5': 'int', 'STRs_WK-4': 'int', 'STRs_WK-3': 'int',
                  'STRs_WK-2': 'int', 'STRs_WK-1': 'int',
                  'DFcst_WK+0': 'int', 'DFcst_WK+1': 'int', 'DFcst_WK+2': 'int', 'DFcst_WK+3': 'int',
                  'DFcst_WK+4': 'int', 'DFcst_WK+5': 'int', 'DFcst_WK+6': 'int',
                  'DFcst_WK+7': 'int', 'DFcst_WK+8': 'int', 'DFcst_WK+9': 'int',
                  'Suggest_WK-6': 'int', 'Suggest_WK-5': 'int', 'Suggest_WK-4': 'int',
                  'Suggest_WK-3': 'int', 'Suggest_WK-2': 'int', 'Suggest_WK-1': 'int',
                  'Suggest_WK+0': 'int', 'Suggest_WK+1': 'int', 'Suggest_WK+2': 'int',
                  'Orders_WK-6': 'int', 'Orders_WK-5': 'int', 'Orders_WK-4': 'int',
                  'Orders_WK-3': 'int', 'Orders_WK-2': 'int', 'Orders_WK-1': 'int',
                  'Orders_WK+0': 'int', 'Orders_WK+1': 'int', 'Orders_WK+2': 'int',
                  'Curr_Conf_WK-6': 'int', 'Curr_Conf_WK-5': 'int', 'Curr_Conf_WK-4': 'int',
                  'Curr_Conf_WK-3': 'int', 'Curr_Conf_WK-2': 'int', 'Curr_Conf_WK-1': 'int',
                  'Curr_Conf_WK+0': 'int', 'Curr_Conf_WK+1': 'int', 'Curr_Conf_WK+2': 'int',
                  'Orig_Conf_WK-6': 'int', 'Orig_Conf_WK-5': 'int', 'Orig_Conf_WK-4': 'int',
                  'Orig_Conf_WK-3': 'int', 'Orig_Conf_WK-2': 'int', 'Orig_Conf_WK-1': 'int',
                  'Orig_Conf_WK+0': 'int', 'Orig_Conf_WK+1': 'int', 'Orig_Conf_WK+2': 'int'})
    # calculate inventory by week - going backwards
    doidf['Beg_Inv_WK-1'] = np.where(doidf['Beg_Inv_WK+0'] - doidf['Curr_Conf_WK-1'] + doidf['STRs_WK-1'] < 0, 0,
                                     doidf['Beg_Inv_WK+0'] - doidf['Curr_Conf_WK-1'] + doidf['STRs_WK-1'])
    doidf['Beg_Inv_WK-2'] = np.where(doidf['Beg_Inv_WK-1'] - doidf['Curr_Conf_WK-2'] + doidf['STRs_WK-2'] < 0, 0,
                                     doidf['Beg_Inv_WK-1'] - doidf['Curr_Conf_WK-2'] + doidf['STRs_WK-2'])
    doidf['Beg_Inv_WK-3'] = np.where(doidf['Beg_Inv_WK-2'] - doidf['Curr_Conf_WK-3'] + doidf['STRs_WK-3'] < 0, 0,
                                     doidf['Beg_Inv_WK-2'] - doidf['Curr_Conf_WK-3'] + doidf['STRs_WK-3'])
    doidf['Beg_Inv_WK-4'] = np.where(doidf['Beg_Inv_WK-3'] - doidf['Curr_Conf_WK-4'] + doidf['STRs_WK-4'] < 0, 0,
                                     doidf['Beg_Inv_WK-3'] - doidf['Curr_Conf_WK-4'] + doidf['STRs_WK-4'])
    doidf['Beg_Inv_WK-5'] = np.where(doidf['Beg_Inv_WK-4'] - doidf['Curr_Conf_WK-5'] + doidf['STRs_WK-5'] < 0, 0,
                                     doidf['Beg_Inv_WK-4'] - doidf['Curr_Conf_WK-5'] + doidf['STRs_WK-5'])
    doidf['Beg_Inv_WK-6'] = np.where(doidf['Beg_Inv_WK-5'] - doidf['Curr_Conf_WK-6'] + doidf['STRs_WK-6'] < 0, 0,
                                     doidf['Beg_Inv_WK-5'] - doidf['Curr_Conf_WK-6'] + doidf['STRs_WK-6'])
    # calculate baseline week +2 beginning inventory
    doidf['Beg_Inv_WK+2'] = np.where(doidf['Beg_Inv_WK+1'] + doidf['Curr_Conf_WK+1'] - doidf['DFcst_WK+1'] < 0, 0,
                                     doidf['Beg_Inv_WK+1'] + doidf['Curr_Conf_WK+1'] - doidf['DFcst_WK+1'])
    # recalculate week +2 beginning inventory based on conditions
    doidf['Adj_Flag'] = np.where(
        doidf['Beg_Inv_WK+0'] + doidf['Curr_Conf_WK+0'] - doidf['DFcst_WK+0'] > doidf['Beg_Inv_WK+1'], True, False)
    # get value for condition 1 and set any negatives to zero
    doidf['Condition_1'] = doidf['Beg_Inv_WK+0'] + doidf['Curr_Conf_WK+0'] - doidf['DFcst_WK+0']
    doidf['Condition_1'] = np.where(doidf['Condition_1'] < 0, 0, doidf['Condition_1'])
    # get value for condition 2 and set any positives to zero
    doidf['Condition_2'] = doidf['Beg_Inv_WK+1'] + doidf['Curr_Conf_WK+1'] - doidf['DFcst_WK+1']
    doidf['Condition_2'] = np.where(doidf['Condition_2'] > 0, 0, doidf['Condition_2'])
    # get value for condition 3 as the absolute value
    doidf['Condition_3'] = doidf['Beg_Inv_WK+0'] - doidf['DFcst_WK+0'] - doidf['Beg_Inv_WK+1']
    doidf['Condition_3'] = np.where(doidf['Condition_3'] < 0, doidf['Condition_3'] * -1, doidf['Condition_3'])
    # calculate adjusted beginning week+2 inventory
    doidf['Beg_Inv_WK+2'] = np.where(doidf['Adj_Flag'],
                                     doidf['Beg_Inv_WK+2'] + doidf['Condition_1'] - doidf['Beg_Inv_WK+1'] + doidf[
                                         'Condition_2'] + doidf['Condition_3'], doidf['Beg_Inv_WK+2'])
    # set any negative values for beginning week+2 inventory as zero
    doidf['Beg_Inv_WK+2'] = np.where(doidf['Beg_Inv_WK+2'] < 0, 0, doidf['Beg_Inv_WK+2'])
    # fill 0s in ShipTo and OSKU columns
    doidf['ShipTo'] = np.where(doidf['ShipTo'] == 0,
                               doidf.apply(lambda x: str_splitter(x['Pair'], '-')[0], axis=1), doidf['ShipTo'])
    doidf['OSKU'] = np.where(doidf['OSKU'] == 0,
                             doidf.apply(lambda x: str_splitter(x['Pair'], '-')[1], axis=1), doidf['OSKU'])
    # create cumulative forecast series
    doidf['Projections_-6_-5'] = doidf['STRs_WK-6'] + doidf['STRs_WK-5']
    doidf['Projections_-6_-4'] = doidf['STRs_WK-6'] + doidf['STRs_WK-5'] + doidf['STRs_WK-4']
    doidf['Projections_-6_-3'] = doidf['STRs_WK-6'] + doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3']
    doidf['Projections_-6_-2'] = doidf['STRs_WK-6'] + doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + \
                                 doidf['STRs_WK-2']
    doidf['Projections_-6_-1'] = doidf['STRs_WK-6'] + doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + \
                                 doidf['STRs_WK-2'] + doidf['STRs_WK-1']
    doidf['Projections_-6_+0'] = doidf['STRs_WK-6'] + doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + \
                                 doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0']
    doidf['Projections_-6_+1'] = doidf['STRs_WK-6'] + doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + \
                                 doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1']
    doidf['Projections_-6_+2'] = doidf['STRs_WK-6'] + doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + \
                                 doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + \
                                 doidf['DFcst_WK+2']
    doidf['Projections_-6_+3'] = doidf['STRs_WK-6'] + doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + \
                                 doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + \
                                 doidf['DFcst_WK+2'] + doidf['DFcst_WK+3']
    doidf['Projections_-6_+4'] = doidf['STRs_WK-6'] + doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + \
                                 doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + \
                                 doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4']
    doidf['Projections_-6_+5'] = doidf['STRs_WK-6'] + doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + \
                                 doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + \
                                 doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5']
    doidf['Projections_-6_+6'] = doidf['STRs_WK-6'] + doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + \
                                 doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + \
                                 doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + \
                                 doidf['DFcst_WK+6']
    doidf['Projections_-6_+7'] = doidf['STRs_WK-6'] + doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + \
                                 doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + \
                                 doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + \
                                 doidf['DFcst_WK+6'] + doidf['DFcst_WK+7']
    doidf['Projections_-6_+8'] = doidf['STRs_WK-6'] + doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + \
                                 doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + \
                                 doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + \
                                 doidf['DFcst_WK+6'] + doidf['DFcst_WK+7'] + doidf['DFcst_WK+8']
    doidf['Projections_-6_+9'] = doidf['STRs_WK-6'] + doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + \
                                 doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + \
                                 doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + \
                                 doidf['DFcst_WK+6'] + doidf['DFcst_WK+7'] + doidf['DFcst_WK+8'] + doidf['DFcst_WK+9']
    doidf['Projections_-5_-4'] = doidf['STRs_WK-5'] + doidf['STRs_WK-4']
    doidf['Projections_-5_-3'] = doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3']
    doidf['Projections_-5_-2'] = doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2']
    doidf['Projections_-5_-1'] = doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + \
                                 doidf['STRs_WK-1']
    doidf['Projections_-5_+0'] = doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + \
                                 doidf['STRs_WK-1'] + doidf['DFcst_WK+0']
    doidf['Projections_-5_+1'] = doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + \
                                 doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1']
    doidf['Projections_-5_+2'] = doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + \
                                 doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2']
    doidf['Projections_-5_+3'] = doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + \
                                 doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + \
                                 doidf['DFcst_WK+3']
    doidf['Projections_-5_+4'] = doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + \
                                 doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + \
                                 doidf['DFcst_WK+3'] + doidf['DFcst_WK+4']
    doidf['Projections_-5_+5'] = doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + \
                                 doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + \
                                 doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5']
    doidf['Projections_-5_+6'] = doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + \
                                 doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + \
                                 doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + doidf['DFcst_WK+6']
    doidf['Projections_-5_+7'] = doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + \
                                 doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + \
                                 doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + doidf['DFcst_WK+6'] + \
                                 doidf['DFcst_WK+7']
    doidf['Projections_-5_+8'] = doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + \
                                 doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + \
                                 doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + doidf['DFcst_WK+6'] + \
                                 doidf['DFcst_WK+7'] + doidf['DFcst_WK+8']
    doidf['Projections_-5_+9'] = doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + \
                                 doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + \
                                 doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + doidf['DFcst_WK+6'] + \
                                 doidf['DFcst_WK+7'] + doidf['DFcst_WK+8'] + doidf['DFcst_WK+9']
    doidf['Projections_-4_-3'] = doidf['STRs_WK-4'] + doidf['STRs_WK-3']
    doidf['Projections_-4_-2'] = doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2']
    doidf['Projections_-4_-1'] = doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + doidf['STRs_WK-1']
    doidf['Projections_-4_+0'] = doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + \
                                 doidf['DFcst_WK+0']
    doidf['Projections_-4_+1'] = doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + \
                                 doidf['DFcst_WK+0'] + doidf['DFcst_WK+1']
    doidf['Projections_-4_+2'] = doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + \
                                 doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2']
    doidf['Projections_-4_+3'] = doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + \
                                 doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3']
    doidf['Projections_-4_+4'] = doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + \
                                 doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + \
                                 doidf['DFcst_WK+4']
    doidf['Projections_-4_+5'] = doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + \
                                 doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + \
                                 doidf['DFcst_WK+4'] + doidf['DFcst_WK+5']
    doidf['Projections_-4_+6'] = doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + \
                                 doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + \
                                 doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + doidf['DFcst_WK+6']
    doidf['Projections_-4_+7'] = doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + \
                                 doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + \
                                 doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + doidf['DFcst_WK+6'] + doidf['DFcst_WK+7']
    doidf['Projections_-4_+8'] = doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + \
                                 doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + \
                                 doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + doidf['DFcst_WK+6'] + doidf['DFcst_WK+7'] + \
                                 doidf['DFcst_WK+8']
    doidf['Projections_-4_+9'] = doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + \
                                 doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + \
                                 doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + doidf['DFcst_WK+6'] + doidf['DFcst_WK+7'] + \
                                 doidf['DFcst_WK+8'] + doidf['DFcst_WK+9']
    doidf['Projections_-3_-2'] = doidf['STRs_WK-3'] + doidf['STRs_WK-2']
    doidf['Projections_-3_-1'] = doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + doidf['STRs_WK-1']
    doidf['Projections_-3_+0'] = doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0']
    doidf['Projections_-3_+1'] = doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + \
                                 doidf['DFcst_WK+1']
    doidf['Projections_-3_+2'] = doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + \
                                 doidf['DFcst_WK+1'] + doidf['DFcst_WK+2']
    doidf['Projections_-3_+3'] = doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + \
                                 doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3']
    doidf['Projections_-3_+4'] = doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + \
                                 doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4']
    doidf['Projections_-3_+5'] = doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + \
                                 doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + \
                                 doidf['DFcst_WK+5']
    doidf['Projections_-3_+6'] = doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + \
                                 doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + \
                                 doidf['DFcst_WK+5'] + doidf['DFcst_WK+6']
    doidf['Projections_-3_+7'] = doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + \
                                 doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + \
                                 doidf['DFcst_WK+5'] + doidf['DFcst_WK+6'] + doidf['DFcst_WK+7']
    doidf['Projections_-3_+8'] = doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + \
                                 doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + \
                                 doidf['DFcst_WK+5'] + doidf['DFcst_WK+6'] + doidf['DFcst_WK+7'] + doidf['DFcst_WK+8']
    doidf['Projections_-3_+9'] = doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + \
                                 doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + \
                                 doidf['DFcst_WK+5'] + doidf['DFcst_WK+6'] + doidf['DFcst_WK+7'] + doidf['DFcst_WK+8'] + \
                                 doidf['DFcst_WK+9']
    doidf['Projections_-2_-1'] = doidf['STRs_WK-2'] + doidf['STRs_WK-1']
    doidf['Projections_-2_+0'] = doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0']
    doidf['Projections_-2_+1'] = doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1']
    doidf['Projections_-2_+2'] = doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + \
                                 doidf['DFcst_WK+2']
    doidf['Projections_-2_+3'] = doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + \
                                 doidf['DFcst_WK+2'] + doidf['DFcst_WK+3']
    doidf['Projections_-2_+4'] = doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + \
                                 doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4']
    doidf['Projections_-2_+5'] = doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + \
                                 doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5']
    doidf['Projections_-2_+6'] = doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + \
                                 doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + \
                                 doidf['DFcst_WK+6']
    doidf['Projections_-2_+7'] = doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + \
                                 doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + \
                                 doidf['DFcst_WK+6'] + doidf['DFcst_WK+7']
    doidf['Projections_-2_+8'] = doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + \
                                 doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + \
                                 doidf['DFcst_WK+6'] + doidf['DFcst_WK+7'] + doidf['DFcst_WK+8']
    doidf['Projections_-2_+9'] = doidf['STRs_WK-2'] + doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + \
                                 doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + \
                                 doidf['DFcst_WK+6'] + doidf['DFcst_WK+7'] + doidf['DFcst_WK+8'] + doidf['DFcst_WK+9']
    doidf['Projections_-1_+0'] = doidf['STRs_WK-1'] + doidf['DFcst_WK+0']
    doidf['Projections_-1_+1'] = doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1']
    doidf['Projections_-1_+2'] = doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2']
    doidf['Projections_-1_+3'] = doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + \
                                 doidf['DFcst_WK+3']
    doidf['Projections_-1_+4'] = doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + \
                                 doidf['DFcst_WK+3'] + doidf['DFcst_WK+4']
    doidf['Projections_-1_+5'] = doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + \
                                 doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5']
    doidf['Projections_-1_+6'] = doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + \
                                 doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + doidf['DFcst_WK+6']
    doidf['Projections_-1_+7'] = doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + \
                                 doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + doidf['DFcst_WK+6'] + \
                                 doidf['DFcst_WK+7']
    doidf['Projections_-1_+8'] = doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + \
                                 doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + doidf['DFcst_WK+6'] + \
                                 doidf['DFcst_WK+7'] + doidf['DFcst_WK+8']
    doidf['Projections_-1_+9'] = doidf['STRs_WK-1'] + doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + \
                                 doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + doidf['DFcst_WK+6'] + \
                                 doidf['DFcst_WK+7'] + doidf['DFcst_WK+8'] + doidf['DFcst_WK+9']
    doidf['Projections_+0_+1'] = doidf['DFcst_WK+0'] + doidf['DFcst_WK+1']
    doidf['Projections_+0_+2'] = doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2']
    doidf['Projections_+0_+3'] = doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3']
    doidf['Projections_+0_+4'] = doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + \
                                 doidf['DFcst_WK+4']
    doidf['Projections_+0_+5'] = doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + \
                                 doidf['DFcst_WK+4'] + doidf['DFcst_WK+5']
    doidf['Projections_+0_+6'] = doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + \
                                 doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + doidf['DFcst_WK+6']
    doidf['Projections_+0_+7'] = doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + \
                                 doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + doidf['DFcst_WK+6'] + doidf['DFcst_WK+7']
    doidf['Projections_+0_+8'] = doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + \
                                 doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + doidf['DFcst_WK+6'] + doidf['DFcst_WK+7'] + \
                                 doidf['DFcst_WK+8']
    doidf['Projections_+0_+9'] = doidf['DFcst_WK+0'] + doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + \
                                 doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + doidf['DFcst_WK+6'] + doidf['DFcst_WK+7'] + \
                                 doidf['DFcst_WK+8'] + doidf['DFcst_WK+9']
    doidf['Projections_+1_+2'] = doidf['DFcst_WK+1'] + doidf['DFcst_WK+2']
    doidf['Projections_+1_+3'] = doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3']
    doidf['Projections_+1_+4'] = doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4']
    doidf['Projections_+1_+5'] = doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + \
                                 doidf['DFcst_WK+5']
    doidf['Projections_+1_+6'] = doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + \
                                 doidf['DFcst_WK+5'] + doidf['DFcst_WK+6']
    doidf['Projections_+1_+7'] = doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + \
                                 doidf['DFcst_WK+5'] + doidf['DFcst_WK+6'] + doidf['DFcst_WK+7']
    doidf['Projections_+1_+8'] = doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + \
                                 doidf['DFcst_WK+5'] + doidf['DFcst_WK+6'] + doidf['DFcst_WK+7'] + doidf['DFcst_WK+8']
    doidf['Projections_+1_+9'] = doidf['DFcst_WK+1'] + doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + \
                                 doidf['DFcst_WK+5'] + doidf['DFcst_WK+6'] + doidf['DFcst_WK+7'] + doidf['DFcst_WK+8'] + \
                                 doidf['DFcst_WK+9']
    doidf['Projections_+2_+3'] = doidf['DFcst_WK+2'] + doidf['DFcst_WK+3']
    doidf['Projections_+2_+4'] = doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4']
    doidf['Projections_+2_+5'] = doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5']
    doidf['Projections_+2_+6'] = doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + \
                                 doidf['DFcst_WK+6']
    doidf['Projections_+2_+7'] = doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + \
                                 doidf['DFcst_WK+6'] + doidf['DFcst_WK+7']
    doidf['Projections_+2_+8'] = doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + \
                                 doidf['DFcst_WK+6'] + doidf['DFcst_WK+7'] + doidf['DFcst_WK+8']
    doidf['Projections_+2_+9'] = doidf['DFcst_WK+2'] + doidf['DFcst_WK+3'] + doidf['DFcst_WK+4'] + doidf['DFcst_WK+5'] + \
                                 doidf['DFcst_WK+6'] + doidf['DFcst_WK+7'] + doidf['DFcst_WK+8'] + doidf['DFcst_WK+9']
    # create distributor forecast series for DOI calculations
    doidf['DFcst_Avg'] = doidf['Projections_+0_+9'] / 10
    # create execution results data series
    doidf['4WK Fcst Avg'] = doidf['Projections_+2_+5'] / 4
    doidf['6WK STR Total'] = doidf['STRs_WK-6'] + doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf[
        'STRs_WK-2'] + doidf['STRs_WK-1']

    # calculate 4 week str average
    fourwkstr_condis = [doidf['STRs_WK-1'].sum() == 0, doidf['STRs_WK-1'].sum() > 0]
    fourwkstr_optis = [(doidf['STRs_WK-5'] + doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2'] / 4),
                       (doidf['STRs_WK-4'] + doidf['STRs_WK-3'] + doidf['STRs_WK-2'] + doidf['STRs_WK-1'] / 4)]
    doidf['4WK STR Avg'] = np.select(fourwkstr_condis, fourwkstr_optis, default=0)
    # create list of conditions to calculate starting DOI
    doicondis_m6 = [(doidf['Beg_Inv_WK-6'] <= doidf['STRs_WK-6']),
                    (doidf['Beg_Inv_WK-6'] <= doidf['Projections_-6_-5']),
                    (doidf['Beg_Inv_WK-6'] <= doidf['Projections_-6_-4']),
                    (doidf['Beg_Inv_WK-6'] <= doidf['Projections_-6_-3']),
                    (doidf['Beg_Inv_WK-6'] <= doidf['Projections_-6_-2']),
                    (doidf['Beg_Inv_WK-6'] <= doidf['Projections_-6_-1']),
                    (doidf['Beg_Inv_WK-6'] <= doidf['Projections_-6_+0']),
                    (doidf['Beg_Inv_WK-6'] <= doidf['Projections_-6_+1']),
                    (doidf['Beg_Inv_WK-6'] <= doidf['Projections_-6_+2']),
                    (doidf['Beg_Inv_WK-6'] <= doidf['Projections_-6_+3']),
                    (doidf['Beg_Inv_WK-6'] <= doidf['Projections_-6_+4']),
                    (doidf['Beg_Inv_WK-6'] <= doidf['Projections_-6_+5']),
                    (doidf['Beg_Inv_WK-6'] <= doidf['Projections_-6_+6']),
                    (doidf['Beg_Inv_WK-6'] <= doidf['Projections_-6_+7']),
                    (doidf['Beg_Inv_WK-6'] <= doidf['Projections_-6_+8']),
                    (doidf['Beg_Inv_WK-6'] <= doidf['Projections_-6_+9']),
                    (doidf['Beg_Inv_WK-6'] > doidf['Projections_+2_+9']), ]
    doicondis_m5 = [(doidf['Beg_Inv_WK-5'] <= doidf['STRs_WK-5']),
                    (doidf['Beg_Inv_WK-5'] <= doidf['Projections_-5_-4']),
                    (doidf['Beg_Inv_WK-5'] <= doidf['Projections_-5_-3']),
                    (doidf['Beg_Inv_WK-5'] <= doidf['Projections_-5_-2']),
                    (doidf['Beg_Inv_WK-5'] <= doidf['Projections_-5_-1']),
                    (doidf['Beg_Inv_WK-5'] <= doidf['Projections_-5_+0']),
                    (doidf['Beg_Inv_WK-5'] <= doidf['Projections_-5_+1']),
                    (doidf['Beg_Inv_WK-5'] <= doidf['Projections_-5_+2']),
                    (doidf['Beg_Inv_WK-5'] <= doidf['Projections_-5_+3']),
                    (doidf['Beg_Inv_WK-5'] <= doidf['Projections_-5_+4']),
                    (doidf['Beg_Inv_WK-5'] <= doidf['Projections_-5_+5']),
                    (doidf['Beg_Inv_WK-5'] <= doidf['Projections_-5_+6']),
                    (doidf['Beg_Inv_WK-5'] <= doidf['Projections_-5_+7']),
                    (doidf['Beg_Inv_WK-5'] <= doidf['Projections_-5_+8']),
                    (doidf['Beg_Inv_WK-5'] <= doidf['Projections_-5_+9']),
                    (doidf['Beg_Inv_WK-5'] > doidf['Projections_+2_+9']), ]
    doicondis_m4 = [(doidf['Beg_Inv_WK-4'] <= doidf['STRs_WK-4']),
                    (doidf['Beg_Inv_WK-4'] <= doidf['Projections_-4_-3']),
                    (doidf['Beg_Inv_WK-4'] <= doidf['Projections_-4_-2']),
                    (doidf['Beg_Inv_WK-4'] <= doidf['Projections_-4_-1']),
                    (doidf['Beg_Inv_WK-4'] <= doidf['Projections_-4_+0']),
                    (doidf['Beg_Inv_WK-4'] <= doidf['Projections_-4_+1']),
                    (doidf['Beg_Inv_WK-4'] <= doidf['Projections_-4_+2']),
                    (doidf['Beg_Inv_WK-4'] <= doidf['Projections_-4_+3']),
                    (doidf['Beg_Inv_WK-4'] <= doidf['Projections_-4_+4']),
                    (doidf['Beg_Inv_WK-4'] <= doidf['Projections_-4_+5']),
                    (doidf['Beg_Inv_WK-4'] <= doidf['Projections_-4_+6']),
                    (doidf['Beg_Inv_WK-4'] <= doidf['Projections_-4_+7']),
                    (doidf['Beg_Inv_WK-4'] <= doidf['Projections_-4_+8']),
                    (doidf['Beg_Inv_WK-4'] <= doidf['Projections_-4_+9']),
                    (doidf['Beg_Inv_WK-4'] > doidf['Projections_+2_+9']), ]
    doicondis_m3 = [(doidf['Beg_Inv_WK-3'] <= doidf['STRs_WK-3']),
                    (doidf['Beg_Inv_WK-3'] <= doidf['Projections_-3_-2']),
                    (doidf['Beg_Inv_WK-3'] <= doidf['Projections_-3_-1']),
                    (doidf['Beg_Inv_WK-3'] <= doidf['Projections_-3_+0']),
                    (doidf['Beg_Inv_WK-3'] <= doidf['Projections_-3_+1']),
                    (doidf['Beg_Inv_WK-3'] <= doidf['Projections_-3_+2']),
                    (doidf['Beg_Inv_WK-3'] <= doidf['Projections_-3_+3']),
                    (doidf['Beg_Inv_WK-3'] <= doidf['Projections_-3_+4']),
                    (doidf['Beg_Inv_WK-3'] <= doidf['Projections_-3_+5']),
                    (doidf['Beg_Inv_WK-3'] <= doidf['Projections_-3_+6']),
                    (doidf['Beg_Inv_WK-3'] <= doidf['Projections_-3_+7']),
                    (doidf['Beg_Inv_WK-3'] <= doidf['Projections_-3_+8']),
                    (doidf['Beg_Inv_WK-3'] <= doidf['Projections_-3_+9']),
                    (doidf['Beg_Inv_WK-3'] > doidf['Projections_+2_+9']), ]
    doicondis_m2 = [(doidf['Beg_Inv_WK-2'] <= doidf['STRs_WK-2']),
                    (doidf['Beg_Inv_WK-2'] <= doidf['Projections_-2_-1']),
                    (doidf['Beg_Inv_WK-2'] <= doidf['Projections_-2_+0']),
                    (doidf['Beg_Inv_WK-2'] <= doidf['Projections_-2_+1']),
                    (doidf['Beg_Inv_WK-2'] <= doidf['Projections_-2_+2']),
                    (doidf['Beg_Inv_WK-2'] <= doidf['Projections_-2_+3']),
                    (doidf['Beg_Inv_WK-2'] <= doidf['Projections_-2_+4']),
                    (doidf['Beg_Inv_WK-2'] <= doidf['Projections_-2_+5']),
                    (doidf['Beg_Inv_WK-2'] <= doidf['Projections_-2_+6']),
                    (doidf['Beg_Inv_WK-2'] <= doidf['Projections_-2_+7']),
                    (doidf['Beg_Inv_WK-2'] <= doidf['Projections_-2_+8']),
                    (doidf['Beg_Inv_WK-2'] <= doidf['Projections_-2_+9']),
                    (doidf['Beg_Inv_WK-2'] > doidf['Projections_+2_+9']), ]
    doicondis_m1 = [(doidf['Beg_Inv_WK-1'] <= doidf['STRs_WK-1']),
                    (doidf['Beg_Inv_WK-1'] <= doidf['Projections_-1_+0']),
                    (doidf['Beg_Inv_WK-1'] <= doidf['Projections_-1_+1']),
                    (doidf['Beg_Inv_WK-1'] <= doidf['Projections_-1_+2']),
                    (doidf['Beg_Inv_WK-1'] <= doidf['Projections_-1_+3']),
                    (doidf['Beg_Inv_WK-1'] <= doidf['Projections_-1_+4']),
                    (doidf['Beg_Inv_WK-1'] <= doidf['Projections_-1_+5']),
                    (doidf['Beg_Inv_WK-1'] <= doidf['Projections_-1_+6']),
                    (doidf['Beg_Inv_WK-1'] <= doidf['Projections_-1_+7']),
                    (doidf['Beg_Inv_WK-1'] <= doidf['Projections_-1_+8']),
                    (doidf['Beg_Inv_WK-1'] <= doidf['Projections_-1_+9']),
                    (doidf['Beg_Inv_WK-1'] > doidf['Projections_+2_+9']), ]
    doicondis_p0 = [(doidf['Beg_Inv_WK+0'] <= doidf['DFcst_WK+0']),
                    (doidf['Beg_Inv_WK+0'] <= doidf['Projections_+0_+1']),
                    (doidf['Beg_Inv_WK+0'] <= doidf['Projections_+0_+2']),
                    (doidf['Beg_Inv_WK+0'] <= doidf['Projections_+0_+3']),
                    (doidf['Beg_Inv_WK+0'] <= doidf['Projections_+0_+4']),
                    (doidf['Beg_Inv_WK+0'] <= doidf['Projections_+0_+5']),
                    (doidf['Beg_Inv_WK+0'] <= doidf['Projections_+0_+6']),
                    (doidf['Beg_Inv_WK+0'] <= doidf['Projections_+0_+7']),
                    (doidf['Beg_Inv_WK+0'] <= doidf['Projections_+0_+8']),
                    (doidf['Beg_Inv_WK+0'] <= doidf['Projections_+0_+9']),
                    (doidf['Beg_Inv_WK+0'] > doidf['Projections_+2_+9']), ]
    doicondis_p1 = [(doidf['Beg_Inv_WK+1'] <= doidf['DFcst_WK+1']),
                    (doidf['Beg_Inv_WK+1'] <= doidf['Projections_+1_+2']),
                    (doidf['Beg_Inv_WK+1'] <= doidf['Projections_+1_+3']),
                    (doidf['Beg_Inv_WK+1'] <= doidf['Projections_+1_+4']),
                    (doidf['Beg_Inv_WK+1'] <= doidf['Projections_+1_+5']),
                    (doidf['Beg_Inv_WK+1'] <= doidf['Projections_+1_+6']),
                    (doidf['Beg_Inv_WK+1'] <= doidf['Projections_+1_+7']),
                    (doidf['Beg_Inv_WK+1'] <= doidf['Projections_+1_+8']),
                    (doidf['Beg_Inv_WK+1'] <= doidf['Projections_+1_+9']),
                    (doidf['Beg_Inv_WK+1'] > doidf['Projections_+2_+9']), ]
    doicondis_p2 = [(doidf['Beg_Inv_WK+2'] <= doidf['DFcst_WK+2']),
                    (doidf['Beg_Inv_WK+2'] <= doidf['Projections_+2_+3']),
                    (doidf['Beg_Inv_WK+2'] <= doidf['Projections_+2_+4']),
                    (doidf['Beg_Inv_WK+2'] <= doidf['Projections_+2_+5']),
                    (doidf['Beg_Inv_WK+2'] <= doidf['Projections_+2_+6']),
                    (doidf['Beg_Inv_WK+2'] <= doidf['Projections_+2_+7']),
                    (doidf['Beg_Inv_WK+2'] <= doidf['Projections_+2_+8']),
                    (doidf['Beg_Inv_WK+2'] <= doidf['Projections_+2_+9']),
                    (doidf['Beg_Inv_WK+2'] > doidf['Projections_+2_+9']), ]
    # create list of starting DOI calculation options
    doioptins_m6 = [round(0 + (doidf['Beg_Inv_WK-6'] / doidf['STRs_WK-6']) * 7),
                    round(7 + ((doidf['Beg_Inv_WK-6'] - doidf['STRs_WK-6']) / doidf['STRs_WK-5']) * 7),
                    round(14 + ((doidf['Beg_Inv_WK-6'] - doidf['Projections_-6_-5']) / doidf['STRs_WK-4']) * 7),
                    round(21 + ((doidf['Beg_Inv_WK-6'] - doidf['Projections_-6_-4']) / doidf['STRs_WK-3']) * 7),
                    round(28 + ((doidf['Beg_Inv_WK-6'] - doidf['Projections_-6_-3']) / doidf['STRs_WK-2']) * 7),
                    round(35 + ((doidf['Beg_Inv_WK-6'] - doidf['Projections_-6_-2']) / doidf['STRs_WK-1']) * 7),
                    round(42 + ((doidf['Beg_Inv_WK-6'] - doidf['Projections_-6_-1']) / doidf['DFcst_WK+0']) * 7),
                    round(49 + ((doidf['Beg_Inv_WK-6'] - doidf['Projections_-6_+0']) / doidf['DFcst_WK+1']) * 7),
                    round(56 + ((doidf['Beg_Inv_WK-6'] - doidf['Projections_-6_+1']) / doidf['DFcst_WK+2']) * 7),
                    round(63 + ((doidf['Beg_Inv_WK-6'] - doidf['Projections_-6_+2']) / doidf['DFcst_WK+3']) * 7),
                    round(77 + ((doidf['Beg_Inv_WK-6'] - doidf['Projections_-6_+3']) / doidf['DFcst_WK+4']) * 7),
                    round(84 + ((doidf['Beg_Inv_WK-6'] - doidf['Projections_-6_+4']) / doidf['DFcst_WK+5']) * 7),
                    round(91 + ((doidf['Beg_Inv_WK-6'] - doidf['Projections_-6_+5']) / doidf['DFcst_WK+6']) * 7),
                    round(98 + ((doidf['Beg_Inv_WK-6'] - doidf['Projections_-6_+6']) / doidf['DFcst_WK+7']) * 7),
                    round(105 + ((doidf['Beg_Inv_WK-6'] - doidf['Projections_-6_+7']) / doidf['DFcst_WK+8']) * 7),
                    round(112 + ((doidf['Beg_Inv_WK-6'] - doidf['Projections_-6_+8']) / doidf['DFcst_WK+9']) * 7),
                    round(119 + ((doidf['Beg_Inv_WK-5'] - doidf['Projections_-5_+9']) / doidf['DFcst_Avg']) * 7)]
    doioptins_m5 = [round(0 + (doidf['Beg_Inv_WK-5'] / doidf['STRs_WK-5']) * 7),
                    round(7 + ((doidf['Beg_Inv_WK-5'] - doidf['STRs_WK-5']) / doidf['STRs_WK-4']) * 7),
                    round(14 + ((doidf['Beg_Inv_WK-5'] - doidf['Projections_-5_-4']) / doidf['STRs_WK-3']) * 7),
                    round(21 + ((doidf['Beg_Inv_WK-5'] - doidf['Projections_-5_-3']) / doidf['STRs_WK-2']) * 7),
                    round(28 + ((doidf['Beg_Inv_WK-5'] - doidf['Projections_-5_-2']) / doidf['STRs_WK-1']) * 7),
                    round(35 + ((doidf['Beg_Inv_WK-5'] - doidf['Projections_-5_-1']) / doidf['DFcst_WK+0']) * 7),
                    round(42 + ((doidf['Beg_Inv_WK-5'] - doidf['Projections_-5_+0']) / doidf['DFcst_WK+1']) * 7),
                    round(49 + ((doidf['Beg_Inv_WK-5'] - doidf['Projections_-5_+1']) / doidf['DFcst_WK+2']) * 7),
                    round(56 + ((doidf['Beg_Inv_WK-5'] - doidf['Projections_-5_+2']) / doidf['DFcst_WK+3']) * 7),
                    round(63 + ((doidf['Beg_Inv_WK-5'] - doidf['Projections_-5_+3']) / doidf['DFcst_WK+4']) * 7),
                    round(77 + ((doidf['Beg_Inv_WK-5'] - doidf['Projections_-5_+4']) / doidf['DFcst_WK+5']) * 7),
                    round(84 + ((doidf['Beg_Inv_WK-5'] - doidf['Projections_-5_+5']) / doidf['DFcst_WK+6']) * 7),
                    round(91 + ((doidf['Beg_Inv_WK-5'] - doidf['Projections_-5_+6']) / doidf['DFcst_WK+7']) * 7),
                    round(98 + ((doidf['Beg_Inv_WK-5'] - doidf['Projections_-5_+7']) / doidf['DFcst_WK+8']) * 7),
                    round(105 + ((doidf['Beg_Inv_WK-5'] - doidf['Projections_-5_+8']) / doidf['DFcst_WK+9']) * 7),
                    round(112 + ((doidf['Beg_Inv_WK-5'] - doidf['Projections_-5_+9']) / doidf['DFcst_Avg']) * 7)]
    doioptins_m4 = [round(0 + (doidf['Beg_Inv_WK-4'] / doidf['STRs_WK-4']) * 7),
                    round(7 + ((doidf['Beg_Inv_WK-4'] - doidf['STRs_WK-4']) / doidf['STRs_WK-3']) * 7),
                    round(14 + ((doidf['Beg_Inv_WK-4'] - doidf['Projections_-4_-3']) / doidf['STRs_WK-2']) * 7),
                    round(21 + ((doidf['Beg_Inv_WK-4'] - doidf['Projections_-4_-2']) / doidf['STRs_WK-1']) * 7),
                    round(28 + ((doidf['Beg_Inv_WK-4'] - doidf['Projections_-4_-1']) / doidf['DFcst_WK+0']) * 7),
                    round(35 + ((doidf['Beg_Inv_WK-4'] - doidf['Projections_-4_+0']) / doidf['DFcst_WK+1']) * 7),
                    round(42 + ((doidf['Beg_Inv_WK-4'] - doidf['Projections_-4_+1']) / doidf['DFcst_WK+2']) * 7),
                    round(49 + ((doidf['Beg_Inv_WK-4'] - doidf['Projections_-4_+2']) / doidf['DFcst_WK+3']) * 7),
                    round(56 + ((doidf['Beg_Inv_WK-4'] - doidf['Projections_-4_+3']) / doidf['DFcst_WK+4']) * 7),
                    round(63 + ((doidf['Beg_Inv_WK-4'] - doidf['Projections_-4_+4']) / doidf['DFcst_WK+5']) * 7),
                    round(77 + ((doidf['Beg_Inv_WK-4'] - doidf['Projections_-4_+5']) / doidf['DFcst_WK+6']) * 7),
                    round(84 + ((doidf['Beg_Inv_WK-4'] - doidf['Projections_-4_+6']) / doidf['DFcst_WK+7']) * 7),
                    round(91 + ((doidf['Beg_Inv_WK-4'] - doidf['Projections_-4_+7']) / doidf['DFcst_WK+8']) * 7),
                    round(98 + ((doidf['Beg_Inv_WK-4'] - doidf['Projections_-4_+8']) / doidf['DFcst_WK+9']) * 7),
                    round(105 + ((doidf['Beg_Inv_WK-4'] - doidf['Projections_-4_+9']) / doidf['DFcst_Avg']) * 7)]
    doioptins_m3 = [round(0 + (doidf['Beg_Inv_WK-3'] / doidf['STRs_WK-3']) * 7),
                    round(7 + ((doidf['Beg_Inv_WK-3'] - doidf['STRs_WK-3']) / doidf['STRs_WK-2']) * 7),
                    round(14 + ((doidf['Beg_Inv_WK-3'] - doidf['Projections_-3_-2']) / doidf['STRs_WK-1']) * 7),
                    round(21 + ((doidf['Beg_Inv_WK-3'] - doidf['Projections_-3_-1']) / doidf['DFcst_WK+0']) * 7),
                    round(28 + ((doidf['Beg_Inv_WK-3'] - doidf['Projections_-3_+0']) / doidf['DFcst_WK+1']) * 7),
                    round(35 + ((doidf['Beg_Inv_WK-3'] - doidf['Projections_-3_+1']) / doidf['DFcst_WK+2']) * 7),
                    round(42 + ((doidf['Beg_Inv_WK-3'] - doidf['Projections_-3_+2']) / doidf['DFcst_WK+3']) * 7),
                    round(49 + ((doidf['Beg_Inv_WK-3'] - doidf['Projections_-3_+3']) / doidf['DFcst_WK+4']) * 7),
                    round(56 + ((doidf['Beg_Inv_WK-3'] - doidf['Projections_-3_+4']) / doidf['DFcst_WK+5']) * 7),
                    round(63 + ((doidf['Beg_Inv_WK-3'] - doidf['Projections_-3_+5']) / doidf['DFcst_WK+6']) * 7),
                    round(77 + ((doidf['Beg_Inv_WK-3'] - doidf['Projections_-3_+6']) / doidf['DFcst_WK+7']) * 7),
                    round(84 + ((doidf['Beg_Inv_WK-3'] - doidf['Projections_-3_+7']) / doidf['DFcst_WK+8']) * 7),
                    round(91 + ((doidf['Beg_Inv_WK-3'] - doidf['Projections_-3_+8']) / doidf['DFcst_WK+9']) * 7),
                    round(98 + ((doidf['Beg_Inv_WK-3'] - doidf['Projections_-3_+9']) / doidf['DFcst_Avg']) * 7)]
    doioptins_m2 = [round(0 + (doidf['Beg_Inv_WK-2'] / doidf['STRs_WK-2']) * 7),
                    round(7 + ((doidf['Beg_Inv_WK-2'] - doidf['STRs_WK-2']) / doidf['STRs_WK-1']) * 7),
                    round(14 + ((doidf['Beg_Inv_WK-2'] - doidf['Projections_-2_-1']) / doidf['DFcst_WK+0']) * 7),
                    round(21 + ((doidf['Beg_Inv_WK-2'] - doidf['Projections_-2_+0']) / doidf['DFcst_WK+1']) * 7),
                    round(28 + ((doidf['Beg_Inv_WK-2'] - doidf['Projections_-2_+1']) / doidf['DFcst_WK+2']) * 7),
                    round(35 + ((doidf['Beg_Inv_WK-2'] - doidf['Projections_-2_+2']) / doidf['DFcst_WK+3']) * 7),
                    round(42 + ((doidf['Beg_Inv_WK-2'] - doidf['Projections_-2_+3']) / doidf['DFcst_WK+4']) * 7),
                    round(49 + ((doidf['Beg_Inv_WK-2'] - doidf['Projections_-2_+4']) / doidf['DFcst_WK+5']) * 7),
                    round(56 + ((doidf['Beg_Inv_WK-2'] - doidf['Projections_-2_+5']) / doidf['DFcst_WK+6']) * 7),
                    round(63 + ((doidf['Beg_Inv_WK-2'] - doidf['Projections_-2_+6']) / doidf['DFcst_WK+7']) * 7),
                    round(77 + ((doidf['Beg_Inv_WK-2'] - doidf['Projections_-2_+7']) / doidf['DFcst_WK+8']) * 7),
                    round(84 + ((doidf['Beg_Inv_WK-2'] - doidf['Projections_-2_+8']) / doidf['DFcst_WK+9']) * 7),
                    round(91 + ((doidf['Beg_Inv_WK-2'] - doidf['Projections_-2_+9']) / doidf['DFcst_Avg']) * 7)]
    doioptins_m1 = [round(0 + (doidf['Beg_Inv_WK-1'] / doidf['STRs_WK-1']) * 7),
                    round(7 + ((doidf['Beg_Inv_WK-1'] - doidf['STRs_WK-1']) / doidf['DFcst_WK+0']) * 7),
                    round(14 + ((doidf['Beg_Inv_WK-1'] - doidf['Projections_-1_+0']) / doidf['DFcst_WK+1']) * 7),
                    round(21 + ((doidf['Beg_Inv_WK-1'] - doidf['Projections_-1_+1']) / doidf['DFcst_WK+2']) * 7),
                    round(28 + ((doidf['Beg_Inv_WK-1'] - doidf['Projections_-1_+2']) / doidf['DFcst_WK+3']) * 7),
                    round(35 + ((doidf['Beg_Inv_WK-1'] - doidf['Projections_-1_+3']) / doidf['DFcst_WK+4']) * 7),
                    round(42 + ((doidf['Beg_Inv_WK-1'] - doidf['Projections_-1_+4']) / doidf['DFcst_WK+5']) * 7),
                    round(49 + ((doidf['Beg_Inv_WK-1'] - doidf['Projections_-1_+5']) / doidf['DFcst_WK+6']) * 7),
                    round(56 + ((doidf['Beg_Inv_WK-1'] - doidf['Projections_-1_+6']) / doidf['DFcst_WK+7']) * 7),
                    round(63 + ((doidf['Beg_Inv_WK-1'] - doidf['Projections_-1_+7']) / doidf['DFcst_WK+8']) * 7),
                    round(77 + ((doidf['Beg_Inv_WK-1'] - doidf['Projections_-1_+8']) / doidf['DFcst_WK+9']) * 7),
                    round(84 + ((doidf['Beg_Inv_WK-1'] - doidf['Projections_-1_+9']) / doidf['DFcst_Avg']) * 7)]
    doioptins_p0 = [round(0 + (doidf['Beg_Inv_WK+0'] / doidf['DFcst_WK+0']) * 7),
                    round(7 + ((doidf['Beg_Inv_WK+0'] - doidf['DFcst_WK+0']) / doidf['DFcst_WK+1']) * 7),
                    round(14 + ((doidf['Beg_Inv_WK+0'] - doidf['Projections_+0_+1']) / doidf['DFcst_WK+2']) * 7),
                    round(21 + ((doidf['Beg_Inv_WK+0'] - doidf['Projections_+0_+2']) / doidf['DFcst_WK+3']) * 7),
                    round(28 + ((doidf['Beg_Inv_WK+0'] - doidf['Projections_+0_+3']) / doidf['DFcst_WK+4']) * 7),
                    round(35 + ((doidf['Beg_Inv_WK+0'] - doidf['Projections_+0_+4']) / doidf['DFcst_WK+5']) * 7),
                    round(42 + ((doidf['Beg_Inv_WK+0'] - doidf['Projections_+0_+5']) / doidf['DFcst_WK+6']) * 7),
                    round(49 + ((doidf['Beg_Inv_WK+0'] - doidf['Projections_+0_+6']) / doidf['DFcst_WK+7']) * 7),
                    round(56 + ((doidf['Beg_Inv_WK+0'] - doidf['Projections_+0_+7']) / doidf['DFcst_WK+8']) * 7),
                    round(63 + ((doidf['Beg_Inv_WK+0'] - doidf['Projections_+0_+8']) / doidf['DFcst_WK+9']) * 7),
                    round(77 + ((doidf['Beg_Inv_WK+0'] - doidf['Projections_+0_+9']) / doidf['DFcst_Avg']) * 7)]
    doioptins_p1 = [round(0 + (doidf['Beg_Inv_WK+1'] / doidf['DFcst_WK+1']) * 7),
                    round(7 + ((doidf['Beg_Inv_WK+1'] - doidf['DFcst_WK+1']) / doidf['DFcst_WK+2']) * 7),
                    round(14 + ((doidf['Beg_Inv_WK+1'] - doidf['Projections_+1_+2']) / doidf['DFcst_WK+3']) * 7),
                    round(21 + ((doidf['Beg_Inv_WK+1'] - doidf['Projections_+1_+3']) / doidf['DFcst_WK+4']) * 7),
                    round(28 + ((doidf['Beg_Inv_WK+1'] - doidf['Projections_+1_+4']) / doidf['DFcst_WK+5']) * 7),
                    round(35 + ((doidf['Beg_Inv_WK+1'] - doidf['Projections_+1_+5']) / doidf['DFcst_WK+6']) * 7),
                    round(42 + ((doidf['Beg_Inv_WK+1'] - doidf['Projections_+1_+6']) / doidf['DFcst_WK+7']) * 7),
                    round(49 + ((doidf['Beg_Inv_WK+1'] - doidf['Projections_+1_+7']) / doidf['DFcst_WK+8']) * 7),
                    round(56 + ((doidf['Beg_Inv_WK+1'] - doidf['Projections_+1_+8']) / doidf['DFcst_WK+9']) * 7),
                    round(63 + ((doidf['Beg_Inv_WK+1'] - doidf['Projections_+1_+9']) / doidf['DFcst_Avg']) * 7)]
    doioptins_p2 = [round(0 + (doidf['Beg_Inv_WK+2'] / doidf['DFcst_WK+2']) * 7),
                    round(7 + ((doidf['Beg_Inv_WK+2'] - doidf['DFcst_WK+2']) / doidf['DFcst_WK+3']) * 7),
                    round(14 + ((doidf['Beg_Inv_WK+2'] - doidf['Projections_+2_+3']) / doidf['DFcst_WK+4']) * 7),
                    round(21 + ((doidf['Beg_Inv_WK+2'] - doidf['Projections_+2_+4']) / doidf['DFcst_WK+5']) * 7),
                    round(28 + ((doidf['Beg_Inv_WK+2'] - doidf['Projections_+2_+5']) / doidf['DFcst_WK+6']) * 7),
                    round(35 + ((doidf['Beg_Inv_WK+2'] - doidf['Projections_+2_+6']) / doidf['DFcst_WK+7']) * 7),
                    round(42 + ((doidf['Beg_Inv_WK+2'] - doidf['Projections_+2_+7']) / doidf['DFcst_WK+8']) * 7),
                    round(49 + ((doidf['Beg_Inv_WK+2'] - doidf['Projections_+2_+8']) / doidf['DFcst_WK+9']) * 7),
                    round(56 + ((doidf['Beg_Inv_WK+2'] - doidf['Projections_+2_+9']) / doidf['DFcst_Avg']) * 7)]
    # create new series based on numpy select conditions
    doidf['Starting DOI WK-6'] = np.select(doicondis_m6, doioptins_m6, default=0)
    doidf['Starting DOI WK-5'] = np.select(doicondis_m5, doioptins_m5, default=0)
    doidf['Starting DOI WK-4'] = np.select(doicondis_m4, doioptins_m4, default=0)
    doidf['Starting DOI WK-3'] = np.select(doicondis_m3, doioptins_m3, default=0)
    doidf['Starting DOI WK-2'] = np.select(doicondis_m2, doioptins_m2, default=0)
    doidf['Starting DOI WK-1'] = np.select(doicondis_m1, doioptins_m1, default=0)
    doidf['Starting DOI WK+0'] = np.select(doicondis_p0, doioptins_p0, default=0)
    doidf['Starting DOI WK+1'] = np.select(doicondis_p1, doioptins_p1, default=0)
    doidf['Starting DOI WK+2'] = np.select(doicondis_p2, doioptins_p2, default=0)
    # create list of conditions to calculate ordered DOI
    doicondis_orders = [
        (doidf['Beg_Inv_WK+2'] + doidf['Orders_WK+2'] <= doidf['DFcst_WK+2']),
        (doidf['Beg_Inv_WK+2'] + doidf['Orders_WK+2'] <= doidf['Projections_+2_+3']),
        (doidf['Beg_Inv_WK+2'] + doidf['Orders_WK+2'] <= doidf['Projections_+2_+4']),
        (doidf['Beg_Inv_WK+2'] + doidf['Orders_WK+2'] <= doidf['Projections_+2_+5']),
        (doidf['Beg_Inv_WK+2'] + doidf['Orders_WK+2'] <= doidf['Projections_+2_+6']),
        (doidf['Beg_Inv_WK+2'] + doidf['Orders_WK+2'] <= doidf['Projections_+2_+7']),
        (doidf['Beg_Inv_WK+2'] + doidf['Orders_WK+2'] <= doidf['Projections_+2_+8']),
        (doidf['Beg_Inv_WK+2'] + doidf['Orders_WK+2'] <= doidf['Projections_+2_+9']),
        (doidf['Beg_Inv_WK+2'] + doidf['Orders_WK+2'] > doidf['Projections_+2_+9']),
    ]
    # create list of starting DOI calculation options
    doichoices_orders = [
        round((0 + ((doidf['Beg_Inv_WK+2'] + doidf['Orders_WK+2'] / doidf['DFcst_WK+2']) * 7))),
        round((7 + (((doidf['Beg_Inv_WK+2'] + doidf['Orders_WK+2'] - doidf['DFcst_WK+2']) / doidf['DFcst_WK+3']) * 7))),
        round((14 + ((doidf['Beg_Inv_WK+2'] + doidf['Orders_WK+2'] - doidf['Projections_+2_+3']) / doidf[
            'DFcst_WK+4'] * 7))),
        round((21 + ((doidf['Beg_Inv_WK+2'] + doidf['Orders_WK+2'] - doidf['Projections_+2_+4']) / doidf[
            'DFcst_WK+5'] * 7))),
        round((28 + ((doidf['Beg_Inv_WK+2'] + doidf['Orders_WK+2'] - doidf['Projections_+2_+5']) / doidf[
            'DFcst_WK+6'] * 7))),
        round((35 + ((doidf['Beg_Inv_WK+2'] + doidf['Orders_WK+2'] - doidf['Projections_+2_+6']) / doidf[
            'DFcst_WK+7'] * 7))),
        round((42 + ((doidf['Beg_Inv_WK+2'] + doidf['Orders_WK+2'] - doidf['Projections_+2_+7']) / doidf[
            'DFcst_WK+8'] * 7))),
        round((49 + ((doidf['Beg_Inv_WK+2'] + doidf['Orders_WK+2'] - doidf['Projections_+2_+8']) / doidf[
            'DFcst_WK+9'] * 7))),
        round((56 + ((doidf['Beg_Inv_WK+2'] + doidf['Orders_WK+2'] - doidf['Projections_+2_+9']) / doidf[
            'DFcst_Avg'] * 7))),
    ]
    # create new series based on numpy select conditions
    doidf['Ordered DOI'] = np.select(doicondis_orders, doichoices_orders, default=0)
    # create list of conditions to calculate ordered DOI
    doicondis_conf = [
        (doidf['Beg_Inv_WK+2'] + doidf['Curr_Conf_WK+2'] <= doidf['DFcst_WK+2']),
        (doidf['Beg_Inv_WK+2'] + doidf['Curr_Conf_WK+2'] <= doidf['Projections_+2_+3']),
        (doidf['Beg_Inv_WK+2'] + doidf['Curr_Conf_WK+2'] <= doidf['Projections_+2_+4']),
        (doidf['Beg_Inv_WK+2'] + doidf['Curr_Conf_WK+2'] <= doidf['Projections_+2_+5']),
        (doidf['Beg_Inv_WK+2'] + doidf['Curr_Conf_WK+2'] <= doidf['Projections_+2_+6']),
        (doidf['Beg_Inv_WK+2'] + doidf['Curr_Conf_WK+2'] <= doidf['Projections_+2_+7']),
        (doidf['Beg_Inv_WK+2'] + doidf['Curr_Conf_WK+2'] <= doidf['Projections_+2_+8']),
        (doidf['Beg_Inv_WK+2'] + doidf['Curr_Conf_WK+2'] <= doidf['Projections_+2_+9']),
        (doidf['Beg_Inv_WK+2'] + doidf['Curr_Conf_WK+2'] > doidf['Projections_+2_+9']),
    ]
    # create list of starting DOI calculation options
    doichoices_conf = [
        round((0 + ((doidf['Beg_Inv_WK+2'] + doidf['Curr_Conf_WK+2'] / doidf['DFcst_WK+2']) * 7))),
        round(
            (7 + (((doidf['Beg_Inv_WK+2'] + doidf['Curr_Conf_WK+2'] - doidf['DFcst_WK+2']) / doidf[
                'DFcst_WK+3']) * 7))),
        round((14 + ((doidf['Beg_Inv_WK+2'] + doidf['Curr_Conf_WK+2'] - doidf['Projections_+2_+3']) / doidf[
            'DFcst_WK+4'] * 7))),
        round((21 + ((doidf['Beg_Inv_WK+2'] + doidf['Curr_Conf_WK+2'] - doidf['Projections_+2_+4']) / doidf[
            'DFcst_WK+5'] * 7))),
        round((28 + ((doidf['Beg_Inv_WK+2'] + doidf['Curr_Conf_WK+2'] - doidf['Projections_+2_+5']) / doidf[
            'DFcst_WK+6'] * 7))),
        round((35 + ((doidf['Beg_Inv_WK+2'] + doidf['Curr_Conf_WK+2'] - doidf['Projections_+2_+6']) / doidf[
            'DFcst_WK+7'] * 7))),
        round((42 + ((doidf['Beg_Inv_WK+2'] + doidf['Curr_Conf_WK+2'] - doidf['Projections_+2_+7']) / doidf[
            'DFcst_WK+8'] * 7))),
        round((49 + ((doidf['Beg_Inv_WK+2'] + doidf['Curr_Conf_WK+2'] - doidf['Projections_+2_+8']) / doidf[
            'DFcst_WK+9'] * 7))),
        round((56 + ((doidf['Beg_Inv_WK+2'] + doidf['Curr_Conf_WK+2'] - doidf['Projections_+2_+9']) / doidf[
            'DFcst_Avg'] * 7))),
    ]
    # create new series based on numpy select conditions
    doidf['Confirmed DOI'] = np.select(doicondis_conf, doichoices_conf, default=0)
    # replace infinite values with 99
    doidf = doidf.replace(np.inf, 99)
    # replace negative infinite and null values with 0
    doidf = doidf.replace([-np.inf, np.nan], 0)
    # round down DOIs above 99 to 99
    doidf['Starting DOI WK-6'] = np.where(doidf['Starting DOI WK-6'] > 99, 99, doidf['Starting DOI WK-6'])
    doidf['Starting DOI WK-5'] = np.where(doidf['Starting DOI WK-5'] > 99, 99, doidf['Starting DOI WK-5'])
    doidf['Starting DOI WK-4'] = np.where(doidf['Starting DOI WK-4'] > 99, 99, doidf['Starting DOI WK-4'])
    doidf['Starting DOI WK-3'] = np.where(doidf['Starting DOI WK-3'] > 99, 99, doidf['Starting DOI WK-3'])
    doidf['Starting DOI WK-2'] = np.where(doidf['Starting DOI WK-2'] > 99, 99, doidf['Starting DOI WK-2'])
    doidf['Starting DOI WK-1'] = np.where(doidf['Starting DOI WK-1'] > 99, 99, doidf['Starting DOI WK-1'])
    doidf['Starting DOI WK+0'] = np.where(doidf['Starting DOI WK+0'] > 99, 99, doidf['Starting DOI WK+0'])
    doidf['Starting DOI WK+1'] = np.where(doidf['Starting DOI WK+1'] > 99, 99, doidf['Starting DOI WK+1'])
    doidf['Starting DOI WK+2'] = np.where(doidf['Starting DOI WK+2'] > 99, 99, doidf['Starting DOI WK+2'])
    doidf['Ordered DOI'] = np.where(doidf['Ordered DOI'] > 99, 99, doidf['Ordered DOI'])
    doidf['Confirmed DOI'] = np.where(doidf['Confirmed DOI'] > 99, 99, doidf['Confirmed DOI'])
    # create series that flags pairs with no forecasts
    doidf['Status'] = np.where((doidf['STRs_WK-6'] == 0) & (doidf['STRs_WK-5'] == 0) &
                               (doidf['STRs_WK-4'] == 0) & (doidf['STRs_WK-3'] == 0) &
                               (doidf['STRs_WK-2'] == 0) & (doidf['STRs_WK-1'] == 0) &
                               (doidf['DFcst_WK+0'] == 0) & (doidf['DFcst_WK+1'] == 0) &
                               (doidf['DFcst_WK+2'] == 0) & (doidf['DFcst_WK+3'] == 0) &
                               (doidf['DFcst_WK+4'] == 0) & (doidf['DFcst_WK+5'] == 0) &
                               (doidf['DFcst_WK+6'] == 0) & (doidf['DFcst_WK+7'] == 0) &
                               (doidf['DFcst_WK+8'] == 0) & (doidf['DFcst_WK+9'] == 0), 'INACTIVE', 'ACTIVE')
    # create series calculating impact by stage
    doidf['Order_Impact_WK-6'] = doidf['Orders_WK-6'] - doidf['Suggest_WK-6']
    doidf['Order_Impact_WK-5'] = doidf['Orders_WK-5'] - doidf['Suggest_WK-5']
    doidf['Order_Impact_WK-4'] = doidf['Orders_WK-4'] - doidf['Suggest_WK-4']
    doidf['Order_Impact_WK-3'] = doidf['Orders_WK-3'] - doidf['Suggest_WK-3']
    doidf['Order_Impact_WK-2'] = doidf['Orders_WK-2'] - doidf['Suggest_WK-2']
    doidf['Order_Impact_WK-1'] = doidf['Orders_WK-1'] - doidf['Suggest_WK-1']
    doidf['Order_Impact_WK+0'] = doidf['Orders_WK+0'] - doidf['Suggest_WK+0']
    doidf['Order_Impact_WK+1'] = doidf['Orders_WK+1'] - doidf['Suggest_WK+1']
    doidf['Order_Impact_WK+2'] = doidf['Orders_WK+2'] - doidf['Suggest_WK+2']
    doidf['Capacity_Impact_WK-6'] = doidf['Orig_Conf_WK-6'] - doidf['Orders_WK-6']
    doidf['Capacity_Impact_WK-5'] = doidf['Orig_Conf_WK-5'] - doidf['Orders_WK-5']
    doidf['Capacity_Impact_WK-4'] = doidf['Orig_Conf_WK-4'] - doidf['Orders_WK-4']
    doidf['Capacity_Impact_WK-3'] = doidf['Orig_Conf_WK-3'] - doidf['Orders_WK-3']
    doidf['Capacity_Impact_WK-2'] = doidf['Orig_Conf_WK-2'] - doidf['Orders_WK-2']
    doidf['Capacity_Impact_WK-1'] = doidf['Orig_Conf_WK-1'] - doidf['Orders_WK-1']
    doidf['Capacity_Impact_WK+0'] = doidf['Orig_Conf_WK+0'] - doidf['Orders_WK+0']
    doidf['Capacity_Impact_WK+1'] = doidf['Orig_Conf_WK+1'] - doidf['Orders_WK+1']
    doidf['Capacity_Impact_WK+2'] = doidf['Orig_Conf_WK+2'] - doidf['Orders_WK+2']
    doidf['Execution_Impact_WK-6'] = doidf['Curr_Conf_WK-6'] - doidf['Orig_Conf_WK-6']
    doidf['Execution_Impact_WK-5'] = doidf['Curr_Conf_WK-5'] - doidf['Orig_Conf_WK-5']
    doidf['Execution_Impact_WK-4'] = doidf['Curr_Conf_WK-4'] - doidf['Orig_Conf_WK-4']
    doidf['Execution_Impact_WK-3'] = doidf['Curr_Conf_WK-3'] - doidf['Orig_Conf_WK-3']
    doidf['Execution_Impact_WK-2'] = doidf['Curr_Conf_WK-2'] - doidf['Orig_Conf_WK-2']
    doidf['Execution_Impact_WK-1'] = doidf['Curr_Conf_WK-1'] - doidf['Orig_Conf_WK-1']
    doidf['Execution_Impact_WK+0'] = doidf['Curr_Conf_WK+0'] - doidf['Orig_Conf_WK+0']
    doidf['Execution_Impact_WK+1'] = doidf['Curr_Conf_WK+1'] - doidf['Orig_Conf_WK+1']
    doidf['Execution_Impact_WK+2'] = doidf['Curr_Conf_WK+2'] - doidf['Orig_Conf_WK+2']
    # calculate on top orders by week
    doidf['On_Tops_WK-6'] = np.where(doidf['Curr_Conf_WK-6'] - doidf['Orders_WK-6'] < 0, 0,
                                     doidf['Curr_Conf_WK-6'] - doidf['Orders_WK-6'])
    doidf['On_Tops_WK-5'] = np.where(doidf['Curr_Conf_WK-5'] - doidf['Orders_WK-5'] < 0, 0,
                                     doidf['Curr_Conf_WK-5'] - doidf['Orders_WK-5'])
    doidf['On_Tops_WK-4'] = np.where(doidf['Curr_Conf_WK-4'] - doidf['Orders_WK-4'] < 0, 0,
                                     doidf['Curr_Conf_WK-4'] - doidf['Orders_WK-4'])
    doidf['On_Tops_WK-3'] = np.where(doidf['Curr_Conf_WK-3'] - doidf['Orders_WK-3'] < 0, 0,
                                     doidf['Curr_Conf_WK-3'] - doidf['Orders_WK-3'])
    doidf['On_Tops_WK-2'] = np.where(doidf['Curr_Conf_WK-2'] - doidf['Orders_WK-2'] < 0, 0,
                                     doidf['Curr_Conf_WK-2'] - doidf['Orders_WK-2'])
    doidf['On_Tops_WK-1'] = np.where(doidf['Curr_Conf_WK-1'] - doidf['Orders_WK-1'] < 0, 0,
                                     doidf['Curr_Conf_WK-1'] - doidf['Orders_WK-1'])
    doidf['On_Tops_WK+0'] = np.where(doidf['Curr_Conf_WK+0'] - doidf['Orders_WK+0'] < 0, 0,
                                     doidf['Curr_Conf_WK+0'] - doidf['Orders_WK+0'])
    doidf['On_Tops_WK+1'] = np.where(doidf['Curr_Conf_WK+1'] - doidf['Orders_WK+1'] < 0, 0,
                                     doidf['Curr_Conf_WK+1'] - doidf['Orders_WK+1'])
    doidf['On_Tops_WK+2'] = np.where(doidf['Curr_Conf_WK+2'] - doidf['Orders_WK+2'] < 0, 0,
                                     doidf['Curr_Conf_WK+2'] - doidf['Orders_WK+2'])
    # calculate primary impact
    primim_condis = [
        ((doidf['Order_Impact_WK+2'] <= doidf['Capacity_Impact_WK+2']) & (
                doidf['Order_Impact_WK+2'] <= doidf['Execution_Impact_WK+2']) & (doidf['Order_Impact_WK+2'] < 0)),
        ((doidf['Capacity_Impact_WK+2'] < doidf['Order_Impact_WK+2']) & (
                doidf['Capacity_Impact_WK+2'] <= doidf['Execution_Impact_WK+2']) & (
                 doidf['Capacity_Impact_WK+2'] < 0)),
        ((doidf['Execution_Impact_WK+2'] < doidf['Order_Impact_WK+2']) & (
                doidf['Execution_Impact_WK+2'] < doidf['Capacity_Impact_WK+2']) & (
                 doidf['Execution_Impact_WK+2'] < 0)),
        ((doidf['Order_Impact_WK+2'] == 0) & (doidf['Capacity_Impact_WK+2'] == 0) & (
                doidf['Capacity_Impact_WK+2'] == 0) & (doidf['Suggest_WK+2'] == 0))
    ]
    primim_optins = ['Order', 'Capacity', 'Execution', 'No Suggest']
    doidf['Primary_Constraint_WK+2'] = np.select(primim_condis, primim_optins, default='-')
    # format series as integers
    doidf = doidf.astype({'ShipTo': int, 'OSKU': int})
    # correct negative sales and forecasts
    doidf['4WK Fcst Avg'] = np.where(doidf['4WK Fcst Avg'] < 0, 0, doidf['4WK Fcst Avg'])
    doidf['6WK STR Total'] = np.where(doidf['6WK STR Total'] < 0, 0, doidf['6WK STR Total'])
    doidf['4WK STR Avg'] = np.where(doidf['4WK STR Avg'] < 0, 0, doidf['4WK STR Avg'])
    # merge dataframes to fill in missing OSKU descriptions
    doidf = doidf.merge(right=dscdf, on='Pair', how='left')
    # rename columns
    doidf = doidf.rename(columns={'OSKU_x': 'OSKU', 'Description_y': 'Description'})
    # drop duplicates in key column
    doidf = doidf.drop_duplicates(subset=['Pair'])
    # reindex dataframe
    doidf.reset_index(drop=True, inplace=True)
    # get expected total volume needed in each week
    volproj_wkm6 = np.where(doidf['STRs_WK-6'].sum() == 0, doidf['DFcst_Avg'].sum(), doidf['STRs_WK-6'].sum())
    volproj_wkm5 = np.where(doidf['STRs_WK-5'].sum() == 0, doidf['DFcst_Avg'].sum(), doidf['STRs_WK-5'].sum())
    volproj_wkm4 = np.where(doidf['STRs_WK-4'].sum() == 0, doidf['DFcst_Avg'].sum(), doidf['STRs_WK-4'].sum())
    volproj_wkm3 = np.where(doidf['STRs_WK-3'].sum() == 0, doidf['DFcst_Avg'].sum(), doidf['STRs_WK-3'].sum())
    volproj_wkm2 = np.where(doidf['STRs_WK-2'].sum() == 0, doidf['DFcst_Avg'].sum(), doidf['STRs_WK-2'].sum())
    volproj_wkm1 = np.where(doidf['STRs_WK-1'].sum() == 0, doidf['DFcst_Avg'].sum(), doidf['STRs_WK-1'].sum())
    volproj_wkp0 = np.where(doidf['DFcst_WK+0'].sum() == 0, doidf['DFcst_Avg'].sum(), doidf['DFcst_WK+0'].sum())
    volproj_wkp1 = np.where(doidf['DFcst_WK+1'].sum() == 0, doidf['DFcst_Avg'].sum(), doidf['DFcst_WK+1'].sum())
    volproj_wkp2 = np.where(doidf['DFcst_WK+2'].sum() == 0, doidf['DFcst_Avg'].sum(), doidf['DFcst_WK+2'].sum())
    # create new series calculating proportional volume in each week for each row of data
    doidf['Vol_Prop_WK-6'] = np.where(doidf['STRs_WK-6'].sum() == 0, doidf['DFcst_Avg'] / volproj_wkm6,
                                      doidf['STRs_WK-6'] / volproj_wkm6)
    doidf['Vol_Prop_WK-5'] = np.where(doidf['STRs_WK-5'].sum() == 0, doidf['DFcst_Avg'] / volproj_wkm5,
                                      doidf['STRs_WK-5'] / volproj_wkm5)
    doidf['Vol_Prop_WK-4'] = np.where(doidf['STRs_WK-4'].sum() == 0, doidf['DFcst_Avg'] / volproj_wkm4,
                                      doidf['STRs_WK-4'] / volproj_wkm4)
    doidf['Vol_Prop_WK-3'] = np.where(doidf['STRs_WK-3'].sum() == 0, doidf['DFcst_Avg'] / volproj_wkm3,
                                      doidf['STRs_WK-3'] / volproj_wkm3)
    doidf['Vol_Prop_WK-2'] = np.where(doidf['STRs_WK-2'].sum() == 0, doidf['DFcst_Avg'] / volproj_wkm2,
                                      doidf['STRs_WK-2'] / volproj_wkm2)
    doidf['Vol_Prop_WK-1'] = np.where(doidf['STRs_WK-1'].sum() == 0, doidf['DFcst_Avg'] / volproj_wkm1,
                                      doidf['STRs_WK-1'] / volproj_wkm1)
    doidf['Vol_Prop_WK+0'] = np.where(doidf['DFcst_WK+0'].sum() == 0, doidf['DFcst_Avg'] / volproj_wkp0,
                                      doidf['DFcst_WK+0'] / volproj_wkp0)
    doidf['Vol_Prop_WK+1'] = np.where(doidf['DFcst_WK+1'].sum() == 0, doidf['DFcst_Avg'] / volproj_wkp1,
                                      doidf['DFcst_WK+1'] / volproj_wkp1)
    doidf['Vol_Prop_WK+2'] = np.where(doidf['DFcst_WK+2'].sum() == 0, doidf['DFcst_Avg'] / volproj_wkp2,
                                      doidf['DFcst_WK+2'] / volproj_wkp2)
    # calculate weighted average DOI for each SKU by week
    doidf['Doi_Prop_WK-6'] = doidf['Vol_Prop_WK-6'] * doidf['Starting DOI WK-6']
    doidf['Doi_Prop_WK-5'] = doidf['Vol_Prop_WK-5'] * doidf['Starting DOI WK-5']
    doidf['Doi_Prop_WK-4'] = doidf['Vol_Prop_WK-4'] * doidf['Starting DOI WK-4']
    doidf['Doi_Prop_WK-3'] = doidf['Vol_Prop_WK-3'] * doidf['Starting DOI WK-3']
    doidf['Doi_Prop_WK-2'] = doidf['Vol_Prop_WK-2'] * doidf['Starting DOI WK-2']
    doidf['Doi_Prop_WK-1'] = doidf['Vol_Prop_WK-1'] * doidf['Starting DOI WK-1']
    doidf['Doi_Prop_WK+0'] = doidf['Vol_Prop_WK+0'] * doidf['Starting DOI WK+0']
    doidf['Doi_Prop_WK+1'] = doidf['Vol_Prop_WK+1'] * doidf['Starting DOI WK+1']
    doidf['Doi_Prop_WK+2'] = doidf['Vol_Prop_WK+2'] * doidf['Starting DOI WK+2']
    # copy dataframe for full execution data output file
    execdf = doidf.copy(deep=True)
    # add blank notes column at end of dataframe
    doidf['Notes'] = ''
    # sort dataframe by 6 week str volume
    doidf = doidf.sort_values(by=['Starting DOI WK+2', '4WK STR Avg'], ascending=[True, False])
    # replace infinite values with 99
    doidf['DNP_Flag'] = doidf['DNP_Flag'].replace(0, '-')
    # create results dataframe by only keeping ACTIVE status rows
    rf = doidf[~(doidf.Status != 'ACTIVE')]
    # create list of average starting dois
    sdoi_list = [round(rf['Doi_Prop_WK-6'].sum()), round(rf['Doi_Prop_WK-5'].sum()),
                 round(rf['Doi_Prop_WK-4'].sum()), round(rf['Doi_Prop_WK-3'].sum()),
                 round(rf['Doi_Prop_WK-2'].sum()), round(rf['Doi_Prop_WK-1'].sum()),
                 round(rf['Doi_Prop_WK+0'].sum()), round(rf['Doi_Prop_WK+1'].sum()),
                 round(rf['Doi_Prop_WK+2'].sum())]
    # reindex dataframe
    doidf = doidf[['Pair', 'ShipTo', 'OSKU', 'Description', 'Status', 'DNP_Flag',
                   '4WK Fcst Avg', '6WK STR Total', '4WK STR Avg',
                   'Suggest_WK+1', 'Orders_WK+1', 'Orig_Conf_WK+1', 'Curr_Conf_WK+1', 'On_Tops_WK+1',
                   'Suggest_WK+2', 'Orders_WK+2', 'Orig_Conf_WK+2', 'Curr_Conf_WK+2', 'On_Tops_WK+2',
                   'Starting DOI WK+1', 'Starting DOI WK+2', 'Ordered DOI', 'Confirmed DOI',
                   'Order_Impact_WK+1', 'Capacity_Impact_WK+1', 'Execution_Impact_WK+1',
                   'Order_Impact_WK+2', 'Capacity_Impact_WK+2', 'Execution_Impact_WK+2',
                   'Primary_Constraint_WK+2', 'Notes']]
    execdf = execdf[['Pair', 'Yr-Wk', 'ShipTo', 'OSKU', 'Description',
                     'Suggest_WK-6', 'Suggest_WK-5', 'Suggest_WK-4', 'Suggest_WK-3', 'Suggest_WK-2',
                     'Suggest_WK-1', 'Suggest_WK+0', 'Suggest_WK+1', 'Suggest_WK+2', 'Orders_WK-6',
                     'Orders_WK-5', 'Orders_WK-4', 'Orders_WK-3', 'Orders_WK-2', 'Orders_WK-1',
                     'Orders_WK+0', 'Orders_WK+1', 'Orders_WK+2', 'Orig_Conf_WK-6', 'Orig_Conf_WK-5',
                     'Orig_Conf_WK-4', 'Orig_Conf_WK-3', 'Orig_Conf_WK-2', 'Orig_Conf_WK-1', 'Orig_Conf_WK+0',
                     'Orig_Conf_WK+1', 'Orig_Conf_WK+2', 'Curr_Conf_WK-6', 'Curr_Conf_WK-5', 'Curr_Conf_WK-4',
                     'Curr_Conf_WK-3', 'Curr_Conf_WK-2', 'Curr_Conf_WK-1', 'Curr_Conf_WK+0', 'Curr_Conf_WK+1',
                     'Curr_Conf_WK+2', 'On_Tops_WK-6', 'On_Tops_WK-5', 'On_Tops_WK-4', 'On_Tops_WK-3',
                     'On_Tops_WK-2', 'On_Tops_WK-1', 'On_Tops_WK+0', 'On_Tops_WK+1', 'On_Tops_WK+2',
                     'STRs_WK-6', 'STRs_WK-5', 'STRs_WK-4', 'STRs_WK-3', 'STRs_WK-2',
                     'STRs_WK-1', 'DFcst_WK+0', 'DFcst_WK+1', 'DFcst_WK+2', 'DFcst_WK+3', 'DFcst_WK+4',
                     'DFcst_WK+5', 'DFcst_WK+6', 'DFcst_WK+7', 'DFcst_WK+8', 'DFcst_WK+9', 'Beg_Inv_WK-6',
                     'Beg_Inv_WK-5', 'Beg_Inv_WK-4', 'Beg_Inv_WK-3', 'Beg_Inv_WK-2', 'Beg_Inv_WK-1',
                     'Beg_Inv_WK+0', 'Beg_Inv_WK+1', 'Beg_Inv_WK+2', 'DFcst_Avg', '4WK Fcst Avg',
                     '6WK STR Total', '4WK STR Avg', 'Starting DOI WK-6', 'Starting DOI WK-5',
                     'Starting DOI WK-4', 'Starting DOI WK-3', 'Starting DOI WK-2', 'Starting DOI WK-1',
                     'Starting DOI WK+0', 'Starting DOI WK+1', 'Starting DOI WK+2', 'Ordered DOI',
                     'Confirmed DOI', 'Status', 'Order_Impact_WK-6', 'Order_Impact_WK-5', 'Order_Impact_WK-4',
                     'Order_Impact_WK-3', 'Order_Impact_WK-2', 'Order_Impact_WK-1', 'Order_Impact_WK+0',
                     'Order_Impact_WK+1', 'Order_Impact_WK+2', 'Capacity_Impact_WK-6', 'Capacity_Impact_WK-5',
                     'Capacity_Impact_WK-4', 'Capacity_Impact_WK-3', 'Capacity_Impact_WK-2',
                     'Capacity_Impact_WK-1', 'Capacity_Impact_WK+0', 'Capacity_Impact_WK+1',
                     'Capacity_Impact_WK+2', 'Execution_Impact_WK-6', 'Execution_Impact_WK-5',
                     'Execution_Impact_WK-4', 'Execution_Impact_WK-3', 'Execution_Impact_WK-2',
                     'Execution_Impact_WK-1', 'Execution_Impact_WK+0', 'Execution_Impact_WK+1',
                     'Execution_Impact_WK+2']]
    # create list for categorical week visualization
    yrwk_cat_list = ['Reported', 'Reported', 'Reported', 'Reported', 'Reported',
                     'Reported', 'Reported', 'Projected', 'Projected']
    # zip list of weeks and dois
    sdoi_tuples = list(zip(doiwkrng, sdoi_list, yrwk_cat_list))
    # create dataframe from zipped list of weeks and dois
    rf1 = pd.DataFrame(sdoi_tuples, columns=['Yr-Wk', 'DOI', 'Category'])
    # create list of net impact by stage for each week
    orimim_list = [rf['Order_Impact_WK-6'].sum(), rf['Order_Impact_WK-5'].sum(),
                   rf['Order_Impact_WK-4'].sum(), rf['Order_Impact_WK-3'].sum(),
                   rf['Order_Impact_WK-2'].sum(), rf['Order_Impact_WK-1'].sum(),
                   rf['Order_Impact_WK+0'].sum(), rf['Order_Impact_WK+1'].sum(),
                   rf['Order_Impact_WK+2'].sum()]
    cpimim_list = [rf['Capacity_Impact_WK-6'].sum(), rf['Capacity_Impact_WK-5'].sum(),
                   rf['Capacity_Impact_WK-4'].sum(), rf['Capacity_Impact_WK-3'].sum(),
                   rf['Capacity_Impact_WK-2'].sum(), rf['Capacity_Impact_WK-1'].sum(),
                   rf['Capacity_Impact_WK+0'].sum(), rf['Capacity_Impact_WK+1'].sum(),
                   rf['Capacity_Impact_WK+2'].sum()]
    eximim_list = [rf['Execution_Impact_WK-6'].sum(), rf['Execution_Impact_WK-5'].sum(),
                   rf['Execution_Impact_WK-4'].sum(), rf['Execution_Impact_WK-3'].sum(),
                   rf['Execution_Impact_WK-2'].sum(), rf['Execution_Impact_WK-1'].sum(),
                   rf['Execution_Impact_WK+0'].sum(), rf['Execution_Impact_WK+1'].sum(),
                   rf['Execution_Impact_WK+2'].sum()]
    # zip list of weeks and dois
    simp_tuples = list(zip(doiwkrng, orimim_list, cpimim_list, eximim_list))
    # create dataframe from zipped list of weeks and stage impacts
    rf2 = pd.DataFrame(simp_tuples, columns=['Yr-Wk', 'Order_Impact', 'Capacity_Impact', 'Execution_Impact'])
    # create new series in results dataframe
    rf2['Net_Impact'] = rf2['Order_Impact'] + rf2['Capacity_Impact'] + rf2['Execution_Impact']
    # get list of values from net impact series
    avg_net_imp_list = rf2['Net_Impact'].to_list()
    # get average doi and impact from values in list without week +2 results
    avg_net_doi = get_avg_min1(sdoi_list)
    avg_net_imp = get_avg_min1(avg_net_imp_list)
    # create new blank dataframe for clustered bar plot
    rf3 = pd.DataFrame()
    # create combined lists from series
    rf3_wks_list = doiwkrng + doiwkrng + doiwkrng + doiwkrng
    rf3_imp_list = rf2['Order_Impact'].to_list() + rf2['Capacity_Impact'].to_list() + \
                   rf2['Execution_Impact'].to_list() + rf2['Net_Impact'].to_list()
    # create new series from lists for graphing
    rf3['Yr-Wk'] = rf3_wks_list
    rf3['Impact'] = rf3_imp_list
    rf3['Category'] = ['1. Order', '1. Order', '1. Order',
                       '1. Order', '1. Order', '1. Order',
                       '1. Order', '1. Order', '1. Order',
                       '2. Capacity', '2. Capacity', '2. Capacity',
                       '2. Capacity', '2. Capacity', '2. Capacity',
                       '2. Capacity', '2. Capacity', '2. Capacity',
                       '3. Execution', '3. Execution', '3. Execution',
                       '3. Execution', '3. Execution', '3. Execution',
                       '3. Execution', '3. Execution', '3. Execution',
                       '4. Net Impact', '4. Net Impact', '4. Net Impact',
                       '4. Net Impact', '4. Net Impact', '4. Net Impact',
                       '4. Net Impact', '4. Net Impact', '4. Net Impact']
    return [doidf, rf1, rf3, execdf, avg_net_doi, avg_net_imp]


def disco_dnp(dnpdf: pd.DataFrame) -> pd.DataFrame:
    """Identify distributor OSKU pairs that should be marked as Do Not Plan"""
    # create required conditions
    dnpdf['rcon1'] = np.where(dnpdf['DNP_Flag'] != 'DNP', 1, 0)
    dnpdf['rcon2'] = np.where(dnpdf['Orders_WK+1'] <= 0, 1, 0)
    dnpdf['rcon3'] = np.where(dnpdf['Orders_WK+2'] <= 0, 1, 0)
    dnpdf['rcon4'] = np.where(dnpdf['Starting DOI WK+1'] <= 99, 1, 0)
    # create optional conditions
    dnpdf['ocon1'] = np.where(dnpdf['Status'] == 'INACTIVE', 1, 0)
    dnpdf['ocon2'] = np.where(dnpdf['Suggest_WK+1'] <= 0, 1, 0)
    dnpdf['ocon3'] = np.where(dnpdf['Starting DOI WK+1'] <= 7, 1, 0)
    dnpdf['ocon4'] = np.where(dnpdf['Curr_Conf_WK+1'] <= 0, 1, 0)
    dnpdf['ocon5'] = np.where(dnpdf['Suggest_WK+2'] <= 0, 1, 0)
    dnpdf['ocon6'] = np.where(dnpdf['Starting DOI WK+2'] <= 7, 1, 0)
    dnpdf['ocon7'] = np.where(dnpdf['Curr_Conf_WK+2'] <= 0, 1, 0)
    dnpdf['ocon8'] = np.where(dnpdf['4WK Fcst Avg'] <= 10, 1, 0)
    dnpdf['ocon9'] = np.where(dnpdf['6WK STR Total'] <= 10, 1, 0)
    dnpdf['ocon10'] = np.where(dnpdf['4WK STR Avg'] <= 10, 1, 0)
    # create series to total optional conditions
    dnpdf['sum_con'] = dnpdf['ocon1'] + dnpdf['ocon2'] + dnpdf['ocon3'] + dnpdf['ocon4'] + dnpdf['ocon5'] + \
                       dnpdf['ocon6'] + dnpdf['ocon7'] + dnpdf['ocon8'] + dnpdf['ocon9'] + dnpdf['ocon10']
    # check if a ShipTo-OSKU pair meets conditions for DNP flag
    dnpdf['DNP_Check'] = np.where((dnpdf['rcon1'] == 1) & (dnpdf['rcon2'] == 1) &
                                  (dnpdf['rcon3'] == 1) & (dnpdf['rcon4'] == 1) &
                                  (dnpdf['sum_con'] >= 7), 1, 0)
    # only retain pairs that meet all criteria
    dnpdf = dnpdf[dnpdf.DNP_Check != 0]
    # reindex dataframe
    dnpdf = dnpdf[['Pair', 'ShipTo', 'OSKU', 'Description', 'Status', '4WK Fcst Avg',
                   '6WK STR Total', '4WK STR Avg', 'Suggest_WK+1', 'Orders_WK+1',
                   'Suggest_WK+2', 'Orders_WK+2', 'Starting DOI WK+1', 'Starting DOI WK+2']]
    # sort dataframe
    dnpdf = dnpdf.sort_values(by=['ShipTo', 'Starting DOI WK+2', '4WK STR Avg'], ascending=[True, True, False])
    return dnpdf


def scorecard_maker(oosfile: str, excfile: str, fcafile: str, shipto: str):
    """Use source ShipTo reporting to create a combined distributor scorecard"""
    # attempt to create the scorecard
    try:
        # time start
        begin_time = datetime.datetime.now()
        # name file with today's date and scorecard title
        dirname = get_timestamps()[7]
        current_date = get_timestamps()[8]
        # create folder name
        foldername = str(shipto) + ' - ' + str(dirname)
        # create scorecard title name
        stitlename = str(shipto) + ' ' + 'SCORECARD'
        # create new unique directory
        savepath, fpathname = uniq_dir_maker(foldername)
        # if folder select cancelled
        if not savepath:
            return
        # create progress bar window
        progr_bar_win = Tk()
        # set window size
        progr_bar_win.geometry('575x85')
        # set window always on top
        progr_bar_win.attributes('-topmost', True)
        # name progress bar title window
        score_name = 'Generating ' + str(shipto).title() + ' Scorecard'
        progr_bar_win.title(score_name)
        # create application icon
        mc_icon = resource_path('mc_icon.ico')
        progr_bar_win.iconbitmap(mc_icon)
        # Gets the requested values of the height and width
        windowWidth = progr_bar_win.winfo_reqwidth()
        windowHeight = progr_bar_win.winfo_reqheight()
        # Gets both half the screen width/height and window width/height
        positionRight = int(progr_bar_win.winfo_screenwidth() / 2 - windowWidth / 2)
        positionDown = int(progr_bar_win.winfo_screenheight() / 2 - windowHeight / 2)
        positionRight = int(positionRight - (windowWidth * 1.5)) + 15
        positionDown = int(positionDown - (windowHeight / 2)) + 120
        # Positions the window in the center of the page.
        progr_bar_win.geometry("+{}+{}".format(positionRight, positionDown))
        # create label widget for % completion and string variable for updating
        progr_l_p = Label(progr_bar_win, text='0%', font=('Inconsolata', 11), bg='#FFFFFF')
        progr_l_p.place(x=525, y=40, anchor='ne')
        # create label widget for naming stages and string variable for updating
        progr_l_s = Label(progr_bar_win, text='Creating Scorecard', font=('Inconsolata', 11), bg='#FFFFFF')
        progr_l_s.place(x=35, y=40)
        # create progress bar widget
        progr_bar = Progressbar(progr_bar_win, orient='horizontal', length=500, mode='determinate')
        # progr_bar = Progressbar(progr_bar_win, orient=HORIZONTAL, length=500, mode='determinate')
        progr_bar.place(x=30, y=15)
        # start progress bar and update window
        progr_bar.start()
        progr_bar.update_idletasks()
        sleep(.3)
        # increase progress bar and update window
        p_step = randint(3, 5)
        progr_bar['value'] = p_step
        progr_l_p.config(text=str(p_step) + '%')
        progr_l_s.config(text='Gathering OOS data')
        progr_bar.update_idletasks()
        sleep(.3)
        # prepare OOS results
        oosrf, oosdf = disco_oos(oosfile)
        # increase progress bar and update window
        p_step += randint(6, 9)
        progr_bar['value'] = p_step
        progr_l_p.config(text=str(p_step) + '%')
        progr_l_s.config(text='Gathering EXC data')
        progr_bar.update_idletasks()
        sleep(.3)
        # prepare DOI results
        doidf, doirf, impdf, excdf, avg_doi, nimp = disco_exc(excfile)
        # increase progress bar and update window
        p_step += randint(2, 5)
        progr_bar['value'] = p_step
        progr_l_p.config(text=str(p_step) + '%')
        progr_l_s.config(text='Gathering DNP data')
        progr_bar.update_idletasks()
        sleep(.3)
        # prepare DNP results
        dnpdf = disco_dnp(doidf)
        # increase progress bar and update window
        p_step += randint(6, 9)
        progr_bar['value'] = p_step
        progr_l_p.config(text=str(p_step) + '%')
        progr_l_s.config(text='Gathering FCA data')
        progr_bar.update_idletasks()
        sleep(.3)
        # prepare forecast accuracy results
        fcarf, fcadf = disco_fca(fcafile)
        # increase progress bar and update window
        p_step = 40
        progr_bar['value'] = p_step
        progr_l_p.config(text=str(p_step) + '%')
        progr_l_s.config(text='Getting distributor list')
        progr_bar.update_idletasks()
        sleep(.3)
        # get unique list of values from OOS dataframe
        shiparray1 = oosdf['ShipTo'].unique()
        distarray1 = oosdf['Distributor Name'].unique()
        # convert numpy arrays to list objects
        shiplist1 = shiparray1.tolist()
        distlist1 = distarray1.tolist()
        # create new list of ShipTos
        newshiplist1 = []
        # add leading zeros to each ShipTo in list
        for ship in shiplist1:
            newshiplist1.append(str(ship).zfill(6))
        # increase progress bar and update window
        p_step += randint(6, 10)
        progr_bar['value'] = p_step
        progr_l_p.config(text=str(p_step) + '%')
        progr_l_s.config(text='Formatting data step 1 of 3')
        progr_bar.update_idletasks()
        sleep(.3)
        # sort lists alphabetically
        newshiplist1.sort()
        distlist1.sort()
        # get unique list of values from doi dataframe
        shiparray2 = doidf['ShipTo'].unique()
        # convert numpy arrays to list objects
        shiplist2 = shiparray2.tolist()
        # create new list of ShipTos
        newshiplist2 = []
        # add leading zeros to each ShipTo in list
        for ship in shiplist2:
            newshiplist2.append(str(ship).zfill(6))
        # sort lists alphabetically
        newshiplist2.sort()
        # increase progress bar and update window
        p_step += randint(6, 10)
        progr_bar['value'] = p_step
        progr_l_p.config(text=str(p_step) + '%')
        progr_l_s.config(text='Formatting data step 2 of 3')
        progr_bar.update_idletasks()
        sleep(.3)
        # compare list of ShipTos and distributors and keep only unique values
        shiplist = list(set(newshiplist1) | set(newshiplist2))
        distlist = list(set(distlist1))
        # reformat list for scorecard about tab
        aboutshiplist = ', '.join(str(ship) for ship in shiplist)
        aboutdistlist = ', '.join(str(dist) for dist in distlist)
        # increase progress bar and update window
        p_step += randint(6, 10)
        progr_bar['value'] = p_step
        progr_l_p.config(text=str(p_step) + '%')
        progr_l_s.config(text='Formatting data step 3 of 3')
        progr_bar.update_idletasks()
        sleep(.3)
        # create file name as a variable
        scorecard = savepath + 'Scorecard - ' + foldername + '.xlsx'
        execscore = savepath + 'Execution Details - ' + foldername + '.xlsx'
        # rewrite filepath with correct operating system separators
        scorecard = os_split_fixer(scorecard)
        execscore = os_split_fixer(execscore)
        # increase progress bar and update window
        p_step = 70
        progr_bar['value'] = p_step
        progr_l_p.config(text=str(p_step) + '%')
        progr_l_s.config(text='Generating new Scorecard file')
        progr_bar.update_idletasks()
        sleep(.3)
        # create a Pandas Excel writer using XlsxWriter as the engine
        writer = pd.ExcelWriter(execscore, engine='xlsxwriter')
        # create workbook and worksheet objects
        execbook = writer.book
        # write each DataFrame to a specific sheet and reset the index
        excdf.to_excel(writer, sheet_name='Execution Detail', index=False)
        # create worksheet objects
        excdeets = writer.sheets['Execution Detail']
        # get create date for workbook properties in list
        times = get_timestamps()
        # set workbook properties
        execbook.set_properties({
            'title': 'Execution Details',
            'subject': 'Distributor Core Metrics Review',
            'author': 'Joseph Arnson',
            'company': 'Molson Coors',
            'category': 'Customer Service Excellence',
            'keywords': 'Out of Stock, Execution, Forecast Accuracy',
            'created': datetime.date(times[0], times[1], times[3]),
            'comments': 'Created with Python and XlsxWriter'})
        # create formatting methods for workbook
        comma_format = execbook.add_format({'num_format': '#,##0.00', 'align': 'right'})
        center_format = execbook.add_format({'align': 'center'})
        left_format = execbook.add_format({'align': 'left'})
        # create table formatting for each worksheet
        format_excel(writer, excdf, 'Execution Detail', 'EXEC', 'Table Style Medium 3', True)
        # format workbook
        excdeets.hide_gridlines(2)
        excdeets.freeze_panes(1, 4)
        # Set the column width and format
        excdeets.set_column('A:A', 13.14, center_format)
        excdeets.set_column('B:B', 8.14, center_format)
        excdeets.set_column('C:C', 8.57, center_format)
        excdeets.set_column('D:D', 7.57, center_format)
        excdeets.set_column('E:E', 36.43, left_format)
        excdeets.set_column('F:F', 15.43, comma_format)
        excdeets.set_column('G:G', 15.43, comma_format)
        excdeets.set_column('H:H', 15.43, comma_format)
        excdeets.set_column('I:I', 15.43, comma_format)
        excdeets.set_column('J:J', 15.43, comma_format)
        excdeets.set_column('K:K', 15.43, comma_format)
        excdeets.set_column('L:L', 15.71, comma_format)
        excdeets.set_column('M:M', 15.71, comma_format)
        excdeets.set_column('N:N', 15.71, comma_format)
        excdeets.set_column('O:O', 14.43, comma_format)
        excdeets.set_column('P:P', 14.43, comma_format)
        excdeets.set_column('Q:Q', 14.43, comma_format)
        excdeets.set_column('R:R', 14.43, comma_format)
        excdeets.set_column('S:S', 14.43, comma_format)
        excdeets.set_column('T:T', 14.43, comma_format)
        excdeets.set_column('U:U', 14.86, comma_format)
        excdeets.set_column('V:V', 14.86, comma_format)
        excdeets.set_column('W:W', 14.86, comma_format)
        excdeets.set_column('X:X', 17.43, comma_format)
        excdeets.set_column('Y:Y', 17.43, comma_format)
        excdeets.set_column('Z:Z', 17.43, comma_format)
        excdeets.set_column('AA:AA', 17.43, comma_format)
        excdeets.set_column('AB:AB', 17.43, comma_format)
        excdeets.set_column('AC:AC', 17.43, comma_format)
        excdeets.set_column('AD:AD', 17.71, comma_format)
        excdeets.set_column('AE:AE', 17.71, comma_format)
        excdeets.set_column('AF:AF', 17.71, comma_format)
        excdeets.set_column('AG:AG', 17.43, comma_format)
        excdeets.set_column('AH:AH', 17.43, comma_format)
        excdeets.set_column('AI:AI', 17.43, comma_format)
        excdeets.set_column('AJ:AJ', 17.43, comma_format)
        excdeets.set_column('AK:AK', 17.43, comma_format)
        excdeets.set_column('AL:AL', 17.43, comma_format)
        excdeets.set_column('AM:AM', 17.71, comma_format)
        excdeets.set_column('AN:AN', 17.71, comma_format)
        excdeets.set_column('AO:AO', 17.71, comma_format)
        excdeets.set_column('AP:AP', 16.29, comma_format)
        excdeets.set_column('AQ:AQ', 16.29, comma_format)
        excdeets.set_column('AR:AR', 16.29, comma_format)
        excdeets.set_column('AS:AS', 16.29, comma_format)
        excdeets.set_column('AT:AT', 16.29, comma_format)
        excdeets.set_column('AU:AU', 16.29, comma_format)
        excdeets.set_column('AV:AV', 16.57, comma_format)
        excdeets.set_column('AW:AW', 16.57, comma_format)
        excdeets.set_column('AX:AX', 16.57, comma_format)
        excdeets.set_column('AY:AY', 12.43, comma_format)
        excdeets.set_column('AZ:AZ', 12.43, comma_format)
        excdeets.set_column('BA:BA', 12.43, comma_format)
        excdeets.set_column('BB:BB', 12.43, comma_format)
        excdeets.set_column('BC:BC', 12.43, comma_format)
        excdeets.set_column('BD:BD', 12.43, comma_format)
        excdeets.set_column('BE:BE', 13.43, comma_format)
        excdeets.set_column('BF:BF', 13.43, comma_format)
        excdeets.set_column('BG:BG', 13.43, comma_format)
        excdeets.set_column('BH:BH', 13.43, comma_format)
        excdeets.set_column('BI:BI', 13.43, comma_format)
        excdeets.set_column('BJ:BJ', 13.43, comma_format)
        excdeets.set_column('BK:BK', 13.43, comma_format)
        excdeets.set_column('BL:BL', 13.43, comma_format)
        excdeets.set_column('BM:BM', 13.43, comma_format)
        excdeets.set_column('BN:BN', 13.43, comma_format)
        excdeets.set_column('BO:BO', 15.57, comma_format)
        excdeets.set_column('BP:BP', 15.57, comma_format)
        excdeets.set_column('BQ:BQ', 15.57, comma_format)
        excdeets.set_column('BR:BR', 15.57, comma_format)
        excdeets.set_column('BS:BS', 15.57, comma_format)
        excdeets.set_column('BT:BT', 15.57, comma_format)
        excdeets.set_column('BU:BU', 15.86, comma_format)
        excdeets.set_column('BV:BV', 15.86, comma_format)
        excdeets.set_column('BW:BW', 15.86, comma_format)
        excdeets.set_column('BX:BX', 11.57, comma_format)
        excdeets.set_column('BY:BY', 14.29, comma_format)
        excdeets.set_column('BZ:BZ', 15.29, comma_format)
        excdeets.set_column('CA:CA', 14, comma_format)
        excdeets.set_column('CB:CB', 18.57, comma_format)
        excdeets.set_column('CC:CC', 18.57, comma_format)
        excdeets.set_column('CD:CD', 18.57, comma_format)
        excdeets.set_column('CE:CE', 18.57, comma_format)
        excdeets.set_column('CF:CF', 18.57, comma_format)
        excdeets.set_column('CG:CG', 18.57, comma_format)
        excdeets.set_column('CH:CH', 19, comma_format)
        excdeets.set_column('CI:CI', 19, comma_format)
        excdeets.set_column('CJ:CJ', 19, comma_format)
        excdeets.set_column('CK:CK', 13.71, comma_format)
        excdeets.set_column('CL:CL', 15.86, comma_format)
        excdeets.set_column('CM:CM', 8.57, comma_format)
        excdeets.set_column('CN:CN', 20.86, comma_format)
        excdeets.set_column('CO:CO', 20.86, comma_format)
        excdeets.set_column('CP:CP', 20.86, comma_format)
        excdeets.set_column('CQ:CQ', 20.86, comma_format)
        excdeets.set_column('CR:CR', 20.86, comma_format)
        excdeets.set_column('CS:CS', 20.86, comma_format)
        excdeets.set_column('CT:CT', 21.14, comma_format)
        excdeets.set_column('CU:CU', 21.14, comma_format)
        excdeets.set_column('CV:CV', 21.14, comma_format)
        excdeets.set_column('CW:CW', 23.29, comma_format)
        excdeets.set_column('CX:CX', 23.29, comma_format)
        excdeets.set_column('CY:CY', 23.29, comma_format)
        excdeets.set_column('CZ:CZ', 23.29, comma_format)
        excdeets.set_column('DA:DA', 23.29, comma_format)
        excdeets.set_column('DB:DB', 23.29, comma_format)
        excdeets.set_column('DC:DC', 23.57, comma_format)
        excdeets.set_column('DD:DD', 23.57, comma_format)
        excdeets.set_column('DE:DE', 23.57, comma_format)
        excdeets.set_column('DF:DF', 24.57, comma_format)
        excdeets.set_column('DG:DG', 24.57, comma_format)
        excdeets.set_column('DH:DH', 24.57, comma_format)
        excdeets.set_column('DI:DI', 24.57, comma_format)
        excdeets.set_column('DJ:DJ', 24.57, comma_format)
        excdeets.set_column('DK:DK', 24.57, comma_format)
        excdeets.set_column('DL:DL', 24.86, comma_format)
        excdeets.set_column('DM:DM', 24.86, comma_format)
        excdeets.set_column('DN:DN', 24.86, comma_format)
        # correct header formatting for each worksheet
        excdeets.set_row(0, None, left_format)
        # close the Pandas Excel writer and output the Excel file
        writer.save()
        # create list of field names for data dictionary
        field_name_list = ['Pair', 'ShipTo', 'OSKU', 'Description', 'Status', 'DNP_Flag', 'ShipTo Realigned STR',
                           'Distributor Forecast', '4WK Fcst Avg', '6WK STR Total', '4WK STR Avg', 'Suggest',
                           'Original Order', 'Original Confirm', 'Current Confirm', 'On Tops', 'Starting DOI',
                           'Ordered DOI', 'Confirmed DOI', 'Order Impact', 'Capacity Impact', 'Execution Impact',
                           'Primary Constraint', 'Out of Stock Incident', 'Out of Stock Volume',
                           'Total OOS Proportion', 'Mean Absolute Percent Error', 'Forecast Accuracy', 'DNP Detail']
        # create list of field definitions for data dictionary
        field_desc_list = ['A unique ShipTo-OSKU combination used here as the primary key.',
                           'A unique six digit number associated with the Distributor Ship Location.',
                           'The orderable stock keeping unit number of the material associated with the pair.',
                           "The orderable stock keeping unit number's written package description associated with the pair.",
                           'Determines if there was any STR or forecast activity for the pair during the reviewed nine week time period.',
                           'ShipTo-OSKU pairs with a DNP in this field signifiy OSKUs that the distributor has requested we not provide a plan for.',
                           'Sales to Retail quantity reported by SRS in units. Updated weekly on Tuesday morning.',
                           'Distributor created forecast in units sourced from APO-SNC.',
                           'Average of the Distributor Forecast during the upcoming four week period for weeks +2 to +5.',
                           'Sum total of the ShipTo Realigned Sales to Retailers over the most recently reported six weeks.',
                           'Average of the most recently reported four weeks of ShipTo Realigned Sales to Retailers.',
                           'Sales order quantity suggested to the distributor in DRIVE. Updated weekly on Saturday night.',
                           'Sales order quantity placed by the distributor in DRIVE. Snapshot taken on Tuesday night.',
                           'Final Wednesday committed order quantity in units. Snapshot taken on Wednesday night following Back Order Processing.',
                           'Sales order quantity committed to by Molson Coors including any changes since the Original Confirm snapshot.',
                           'Sales order quantity committed to distributors in addition to their original order placed during the order window.',
                           'The days of inventory the distributor OSKU pair is expected to begin the week with. ',
                           'The days of inventory the distributor OSKU pair would end the week with if they received a full order confirmation.',
                           'The days of inventory the distributor OSKU pair is expected to end the week with based on their current confirmed order.',
                           'Represents the overall change in inventory expected based on the difference between the original order and the suggest.',
                           'Represents the overall change in inventory expected based on the difference between the original confirm and the original order.',
                           'Represents the overall change in inventory expected based on the difference between the current confirm and the original confirm.',
                           'Determines which of the three execution stage impacts has the most negative effect on the expected inventory outcome.',
                           'Determines if there has been an occurrence of sales lost due to an inventory out of stock when the distributor would expect to have sales.',
                           'Volume in BBLs of potential sales lost by an out of stock incident. Determined by the average STR over the most recent 13 week period.',
                           'The proportion of all out of stock volume listed in this file attributed to a ShipTo-OSKU pair in a given week.',
                           'A measure of prediction accuracy determined by the absolute difference between a reported STR and its corresponding distributor forecast.',
                           'A measure of prediction accuracy determined by taking 100% minus the value of the mean absolute percent error for that OSKU.',
                           'This worksheet displays a series of ShipTo-OSKU Pairs that should be considered by the distributor for DNP flagging.'
                           ]
        # zip list of field names and definitions
        data_dict_tuples = list(zip(field_name_list, field_desc_list))
        # create dataframe from zipped list of field names and definitions
        dicdf = pd.DataFrame(data_dict_tuples, columns=['Field Name', 'Field Definition'])
        # create a Pandas Excel writer using XlsxWriter as the engine
        writer = pd.ExcelWriter(scorecard, engine='xlsxwriter')
        # create workbook and worksheet objects
        scorebook = writer.book
        aboutwks = scorebook.add_worksheet('About')
        distdash = scorebook.add_worksheet('Scorecard')
        # write each DataFrame to a specific sheet and reset the index
        oosdf.to_excel(writer, sheet_name='OOS Detail', index=False)
        doidf.to_excel(writer, sheet_name='EXC Detail', index=False)
        fcadf.to_excel(writer, sheet_name='FCA Detail', index=False)
        dnpdf.to_excel(writer, sheet_name='DNP Detail', index=False)
        dicdf.to_excel(writer, sheet_name='Data Dictionary', index=False)
        # create worksheet objects
        oosdeets = writer.sheets['OOS Detail']
        doideets = writer.sheets['EXC Detail']
        fcadeets = writer.sheets['FCA Detail']
        dnpdeets = writer.sheets['DNP Detail']
        datadict = writer.sheets['Data Dictionary']
        # get create date for workbook properties in list
        times = get_timestamps()
        # set workbook properties
        scorebook.set_properties({
            'title': 'Distributor Scorecard',
            'subject': 'Distributor Core Metrics Review',
            'author': 'Joseph Arnson',
            'company': 'Molson Coors',
            'category': 'Customer Service Excellence',
            'keywords': 'Out of Stock, Execution, Forecast Accuracy',
            'created': datetime.date(times[0], times[1], times[3]),
            'comments': 'Created with Python and XlsxWriter'})
        # create formatting methods for workbook
        comma_format = scorebook.add_format({'num_format': '#,##0.00', 'align': 'right'})
        center_format = scorebook.add_format({'align': 'center'})
        left_format = scorebook.add_format({'align': 'left'})
        prcnt_format = scorebook.add_format({'num_format': '0.00%', 'align': 'right'})
        sixdig_format = scorebook.add_format({'num_format': '00000#', 'align': 'center'})
        # create formatting for conditional formatting
        con_good = scorebook.add_format({'bg_color': '#C6EFCE', 'font_color': '#006100'})
        con_bad = scorebook.add_format({'bg_color': '#FFC7CE', 'font_color': '#9C0006'})
        con_note = scorebook.add_format({'bg_color': '#FCD5B4', 'font_color': '#974706'})
        # get each dataframe used range
        oosmax_row, oosmax_col = oosdf.shape
        doimax_row, doimax_col = doidf.shape
        fcamax_row, fcamax_col = fcadf.shape
        # create table formatting for each worksheet
        format_excel(writer, oosdf, 'OOS Detail', 'OOS', 'Table Style Medium 2', True)
        format_excel(writer, doidf, 'EXC Detail', 'EXC', 'Table Style Medium 2', True)
        format_excel(writer, fcadf, 'FCA Detail', 'FCA', 'Table Style Medium 2', True)
        format_excel(writer, dnpdf, 'DNP Detail', 'DNP', 'Table Style Medium 2', True)
        format_excel(writer, dicdf, 'Data Dictionary', 'DICT', 'Table Style Medium 2', False)
        # Set the column width and format - OOS tab
        oosdeets.set_column('A:A', 10, center_format)
        oosdeets.set_column('B:B', 10, center_format)
        oosdeets.set_column('C:C', 10, sixdig_format)
        oosdeets.set_column('D:D', 45, left_format)
        oosdeets.set_column('E:E', 10, center_format)
        oosdeets.set_column('F:F', 45, left_format)
        oosdeets.set_column('G:G', 15.86, comma_format)
        oosdeets.set_column('H:H', 18, prcnt_format)
        # Set the column width and format - EXC tab
        doideets.set_column('A:A', 13.14, center_format)
        doideets.set_column('B:B', 8.57, center_format)
        doideets.set_column('C:C', 7.57, center_format)
        doideets.set_column('D:D', 36.43, left_format)
        doideets.set_column('E:E', 9, left_format)
        doideets.set_column('F:F', 11, center_format)
        doideets.set_column('G:G', 14.29, comma_format)
        doideets.set_column('H:H', 15.29, comma_format)
        doideets.set_column('I:I', 14, comma_format)
        doideets.set_column('J:J', 15.71, comma_format, {'level': 1, 'hidden': True})
        doideets.set_column('K:K', 14.86, comma_format, {'level': 1, 'hidden': True})
        doideets.set_column('L:L', 17.71, comma_format, {'level': 1, 'hidden': True})
        doideets.set_column('M:M', 17.71, comma_format, {'level': 1, 'hidden': True})
        doideets.set_column('N:N', 16.57, comma_format, {'level': 1, 'hidden': True})
        doideets.set_column('O:O', 15.71, comma_format, {'level': 1, 'hidden': True})
        doideets.set_column('P:P', 14.86, comma_format, {'level': 1, 'hidden': True})
        doideets.set_column('Q:Q', 17.71, comma_format, {'level': 1, 'hidden': True})
        doideets.set_column('R:R', 17.71, comma_format, {'level': 1, 'hidden': True})
        doideets.set_column('S:S', 16.57, comma_format, {'level': 1, 'hidden': True})
        doideets.set_column('T:T', 19, comma_format, {'collapsed': True})
        doideets.set_column('U:U', 19, comma_format)
        doideets.set_column('V:V', 13.71, comma_format)
        doideets.set_column('W:W', 15.86, comma_format)
        doideets.set_column('X:X', 21.14, comma_format, {'level': 1, 'hidden': True})
        doideets.set_column('Y:Y', 23.57, comma_format, {'level': 1, 'hidden': True})
        doideets.set_column('Z:Z', 24.86, comma_format, {'level': 1, 'hidden': True})
        doideets.set_column('AA:AA', 21.14, comma_format, {'level': 1, 'hidden': True})
        doideets.set_column('AB:AB', 23.57, comma_format, {'level': 1, 'hidden': True})
        doideets.set_column('AC:AC', 24.86, comma_format, {'level': 1, 'hidden': True})
        doideets.set_column('AD:AD', 26.14, left_format, {'collapsed': True})
        doideets.set_column('AE:AE', 30, left_format)
        # Set the column width and format - FCA tab
        fcadeets.set_column('A:A', 10, center_format)
        fcadeets.set_column('B:B', 14, center_format)
        fcadeets.set_column('C:C', 10, sixdig_format)
        fcadeets.set_column('D:D', 10, center_format)
        fcadeets.set_column('E:E', 45, left_format)
        fcadeets.set_column('F:F', 16.29, comma_format)
        fcadeets.set_column('G:G', 13, comma_format)
        fcadeets.set_column('H:H', 12, comma_format)
        fcadeets.set_column('I:I', 12, prcnt_format)
        fcadeets.set_column('J:J', 14.14, prcnt_format)
        # Set the column width and format - DNP tab
        dnpdeets.set_column('A:A', 14, center_format)
        dnpdeets.set_column('B:B', 8, center_format)
        dnpdeets.set_column('C:C', 8, center_format)
        dnpdeets.set_column('D:D', 36.43, left_format)
        dnpdeets.set_column('E:E', 8.57, left_format)
        dnpdeets.set_column('F:F', 14.29, comma_format)
        dnpdeets.set_column('G:G', 15.29, comma_format)
        dnpdeets.set_column('H:H', 14, comma_format)
        dnpdeets.set_column('I:I', 15.71, comma_format)
        dnpdeets.set_column('J:J', 14.86, comma_format)
        dnpdeets.set_column('K:K', 15.71, comma_format)
        dnpdeets.set_column('L:L', 14.86, comma_format)
        dnpdeets.set_column('M:M', 19, comma_format)
        dnpdeets.set_column('N:N', 19, comma_format)
        # Set the column width and format - dictionary tab
        datadict.set_column('A:A', 30, left_format)
        datadict.set_column('B:B', 130, left_format)
        # set worksheet zoom
        distdash.set_zoom(100)
        # hide worksheet gridlines
        aboutwks.hide_gridlines(2)
        distdash.hide_gridlines(2)
        oosdeets.hide_gridlines(2)
        doideets.hide_gridlines(2)
        fcadeets.hide_gridlines(2)
        dnpdeets.hide_gridlines(2)
        datadict.hide_gridlines(2)
        # freeze worksheets
        oosdeets.freeze_panes(1, 0)
        doideets.freeze_panes(1, 4)
        fcadeets.freeze_panes(1, 0)
        dnpdeets.freeze_panes(1, 0)
        datadict.freeze_panes(1, 0)
        # add conditional formatting - OOS detail tab
        oosdeets.conditional_format('G2:G' + str(oosmax_row + 1), {'type': '3_color_scale',
                                                                   'min_color': '#63be7b',
                                                                   'mid_color': '#ffeb84',
                                                                   'max_color': '#f8696b'})
        oosdeets.conditional_format('H2:H' + str(oosmax_row + 1), {'type': '3_color_scale',
                                                                   'min_color': '#63be7b',
                                                                   'mid_color': '#ffeb84',
                                                                   'max_color': '#f8696b'})
        # add conditional formatting - EXC detail tab
        doideets.conditional_format(f'E2:E{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '=',
                                                                   'value': '"INACTIVE"', 'format': con_note})
        doideets.conditional_format(f'F2:F{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '=',
                                                                   'value': '"DNP"', 'format': con_bad})
        doideets.conditional_format(f'G2:G{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '=',
                                                                   'value': 0, 'format': con_bad})
        doideets.conditional_format(f'H2:H{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '=',
                                                                   'value': 0, 'format': con_bad})
        doideets.conditional_format(f'I2:I{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '=',
                                                                   'value': 0, 'format': con_bad})
        doideets.conditional_format(f'J2:J{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '=',
                                                                   'value': 0, 'format': con_note})
        doideets.conditional_format(f'K2:K{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '=',
                                                                   'value': 0, 'format': con_note})
        doideets.conditional_format(f'L2:L{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '=',
                                                                   'value': 0, 'format': con_note})
        doideets.conditional_format(f'M2:M{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '=',
                                                                   'value': 0, 'format': con_note})
        doideets.conditional_format(f'N2:N{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '>',
                                                                   'value': 0, 'format': con_good})
        doideets.conditional_format(f'O2:O{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '=',
                                                                   'value': 0, 'format': con_note})
        doideets.conditional_format(f'P2:P{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '=',
                                                                   'value': 0, 'format': con_note})
        doideets.conditional_format(f'Q2:Q{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '=',
                                                                   'value': 0, 'format': con_note})
        doideets.conditional_format(f'R2:R{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '=',
                                                                   'value': 0, 'format': con_note})
        doideets.conditional_format(f'S2:S{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '>',
                                                                   'value': 0, 'format': con_good})
        doideets.conditional_format(f'T2:T{str(doimax_row + 1)}', {'type': '3_color_scale',
                                                                   'min_color': '#f8696b',
                                                                   'mid_color': '#ffeb84',
                                                                   'max_color': '#63be7b',
                                                                   'min_type': 'num',
                                                                   'mid_type': 'num',
                                                                   'max_type': 'num',
                                                                   'min_value': 7,
                                                                   'mid_value': 15,
                                                                   'max_value': 30})
        doideets.conditional_format(f'U2:U{str(doimax_row + 1)}', {'type': '3_color_scale',
                                                                   'min_color': '#f8696b',
                                                                   'mid_color': '#ffeb84',
                                                                   'max_color': '#63be7b',
                                                                   'min_type': 'num',
                                                                   'mid_type': 'num',
                                                                   'max_type': 'num',
                                                                   'min_value': 7,
                                                                   'mid_value': 15,
                                                                   'max_value': 30})
        doideets.conditional_format(f'V2:V{str(doimax_row + 1)}', {'type': '3_color_scale',
                                                                   'min_color': '#f8696b',
                                                                   'mid_color': '#ffeb84',
                                                                   'max_color': '#63be7b',
                                                                   'min_type': 'num',
                                                                   'mid_type': 'num',
                                                                   'max_type': 'num',
                                                                   'min_value': 7,
                                                                   'mid_value': 15,
                                                                   'max_value': 30})
        doideets.conditional_format(f'W2:W{str(doimax_row + 1)}', {'type': '3_color_scale',
                                                                   'min_color': '#f8696b',
                                                                   'mid_color': '#ffeb84',
                                                                   'max_color': '#63be7b',
                                                                   'min_type': 'num',
                                                                   'mid_type': 'num',
                                                                   'max_type': 'num',
                                                                   'min_value': 7,
                                                                   'mid_value': 15,
                                                                   'max_value': 30})
        doideets.conditional_format(f'X2:X{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '>',
                                                                   'value': 0, 'format': con_good})
        doideets.conditional_format(f'X2:X{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '<',
                                                                   'value': 0, 'format': con_bad})
        doideets.conditional_format(f'Y2:Y{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '>',
                                                                   'value': 0, 'format': con_good})
        doideets.conditional_format(f'Y2:Y{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '<',
                                                                   'value': 0, 'format': con_bad})
        doideets.conditional_format(f'Z2:X{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '>',
                                                                   'value': 0, 'format': con_good})
        doideets.conditional_format(f'Z2:X{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '<',
                                                                   'value': 0, 'format': con_bad})
        doideets.conditional_format(f'AA2:AA{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '>',
                                                                     'value': 0, 'format': con_good})
        doideets.conditional_format(f'AA2:AA{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '<',
                                                                     'value': 0, 'format': con_bad})
        doideets.conditional_format(f'AB2:AB{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '>',
                                                                     'value': 0, 'format': con_good})
        doideets.conditional_format(f'AB2:AB{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '<',
                                                                     'value': 0, 'format': con_bad})
        doideets.conditional_format(f'AC2:AC{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '>',
                                                                     'value': 0, 'format': con_good})
        doideets.conditional_format(f'AC2:AC{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '<',
                                                                     'value': 0, 'format': con_bad})
        doideets.conditional_format(f'AD2:AD{str(doimax_row + 1)}', {'type': 'cell', 'criteria': '=',
                                                                     'value': '"NO SUGGEST"', 'format': con_note})
        # add conditional formatting - FCA detail tab
        fcadeets.conditional_format('I2:I' + str(fcamax_row + 1), {'type': '3_color_scale',
                                                                   'min_color': '#63be7b',
                                                                   'mid_color': '#ffeb84',
                                                                   'max_color': '#f8696b'})
        fcadeets.conditional_format('J2:J' + str(fcamax_row + 1), {'type': '3_color_scale',
                                                                   'min_color': '#f8696b',
                                                                   'mid_color': '#ffeb84',
                                                                   'max_color': '#63be7b'})
        # calculate average results for each metric category
        avgip = oosrf['OOSi_%'].mean()
        avgic = oosrf['OOSi'].mean()
        avgvp = oosrf['OOSv_%'].mean()
        avgvc = oosrf['OOSv'].mean()
        # avg_doi = doirf['DOI'].mean()
        avg_fa = fcarf['Fcst_Acc'].mean()
        # increase progress bar and update window
        p_step += randint(4, 7)
        progr_bar['value'] = p_step
        progr_l_p.config(text=str(p_step) + '%')
        progr_l_s.config(text='Plotting new Scorecard results')
        progr_bar.update_idletasks()
        sleep(.3)
        # create graphs
        oosip = oosip_barplot(oosrf, savepath, avgip)
        oosic = oosic_barplot(oosrf, savepath, avgic)
        oosvp = oosvp_barplot(oosrf, savepath, avgvp)
        oosvc = oosvc_barplot(oosrf, savepath, avgvc)
        doiplot = doip_barplot(doirf, savepath, avg_doi)
        faplot = fap_barplot(fcarf, savepath, avg_fa)
        socplot = socc_barplot(impdf, savepath, nimp)
        # open new pngs as image objects
        pic_oosip = Image.open(oosip)
        pic_oosic = Image.open(oosic)
        pic_oosvp = Image.open(oosvp)
        pic_oosvc = Image.open(oosvc)
        # concat images into dashboard
        plot2i_finalizer(pic_oosip, pic_oosic, savepath, dirname)
        plot2v_finalizer(pic_oosvp, pic_oosvc, savepath, dirname)
        oosfinal = oosplot_finalizer(pic_oosip, pic_oosic, pic_oosvp, pic_oosvc, savepath, dirname)
        # resize final graphs preserving aspect ratio
        pic_scaler(oosfinal, .85)
        pic_scaler(doiplot, .85)
        # pic_scaler(socplot, 1.2)
        pic_scaler(faplot, .85)
        # increase progress bar and update window
        p_step += randint(4, 7)
        progr_bar['value'] = p_step
        progr_l_p.config(text=str(p_step) + '%')
        progr_l_s.config(text='Creating new Scorecard dashboard')
        progr_bar.update_idletasks()
        sleep(.3)
        # open new pngs as image objects
        pic_doi = Image.open(doiplot)
        pic_soc = Image.open(socplot)
        pic_fca = Image.open(faplot)
        # combine DOI and fca plots
        fnifinal = fniplot_finalizer(pic_doi, pic_fca, savepath, dirname)
        # combine both final plots into one dashboard
        pic_oosfinal = Image.open(oosfinal)
        pic_fnifinal = Image.open(fnifinal)
        scoreboard = plotplot_finalizer(pic_oosfinal, pic_fnifinal, savepath, dirname)
        # open new pngs as image objects
        pic_scoreboard = Image.open(scoreboard)
        # combine OOS, DOI, and FCA dashboard with SOC plot
        scoredash = plotplotplot_finalizer(pic_scoreboard, pic_soc, savepath, dirname)
        # insert graphs to scorecard sheet
        distdash.insert_image('A1', scoredash)
        # increase progress bar and update window
        p_step += randint(4, 8)
        progr_bar['value'] = p_step
        progr_l_p.config(text=str(p_step) + '%')
        progr_l_s.config(text='Managing new file and source data')
        progr_bar.update_idletasks()
        sleep(.3)
        # remove extra created plot pictures
        remove(oosip)
        remove(oosic)
        remove(oosvp)
        remove(oosvc)
        # move source files to new directory
        move(oosfile, savepath)
        move(excfile, savepath)
        move(fcafile, savepath)
        # get current time
        current_time = datetime.datetime.today().strftime('%I:%M %p')
        # get used weeks in scorecard
        used_weeks = ', '.join(used_range(current_date))
        # get source file names
        oosfile_name = path.basename(oosfile)
        onifile_name = path.basename(excfile)
        fcafile_name = path.basename(fcafile)
        # create formatting methods for about worksheet
        title_format = scorebook.add_format({'bold': True, 'font': 'Century Gothic',
                                             'font_size': 11, 'align': 'left'})
        title_format.set_bottom(2)
        subtitle_format = scorebook.add_format({'bold': True, 'font_name': 'Calibri',
                                                'font_size': 11, 'align': 'left'})
        merge_format = scorebook.add_format({'align': 'center', 'bold': True, 'font_name': 'Century Gothic',
                                             'font_size': 20})
        bottom_format = scorebook.add_format()
        bottom_format.set_bottom(2)
        # fill out the About tab column B
        aboutwks.set_column('B:B', 18)
        aboutwks.merge_range('B3:H3', stitlename, merge_format)
        aboutwks.write(5, 1, 'File Details', title_format)
        aboutwks.write(5, 2, '', bottom_format)
        aboutwks.write(5, 3, '', bottom_format)
        aboutwks.write(5, 4, '', bottom_format)
        aboutwks.write(5, 5, '', bottom_format)
        aboutwks.write(5, 6, '', bottom_format)
        aboutwks.write(5, 7, '', bottom_format)
        aboutwks.write(7, 1, 'ShipTo(s)', subtitle_format)
        aboutwks.write(8, 1, 'Distributor(s)', subtitle_format)
        aboutwks.write(9, 1, 'Period', subtitle_format)
        aboutwks.write(10, 1, 'Created On', subtitle_format)
        aboutwks.write(11, 1, 'Time Created', subtitle_format)
        aboutwks.write(12, 1, 'Processing Time', subtitle_format)
        aboutwks.write(15, 1, 'File Sources Used', title_format)
        aboutwks.write(15, 2, '', bottom_format)
        aboutwks.write(15, 3, '', bottom_format)
        aboutwks.write(15, 4, '', bottom_format)
        aboutwks.write(15, 5, '', bottom_format)
        aboutwks.write(15, 6, '', bottom_format)
        aboutwks.write(15, 7, '', bottom_format)
        aboutwks.write(17, 1, 'OOS File', subtitle_format)
        aboutwks.write(18, 1, 'EXC File', subtitle_format)
        aboutwks.write(19, 1, 'FCA File', subtitle_format)
        aboutwks.write(22, 1, 'About', title_format)
        aboutwks.write(22, 2, '', bottom_format)
        aboutwks.write(22, 3, '', bottom_format)
        aboutwks.write(22, 4, '', bottom_format)
        aboutwks.write(22, 5, '', bottom_format)
        aboutwks.write(22, 6, '', bottom_format)
        aboutwks.write(22, 7, '', bottom_format)
        aboutwks.write(24, 1, 'Developer', subtitle_format)
        aboutwks.write(25, 1, 'Email', subtitle_format)
        # correct header formatting for each worksheet
        oosdeets.set_row(0, None, left_format)
        doideets.set_row(0, None, left_format)
        fcadeets.set_row(0, None, left_format)
        dnpdeets.set_row(0, None, left_format)
        datadict.set_row(0, None, left_format)
        # increase progress bar and update window
        p_step += randint(4, 8)
        progr_bar['value'] = p_step
        progr_l_p.config(text=str(p_step) + '%')
        progr_bar.update_idletasks()
        sleep(.3)
        # fill out the About tab column C
        aboutwks.write(7, 2, aboutshiplist, left_format)
        aboutwks.write(8, 2, aboutdistlist, left_format)
        aboutwks.write(9, 2, used_weeks, left_format)
        aboutwks.write(10, 2, current_date)
        aboutwks.write(11, 2, current_time)
        aboutwks.write(17, 2, oosfile_name, left_format)
        aboutwks.write(18, 2, onifile_name, left_format)
        aboutwks.write(19, 2, fcafile_name, left_format)
        aboutwks.write(24, 2, 'Joseph Arnson')
        aboutwks.write_url('C26', 'mailto:Joseph.Arnson@MolsonCoors.com', string='Joseph.Arnson@MolsonCoors.com')
        # increase progress bar and update window
        p_step = 100
        progr_bar['value'] = p_step
        progr_l_p.config(text=str(p_step) + '%')
        progr_bar.update_idletasks()
        sleep(.3)
        # end the progress bar and update window
        progr_bar.stop()
        progr_bar.update_idletasks()
        # kill progress bar and window
        progr_bar.destroy()
        progr_bar_win.destroy()
        # end time
        end_time = datetime.datetime.now() - begin_time
        end_time = str(end_time)
        aboutwks.write(12, 2, end_time)
        # close the Pandas Excel writer and output the Excel file
        writer.save()
        # send message if completion
        if not savepath:
            return
        ctypes.windll.user32.MessageBoxW(0, 'Your Distributor Scorecard has been completed successfully!',
                                         'Scorecard Completed', 0)
    except:  # error handling for any reason
        # check if new directory was made
        if path.isdir(fpathname):
            # deleted newly created directory and any files inside
            rmtree(fpathname)
        # check if progress bar exists and destroy it
        if progr_bar:
            progr_bar.destroy()
        # check if progress bar window exists and destroy it
        if progr_bar_win:
            progr_bar_win.destroy()
        # send popup error message
        ctypes.windll.user32.MessageBoxW(0,
                                         'An error was encountered while preparing the Distributor Scorecard. Please check your files and try again. For best results, ensure both the Distributor Scorecard application and source files are saved locally on your computer.',
                                         'Scorecard Error', 0)
    finally:
        # check if root window exists and destroy it
        if root:
            root.destroy()
        # reopen scorecard menu
        scorecard_menu()
    return savepath


def upd_label1(txt):
    """Function to label 1"""
    label1.set(txt)
    return


def upd_label2(txt):
    """Function to label 2"""
    label2.set(txt)
    return


def upd_label3(txt):
    """Function to label 3"""
    label3.set(txt)
    return


def open_oosfile():
    """Select a Distributor Scorecard OOS Report file"""
    Tk().withdraw()  # prevent root window
    global oosfilenamex
    oosfilenamex = askopenfilename(title='Select a OOS Scorecard Report',
                                   filetypes=[('Excel files', '.xlsx .xls')])
    if oosfilenamex != '':
        label1 = 'File selected'
        upd_label1(label1)
    else:
        label1 = 'Select a OOS Scorecard Report'
        upd_label1(label1)
    return oosfilenamex, label1


def open_excfile():
    """Select a Distributor Scorecard EXC Report file"""
    Tk().withdraw()  # prevent root window
    global onifilenamex
    onifilenamex = askopenfilename(title='Select an EXC Scorecard Report',
                                   filetypes=[('Excel files', '.xlsx .xls')])
    if onifilenamex != '':
        label2 = 'File selected'
        upd_label2(label2)
    else:
        label2 = 'Select an EXC Scorecard Report'
        upd_label2(label2)
    return oosfilenamex, label2


def open_fcafile():
    """Select a Distributor Scorecard DOI Report file"""
    Tk().withdraw()  # prevent root window
    global fcafilenamex
    fcafilenamex = askopenfilename(title='Select a FCA Scorecard Report',
                                   filetypes=[('Excel files', '.xlsx .xls')])
    if fcafilenamex != '':
        label3 = 'File selected'
        upd_label3(label3)
    else:
        label3 = 'Select a FCA Scorecard Report'
        upd_label3(label3)
    return fcafilenamex, label3


def scorecard_maker_btn():
    """Button that completes file loading and launches scorecard_maker"""
    global erunname
    shiptonumber = str(erunname.get()).upper()
    if oosfilenamex == '' or onifilenamex == '' or fcafilenamex == '':
        ctypes.windll.user32.MessageBoxW(0, 'Please select all files needed for processing.', 'Files not selected', 0)
    elif shiptonumber == '':
        ctypes.windll.user32.MessageBoxW(0, 'Please enter a scorecard title for processing.', 'Title not selected', 0)
    else:
        scorecard_maker(oosfilenamex, onifilenamex, fcafilenamex, shiptonumber)
        ctypes.windll.user32.MessageBoxW(0, 'Your Distributor Scorecard has been completed successfully!',
                                         'Scorecard Completed', 0)
        root.destroy()
        scorecard_menu()
    return


def scorecard_menu():
    # ----- main --> define variables
    global oosfilenamex
    global onifilenamex
    global fcafilenamex
    global root
    oosfilenamex = ''
    onifilenamex = ''
    fcafilenamex = ''
    lfht = 215
    lfwt = 600
    coltwo = 365
    coloff = coltwo + 43
    # ----- create widget variables
    bht = 1  # button height
    bwt = 6  # button width
    # ----- create root window
    root = tk.Tk()
    # set as active window
    root.focus_force()
    # Gets the requested values of the height and width
    windowWidth = root.winfo_reqwidth()
    windowHeight = root.winfo_reqheight()
    # Gets both half the screen width/height and window width/height
    positionRight = int(root.winfo_screenwidth() / 2 - windowWidth / 2)
    positionDown = int(root.winfo_screenheight() / 2 - windowHeight / 2)
    positionRight = int(positionRight - (windowWidth * 1.5))
    positionDown = int(positionDown - (windowHeight / 2))
    # Positions the window in the center of the page.
    root.geometry("+{}+{}".format(positionRight, positionDown))
    # name window
    # apptitle = 'Distributor Scorecard v1.2.3'
    root.title(apptitle)
    mc_icon = resource_path('mc_icon.ico')
    root.iconbitmap(mc_icon)
    # create gui canvas
    canvas = tk.Canvas(root, height=lfht, width=lfwt)
    canvas.pack()
    # ----- Design GUI
    frame = tk.Frame(root, bg='#D8E0E4')
    frame.place(relx=0, rely=0, relwidth=1, relheight=1)
    finfo = Frame(root, bd=0, height=130, width=575, highlightbackground='#091F3F',
                  highlightcolor='#D8E0E4', highlightthickness=1).place(x=15, y=75)
    # create labels and buttons
    global label1
    label1 = StringVar()
    label1.set('Select an Out of Stock Report')
    global label2
    label2 = StringVar()
    label2.set('Select an Execution Data Report')
    global label3
    label3 = StringVar()
    label3.set('Select a Forecast Accuracy Report')
    # ----- Main Window frames and widgets
    tcenter = (lfwt / 2) + 30
    lflabel1 = Label(root, text='Distributor Scorecard', fg='#091F3F', bg='#D8E0E4',
                     font=('Tuno', 32)).place(x=tcenter, y=8, anchor='n')
    lflabel2 = Label(root, text='by Joseph Arnson', fg='#091F3F',
                     font=('Tuno', 14)).place(x=coltwo, y=86, anchor='nw')
    lwvbtn = Button(root, text='OOS', width=bwt, command=open_oosfile,
                    font=('Century_Gothic', 12), bg='#FFB000', fg='#091F3F').place(x=30, y=90)
    lwvlbl = Label(root, textvariable=label1, font=('Century_Gothic', 11)).place(x=100, y=95)
    updbtn = Button(root, text='EXC', width=bwt, command=open_excfile,
                    font=('Century_Gothic', 12), bg='#FF6912', fg='#091F3F').place(x=30, y=122)
    updlbl = Label(root, textvariable=label2, font=('Century_Gothic', 11)).place(x=100, y=127)
    atpbtn = Button(root, text='FCA', width=bwt, command=open_fcafile,
                    font=('Century_Gothic', 12), bg='#1496FF', fg='#091F3F').place(x=30, y=154)
    l3 = Label(root, textvariable=label3, font=('Century_Gothic', 11)).place(x=100, y=159)
    # ShipTo entry
    response96 = StringVar()
    global erunname
    erunname = Entry(root, textvariable=response96, width=20, state='normal', justify=CENTER,
                     font=('Century_Gothic', 11))
    erunname.place(x=coloff, y=122)
    # get current date to name directory output name
    runname = ''
    response96.set(runname)
    # label 7
    labelrunname = Label(root, text='Title', font=('Century_Gothic', 11)).place(x=coltwo, y=122)
    # variable responses
    response96x = str(response96.get())
    # create distributor scorecard button
    global optbtn1
    optbtn1 = Button(root, text='Create Scorecard', height=bht, width=22, command=scorecard_maker_btn,
                     font=('Century_Gothic', 12), fg='#091F3F', bg='#D8E0E4')
    optbtn1.place(x=coltwo, y=154)
    optbtn2 = Button(root, text='Back', height=bht, width=bwt, command=dist_go_back,
                     font=('Century_Gothic', 12), fg='#091F3F', bg='#D8E0E4')
    optbtn2.place(x=30, y=30)
    # ----- create window loop
    root.mainloop()
    return


def open_scorecard_menu():
    """From main menu open the Distributor Scorecard menu and close main menu"""
    win.destroy()
    scorecard_menu()
    return


def dist_go_back():
    # close distributor scorecard menu
    root.destroy()
    # open jars main menu
    jars_menu()
    return


"""Confirmations and Winshuttle Functions"""


def vbs_converter_creator():
    """Creates Excel to csv vbs script at application realpath directory"""
    # write vbscript to file
    vbscript = """if WScript.Arguments.Count < 3 Then
        WScript.Echo "Please specify the source and the destination files. Usage: ExcelToCsv <//NoLogo> <xls/xlsx source file> <csv destination file> <worksheet number (starts at 1)>"
        Wscript.Quit
    End If

    csv_format = 6

    Set objFSO = CreateObject("Scripting.FileSystemObject")

    src_file = objFSO.GetAbsolutePathName(WScript.Arguments.Item(0))
    dest_file = objFSO.GetAbsolutePathName(WScript.Arguments.Item(1))
    worksheet_number = CInt(WScript.Arguments.Item(2))

    Dim oExcel
    Set oExcel = CreateObject("Excel.Application")

    Dim oBook
    Set oBook = oExcel.Workbooks.Open(src_file)
    oBook.Worksheets(worksheet_number).Activate

    oBook.SaveAs dest_file, csv_format

    oBook.Close False
    oExcel.Quit
    """
    f = open('ExcelToCsv.vbs', 'wb')
    f.write(vbscript.encode('utf-8'))
    f.close()
    return


def priority_msku(pa_df: pd.DataFrame) -> pd.DataFrame:
    """
    This function takes in a dataframe of product authorization SKUs
    and returns a dataframe of the MSKU pairs with their priority.

    Args:
        pa_df (pd.DataFrame): A dataframe of the ShipTo-OSKU pairs with all product authorization data.

    Returns:
        pa_df (pd.DataFrame): The updated dataframe of the ShipTo-OSKU pairs with their identified priority MSKU.
    """
    # recalculate priority MSKU to handle customer service defined priority ordering by adding 100 for sorting
    pa_df['MSKU_Priority'] = np.select([pa_df['MSKU_Priority'] == 0,
                                        pa_df['MSKU_Priority'] == 1,
                                        pa_df['MSKU_Priority'] == 99],
                                       [100, 101, 199],
                                       default=pa_df['MSKU_Priority'])
    # sort pa_df dataframe - lowest to highest
    pa_df.sort_values(by=['MSKU_Priority', 'ShipTo', 'OSKU'], ascending=[True, True, True], inplace=True)
    # drop duplicate pairs
    pa_df.drop_duplicates(subset='Key', inplace=True)
    # reset index
    pa_df.reset_index(drop=True, inplace=True)
    # undo priority MSKU by subtracting 100 for pairs that were added for sorting
    pa_df['MSKU_Priority'] = np.select([pa_df['MSKU_Priority'] == 100,
                                        pa_df['MSKU_Priority'] == 101,
                                        pa_df['MSKU_Priority'] == 199],
                                       [0, 1, 99],
                                       default=pa_df['MSKU_Priority'])
    # reindex dataframe
    pa_df = pa_df[['Key', 'MSKU', 'Contract']]
    # pa_df = pa_df[['Key', 'ShipTo', 'OSKU', 'MSKU', 'MSKU_Priority']]
    return pa_df


def order_week_reframe(or_df: pd.DataFrame) -> pd.DataFrame:
    """
    This function takes in a dataframe of order data and reframes it to separate
    week 1 and 2 tabular data into new series for sales orders, suggests, and original orders.
    Also replaces missing sales orders with the first associated sales order using the plant-soldto-ShipTo pair

    Args:
        or_df (pd.DataFrame): A dataframe of the order data.

    Returns:
        or_df (pd.DataFrame): The updated dataframe of the order data with data separated into week 1 and 2.
    """
    # create new dataframe with pivot table of suggest and orders in units
    nr_df = pd.pivot_table(or_df, values=['Suggest_Units', 'Original_Order_Units'],
                           index='Key', columns='Yr-Wk', aggfunc=np.sum)
    # rename columns based on position in dataframe
    nr_df.columns = ['Original Order (Units) 1', 'Original Order (Units) 2',
                     'Suggest (Units) 1', 'Suggest (Units) 2']
    # create copy of or_df dataframe
    or_df2 = or_df.copy()
    # create new series key-yr-wk
    or_df2['Key-Yr-Wk'] = or_df2['Key'].astype(str) + '-' + or_df2['Yr-Wk'].astype(str)
    # drop duplicates in key-yr-wk
    or_df2.drop_duplicates(subset='Key-Yr-Wk', inplace=True)
    # create new dataframe by grouby pair week and getting the sales order for each week
    so_df = pd.DataFrame(or_df2.groupby(['Key', 'Yr-Wk'])['SO'].sum())
    # unstack so_df dataframe
    so_df = so_df.unstack(level=1)
    # rename columns based on position in dataframe
    so_df.columns = ['SO 1', 'SO 2']
    # reset index
    so_df.reset_index(drop=False, inplace=True)
    # merge so_df dataframe with nr_df dataframe on the key
    nr_df = nr_df.merge(so_df, on='Key', how='left')
    # fill missing values with 0
    nr_df = nr_df.fillna(0)
    # drop duplicate pairs
    nr_df.drop_duplicates(subset='Key', inplace=True)
    # reindex or_df dataframe
    or_df = or_df[['Key', 'Plant', 'SoldTo', 'ShipTo', 'OSKU', 'Segment',
                   'ShipTo_Name', 'State', 'Mode', 'Brand', 'Description']]
    # merge nr_df dataframe with or_df dataframe on the key
    or_df = or_df.merge(nr_df, on='Key', how='left')
    # drop duplicate pairs
    or_df.drop_duplicates(subset='Key', inplace=True)
    # reindex or_df dataframe for sales order correction
    or_df = or_df[['Key', 'Plant', 'SoldTo', 'ShipTo', 'OSKU', 'Segment',
                   'ShipTo_Name', 'State', 'Mode', 'Brand',
                   'Description', 'SO 1', 'SO 2',
                   'Suggest (Units) 1', 'Suggest (Units) 2',
                   'Original Order (Units) 1', 'Original Order (Units) 2']]
    # create a key for Plant-SoldTo-ShipTo pairs in each dataframe
    or_df['PSS_Key'] = or_df['Plant'].astype(str) + '-' + \
                       or_df['SoldTo'].astype(str) + '-' + \
                       or_df['ShipTo'].astype(str)
    # copy or_df for each sales order week
    so1_df = or_df.copy()
    so2_df = or_df.copy()
    # reindex so dataframes
    so1_df = so1_df[['PSS_Key', 'SO 1']]
    so2_df = so2_df[['PSS_Key', 'SO 2']]
    # drop rows with no sales order
    so1_df = so1_df[so1_df['SO 1'] > 0]
    so2_df = so2_df[so2_df['SO 2'] > 0]
    # sort sales order dataframes by oldest sales orders first
    so1_df.sort_values(by='SO 1', ascending=True, inplace=True)
    so2_df.sort_values(by='SO 2', ascending=True, inplace=True)
    # rename columns for sales order dataframes
    so1_df.columns = ['PSS_Key', 'SO 1 Backup']
    so2_df.columns = ['PSS_Key', 'SO 2 Backup']
    # merge so dataframes with or_df dataframe on the PSS_Key
    or_df = or_df.merge(so1_df, on='PSS_Key', how='left')
    # drop duplicate rows in or_df dataframe
    or_df.drop_duplicates(subset='Key', inplace=True)
    # merge so dataframes with or_df dataframe on the PSS_Key
    or_df = or_df.merge(so2_df, on='PSS_Key', how='left')
    # drop duplicate rows in or_df dataframe
    or_df.drop_duplicates(subset='Key', inplace=True)
    # fill missing values with 0
    or_df = or_df.fillna(0)
    # replace missing sales orders with backup sales orders
    or_df['SO 1'] = np.where((or_df['SO 1'] == 0) & (or_df['SO 1 Backup'] != 0), or_df['SO 1 Backup'], or_df['SO 1'])
    or_df['SO 2'] = np.where((or_df['SO 2'] == 0) & (or_df['SO 2 Backup'] != 0), or_df['SO 2 Backup'], or_df['SO 2'])
    # # reindex or_df dataframe
    or_df = or_df[['Key', 'Plant', 'ShipTo', 'OSKU', 'Segment',
                   'ShipTo_Name', 'State', 'Mode', 'Brand',
                   'Description', 'SO 1', 'SO 2',
                   'Suggest (Units) 1', 'Suggest (Units) 2',
                   'Original Order (Units) 1', 'Original Order (Units) 2']]
    # reset index
    or_df.reset_index(drop=True, inplace=True)
    return or_df


def procurement_msku(pt_df: pd.DataFrame) -> pd.DataFrame:
    """
    This function takes in a dataframe of product masterdata and identifies the correct procurement type
    for each Plant-MSKU pair.

    Args:
        pt_df (pd.DataFrame): A dataframe of the Plant-MSKU pairs with all product master data.

    Returns:
        pt_df (pd.DataFrame): The updated dataframe of the Plant-MSKU pairs with their identified procurement type.
    """
    # create new column if internally procured - proc_type either E or X
    pt_df['Split'] = np.where((pt_df['Proc_Type'] == 'E') | (pt_df['Proc_Type'] == 'X'), 'Internal', 'External')
    # copy md_df dataframe
    nd_df = pt_df.copy()  # no duplicates dataframe
    # sort nd_df dataframe by plant, OSKU, proc_type - [lowest to highest, lowest to highest, reverse alphabetical]
    nd_df.sort_values(by=['Plant', 'OSKU', 'Split'], ascending=[True, True, False], inplace=True)
    # reindex nd_df dataframe
    nd_df = nd_df[['OSKU', 'Split']]
    # drop duplicate pairs
    nd_df.drop_duplicates(subset='OSKU', inplace=True)
    # merge nd_df dataframe with pt_df dataframe on the key
    pt_df = pt_df.merge(nd_df, on='OSKU', how='left')
    # create list of conditions for internal inclusion for F-Push SKUs
    condlist = [(pt_df['Split_x'] == 'Internal'),
                (pt_df['Split_x'] == 'External') & (pt_df['Split_y'] == 'Internal'),
                (pt_df['Split_x'] == 'External') & (pt_df['Split_y'] == 'External')]
    # create list of values for internal inclusion for F-Push SKUs
    choicelist = ['Internal', 'Internal', 'External']
    # check if an MSKU should be included internally or externally
    pt_df['Include'] = np.select(condlist, choicelist, default='External')
    # reindex dataframe
    pt_df = pt_df[['Agg_Key', 'Include']]
    return pt_df


def deliv_week_reframe(dl_df: pd.DataFrame, order_week_list: list) -> pd.DataFrame:
    """
    This function takes in a dataframe of order delivery and reframes it to separate
    week 1 and 2 tabular data into new series for delivery arrivals.

    Args:
        dl_df (pd.DataFrame): A dataframe of the delivery arrivals data.
        order_week_list (list): A list of the order weeks.

    Returns:
        dl_df (pd.DataFrame): The updated dataframe of the delivery arrivals data separated into week 1 and 2.
    """
    # get date three and four weeks from now
    wp3_today = datetime.datetime.today() + datetime.timedelta(days=21)
    wp4_today = datetime.datetime.today() + datetime.timedelta(days=28)
    # if week list is empty then calculate weeks
    if len(order_week_list) == 0:
        # create yr-wk three and four weeks from now
        yr_wk1 = f"{wp3_today.strftime('%Y')}-{wp3_today.strftime('%W')}"
        yr_wk2 = f"{wp4_today.strftime('%Y')}-{wp4_today.strftime('%W')}"
        # create list of order weeks
        order_week_list = [yr_wk1, yr_wk2]
    # slice order list to first two items
    order_week_list = order_week_list[:2]
    # check if there are fewer than two weeks of data
    if dl_df.nunique()['Yr-Wk'] < 2:
        # check if each week exists in the dataframe
        if yr_wk1 not in dl_df['Yr-Wk']:
            # create row to append to dataframe with first week in list
            wf1 = pd.DataFrame(np.array([[0, 0, 0, 0, order_week_list[0], 0]]),
                               columns=['Merge_Key', 'Key', 'ShipTo', 'OSKU', 'Yr-Wk', 'Arrivals_Units'])
            # concatenate dataframes
            dl_df = pd.concat([dl_df, wf1], ignore_index=True)
        if yr_wk2 not in dl_df['Yr-Wk']:
            # create row to append to dataframe with second week in list
            wf2 = pd.DataFrame(np.array([[0, 0, 0, 0, order_week_list[1], 0]]),
                               columns=['Merge_Key', 'Key', 'ShipTo', 'OSKU', 'Yr-Wk', 'Arrivals_Units'])
            # concatenate dataframes
            dl_df = pd.concat([dl_df, wf2], ignore_index=True)
    # create new dataframe with pivot table of delivery arrivals in units
    nl_df = pd.pivot_table(dl_df, values='Arrivals_Units', index='Key', columns='Yr-Wk', aggfunc=np.sum)
    # rename columns based on position in dataframe
    nl_df.columns = ['Arrivals (Units) 1', 'Arrivals (Units) 2']
    # fill missing values with 0
    nr_df = nl_df.fillna(0)
    # reindex or_df dataframe
    dl_df = dl_df[['Key', 'ShipTo', 'OSKU']]
    # merge nr_df dataframe with or_df dataframe on the key
    dl_df = dl_df.merge(nr_df, on='Key', how='left')
    return dl_df


def excel_to_csv_converter(cutfile: str, savepath: str) -> str:
    """Convert source report into multiple csv files for faster processing"""
    # create vbs conversion script at application realpath
    vbs_converter_creator()
    # create a list with sheet numbers you want to process - 8 worksheets
    sheets = map(str, range(1, 8 + 1))
    # create new blank dataframe
    df = {}
    # find script directory for Excel to csv vbs script
    exctocsvvbs = os_split_fixer(os.path.join(os.path.dirname(os.path.realpath(__file__)), 'ExcelToCsv.vbs'))
    # iterate through list of sheets to convert and save each as csv
    for sheet in sheets:
        # create new file for each csv
        csv = savepath + sheet + '.csv'
        # call vbs script
        call(['cscript.exe', '-nologo', exctocsvvbs, cutfile, csv, sheet])
        # read each csv to dataframe
        df[sheet] = pd.read_csv(csv, dtype=object, engine='python', encoding='windows-1252')
    # delete vbs script file
    os.remove(exctocsvvbs)
    return


def excel_cut_source_data(cutfile: str, line_mapping: str, arrivals: str):
    """
    Prepare data from the cut report data source BOBJ file

    Args:
        cutfile (str): path to cut report data source BOBJ file
        line_mapping (str): path to OSKU line mapping file
        arrivals (str): path to delivery file
        savepath (str): path to save cut report data source files

    Returns:
        ff (pd.dataframe): cut report data source dataframe
        df9 (pd.dataframe): production dataframe
        df8 (pd.dataframe): masterdata dataframe
    """
    # define progress bar global variables
    global my_progress
    global pbar
    # turn off warnings
    warnings.filterwarnings('ignore', category=UserWarning)
    warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)
    # initialize list to store data
    dataframes = []
    # create Excel file as object
    xl = pd.ExcelFile(cutfile)
    # loop through sheets in Excel file
    for sheet in xl.sheet_names:
        # create dataframe from sheet
        df = xl.parse(sheet)
        # add dataframe to list
        dataframes.append(df)
    # read in new dataframes and fill blanks with 0
    df1 = dataframes[0].fillna(value=0)  # orders
    df2 = dataframes[1].fillna(value=0)  # forecast_dist
    df3 = dataframes[2].fillna(value=0)  # forecast_le
    df4 = dataframes[3].fillna(value=0)  # inventory
    df5 = dataframes[4].fillna(value=0)  # str_actuals
    df6 = dataframes[5].fillna(value=0)  # SKU_data
    df7 = dataframes[6].fillna(value=0)  # periodicity
    df8 = dataframes[7].fillna(value=0)  # masterdata
    df9 = pd.read_excel(line_mapping).fillna(value=0)  # production
    df10 = pd.read_excel(arrivals).fillna(value=0)  # deliveries
    # deep copy master data dataframe for merging
    df0 = df8.copy(deep=True)
    # get list of weeks listed in order data
    orderweeks = df1['Yr-Wk'].unique()
    # slice order list to first two items
    orderweeks = orderweeks[:2]
    # sort orders in ascending order
    orderweeks.sort()
    # update order dataframe to separate by week
    df1 = order_week_reframe(df1)
    # calculate priority MSKU for each pair
    df6 = priority_msku(df6)  # df6[['Key', 'MSKU', 'Contract']]
    # calculate internal or external status for each pair in masterdata
    split_df = procurement_msku(df8)  # pt_df[['Agg_Key', 'Include']]
    # update order dataframe to separate by week
    df10 = deliv_week_reframe(df10, orderweeks)
    # remove unnecessary columns from dataframes
    df2 = df2[['Key', 'Yr-Wk', 'Dist_Fcst_STR_Units']]
    df3 = df3[['Key', 'MC_Fcst_STR_Units']]
    df4 = df4[['Key', 'Act_Beg_Inv_Units']]
    df5 = df5[['Key', 'Yr-Wk', 'Act_STR_Units']]
    df7 = df7[['MSKU', 'Run_Freq']]
    df0 = df0[['MSKU', 'Units_p_Pal', 'BBL_Conv']]
    df10 = df10[['Key', 'Arrivals (Units) 1', 'Arrivals (Units) 2']]
    # format dataframe series astype
    df2['Dist_Fcst_STR_Units'] = df2['Dist_Fcst_STR_Units'].astype(int)
    df3['MC_Fcst_STR_Units'] = df3['MC_Fcst_STR_Units'].astype(int)
    df4['Act_Beg_Inv_Units'] = df4['Act_Beg_Inv_Units'].astype(int)
    df5['Act_STR_Units'] = df5['Act_STR_Units'].astype(int)
    df0['Units_p_Pal'] = df0['Units_p_Pal'].astype(int)
    df0['BBL_Conv'] = df0['BBL_Conv'].astype(float)
    df10['Arrivals (Units) 1'] = df10['Arrivals (Units) 1'].astype(int)
    df10['Arrivals (Units) 2'] = df10['Arrivals (Units) 2'].astype(int)
    # create sum of both original orders for sorting
    df1['Total_Original_Order_Units'] = df1['Original Order (Units) 1'] + df1['Original Order (Units) 2']
    # sort dataframes by highest total order volumes
    df1 = df1.sort_values(by='Total_Original_Order_Units', ascending=False)
    # create pivot table of distributor forecasts
    df2 = pd.pivot_table(df2, values='Dist_Fcst_STR_Units', index='Key', columns='Yr-Wk', aggfunc=np.sum)
    # reset index
    df2.reset_index(inplace=True)
    # rename columns
    df2 = df2.rename(columns={df2.columns[1]: 'DFcst_Wk0',
                              df2.columns[2]: 'DFcst_Wk1',
                              df2.columns[3]: 'DFcst_Wk2',
                              df2.columns[4]: 'DFcst_Wk3',
                              df2.columns[5]: 'DFcst_Wk4'})
    # remove unnecessary columns from dataframes
    df2 = df2[['Key', 'DFcst_Wk0', 'DFcst_Wk1', 'DFcst_Wk2', 'DFcst_Wk3', 'DFcst_Wk4']]
    # calculate forecast average to replace missing values with
    df2['DFcst_Avg'] = df2[['DFcst_Wk0', 'DFcst_Wk1', 'DFcst_Wk2', 'DFcst_Wk3', 'DFcst_Wk4']].mean(axis=1)
    # round forecast average to integer
    df2['DFcst_Avg'] = df2['DFcst_Avg'].round()
    # fill blank values with zeros
    df2 = df2.fillna(value=999999)
    # replace missing values with average forecast
    df2['DFcst_Wk0'] = np.where(df2['DFcst_Wk0'] == 999999, df2['DFcst_Avg'], df2['DFcst_Wk0'])
    df2['DFcst_Wk1'] = np.where(df2['DFcst_Wk1'] == 999999, df2['DFcst_Avg'], df2['DFcst_Wk1'])
    df2['DFcst_Wk2'] = np.where(df2['DFcst_Wk2'] == 999999, df2['DFcst_Avg'], df2['DFcst_Wk2'])
    df2['DFcst_Wk3'] = np.where(df2['DFcst_Wk3'] == 999999, df2['DFcst_Avg'], df2['DFcst_Wk3'])
    df2['DFcst_Wk4'] = np.where(df2['DFcst_Wk4'] == 999999, df2['DFcst_Avg'], df2['DFcst_Wk4'])
    # fill blank values with zeros
    df2 = df2.fillna(value=0)
    df3 = df3.fillna(value=0)
    # summarize dataframe by week
    df5 = pd.DataFrame(df5.groupby('Key')['Act_STR_Units'].mean())
    # rename columns
    df5 = df5.rename(columns={'Act_STR_Units': 'Avg_STR_Units'})
    # create list of dataframes to merge
    dflist1 = [df1, df3, df4]
    # reduce dataframes into combined frame -> merge_key
    df = reduce(lambda left, right: pd.merge(left, right, on='Key', how='outer'), dflist1)
    # create list of dataframes to merge
    dflist2 = [df, df2, df5, df6]
    # reduce dataframes into combined frame -> key
    df = reduce(lambda left, right: pd.merge(left, right, on='Key', how='outer'), dflist2)
    # create list of dataframes to merge
    dflist3 = [df, df0, df7]
    # reduce dataframes into combined frame -> MSKU
    ff = reduce(lambda left, right: pd.merge(left, right, on='MSKU', how='outer'), dflist3)
    # create list of dataframes to merge
    dflist4 = [ff, df10]
    # reduce dataframes into combined frame -> merge_key
    ff = reduce(lambda left, right: pd.merge(left, right, on='Key', how='outer'), dflist4)
    # drop duplicates in key column
    ff = ff.drop_duplicates(subset=['Key'])
    # fill blank values with zeros
    ff = ff.fillna(value=0)
    # drop rows where merge key is 0 from dataframe
    ff = ff[ff['Key'] != 0]
    # drop rows where production line 0 from dataframe
    df9 = df9[df9['Line'] != 0]
    # create new series
    ff['Agg_Key'] = ff['Plant'].astype(str).str[:4] + '-' + ff['OSKU'].astype(str).str[:6]
    # merge split_df with ff to determine what should be included in either the confirmations or STO planner
    ff = pd.merge(ff, split_df, on=['Agg_Key'], how='left')
    # identify rows where both original orders are 0 and MSKU is 0
    ff['Blank_Order'] = np.where((ff['Original Order (Units) 1'] == 0) &
                                 (ff['Original Order (Units) 2'] == 0) &
                                 (ff['MSKU'] == 0), 1, 0)
    # drop rows where blank order is 1
    ff = ff[ff['Blank_Order'] != 1]
    # fill blank values with zeros
    ff = ff.fillna(value=0)
    df9 = df9.fillna(value=0)
    # return order dataframe, production dataframe, masterdata dataframe, and order weeks list
    return [ff, df9, df8, orderweeks]


def conf_planner_creator(cutfile, arrfile, olmfile, conftype):
    # turn off warnings for chained asignments
    pd.options.mode.chained_assignment = None  # default='warn'
    # define progress bar global variables
    global my_progress
    global pbar
    # time start
    begin_time = datetime.datetime.now()
    # check if internal or external confirmations for naming
    if conftype == 0:
        foldername = 'Confirmations'
    else:
        foldername = 'STO Confirmations'
    # check for errors loading file
    try:
        # create new unique directory
        savepath, fpathname, cdirspath = conf_uniq_dir_maker(foldername)
        # check if no save destination selected
        if not savepath:
            # create popup if no destination is selected
            ctypes.windll.user32.MessageBoxW(0, 'Save location select cancelled.', 'Confirmations', 0)
            return
    except:
        # deleted newly created directory and any files inside
        shutil.rmtree(fpathname)
        # create pop up message warning
        ctypes.windll.user32.MessageBoxW(0,
                                         'An error has occurred. Please make sure the selected Weekly Confirmations - Data Source file is closed and try again.',
                                         'Confirmations', 0)
        return
    # try to load in data from Excel sources
    try:
        # load in cut report source data using Excel conversion if unicode error detected
        frames = excel_cut_source_data(cutfile, olmfile, arrfile)
        # get first dataframe from source file
        df = frames[0]
        # get second dataframe from source file
        rf = frames[1]
        # get the third dataframe from source file
        md = frames[2]
        # get list of weeks listed in order data
        orderweeks = frames[3]
        # for each item in list, get value after '-' and convert to integer
        orderweeks = [int(x.split('-')[1]) for x in orderweeks]
        # create new unique folder name with plant list without overwriting
        fpathnewnam = uniq_file_maker(f"{cdirspath}{os.sep}WK{str(orderweeks[0])}-{str(orderweeks[1])} {foldername}")
        # update foldername with used weeks
        foldername = f"WK{str(orderweeks[0])}-{str(orderweeks[1])} {foldername}"
        # rename the newly created directory
        os.rename(savepath, fpathnewnam)
        # update savepath
        savepath = os_split_fixer(f"{fpathnewnam}{os.sep}")
    # if any other error occurs
    except:
        # create pop up message warning
        ctypes.windll.user32.MessageBoxW(0,
                                         'An error has occurred loading in the Confirmations Data Source file. Please make sure the selected source files are closed and that it is not currently syncing with OneDrive and try again.\nIf the issue persists, please contact your team lead and the JARS application administrator.',
                                         'Confirmations', 0)
        return
    # try to generate confirmations planner
    try:
        # check if internal or external confirmations
        if conftype == 0:
            # only include rows where Include == Internal
            df = df[df['Include'] == 'Internal']
        else:
            # only include rows where Include == Internal
            df = df[df['Include'] == 'External']
        # copy masterdata dataframe
        md_copy = md.copy()
        # reindex dataframe
        md_copy = md_copy[['Agg_Key', 'MSKU']]
        # remove production line 0 from dataframe
        rf = rf[rf['Line'] != 0]
        # set series as integers
        df['ShipTo'] = df['ShipTo'].astype(int)
        df['OSKU'] = df['OSKU'].astype(int)
        df['MSKU'] = df['MSKU'].astype(int)
        # create new series
        df['MSKU_Key'] = df['ShipTo'].astype(str) + '-' + df['MSKU'].astype(str)
        # rename columns
        df = df.rename(columns={'Key': 'OSKU_Key'})
        # fill blank values with zeros
        df = df.fillna(value=0)
        # create new series
        df['Orig_Beg_Inv_Units'] = df['Act_Beg_Inv_Units']
        # use vectorization to set all negatives in Arrivals_Units to zero
        df['Arrivals (Units) 1'] = np.where(df['Arrivals (Units) 1'] < 0, 0, df['Arrivals (Units) 1'])
        df['Arrivals (Units) 2'] = np.where(df['Arrivals (Units) 2'] < 0, 0, df['Arrivals (Units) 2'])
        # add arrivals to inventory
        df['Act_Beg_Inv_Units'] = df['Act_Beg_Inv_Units'] + df['Arrivals (Units) 1']
        # drop duplicates in key column
        df = df.drop_duplicates(subset='OSKU_Key', keep='first')
        # calculate conditions for calculating average suggest
        sugg_conds = [
            ((df['Suggest (Units) 1'] > 0) & (df['Suggest (Units) 2'] > 0)),
            ((df['Suggest (Units) 1'] > 0) & (df['Suggest (Units) 2'] <= 0)),
            ((df['Suggest (Units) 1'] <= 0) & (df['Suggest (Units) 2'] > 0)),
            ((df['Suggest (Units) 1'] <= 0) & (df['Suggest (Units) 2'] <= 0)),
        ]
        # calculate choices for calculating average suggest
        sugg_chois = [
            (df['Suggest (Units) 1'] + df['Suggest (Units) 2']) / 2,
            df['Suggest (Units) 1'],
            df['Suggest (Units) 2'],
            0,
        ]
        # calculate average suggest units
        df['Suggest_Units'] = np.select(sugg_conds, sugg_chois, default=0)
        # create conditions to calculate Molson Coors baseline
        xtrcons1 = [(df['Suggest_Units'] > 0) & (df['MC_Fcst_STR_Units'] > 0),
                    (df['Suggest_Units'] > 0) & (df['MC_Fcst_STR_Units'] <= 0),
                    (df['Suggest_Units'] <= 0) & (df['MC_Fcst_STR_Units'] > 0),
                    (df['Suggest_Units'] <= 0) & (df['MC_Fcst_STR_Units'] <= 0)]
        # create results to calculate expected strs
        xtrress1 = [(df['Suggest_Units'] + df['MC_Fcst_STR_Units']) / 2,
                    df['Suggest_Units'], df['MC_Fcst_STR_Units'], 99999]
        # create expected str series by averaging suggest and LE
        df['MC_Baseline'] = np.select(xtrcons1, xtrress1, default=0)
        # create conditions to calculate STR baseline
        xtrcons2 = [(df['Suggest_Units'] > 0) & (df['MC_Fcst_STR_Units'] > 0) & (df['Avg_STR_Units'] > 0),
                    (df['Suggest_Units'] <= 0) & (df['MC_Fcst_STR_Units'] > 0) & (df['Avg_STR_Units'] > 0),
                    (df['Suggest_Units'] > 0) & (df['MC_Fcst_STR_Units'] <= 0) & (df['Avg_STR_Units'] > 0),
                    (df['Suggest_Units'] > 0) & (df['MC_Fcst_STR_Units'] > 0) & (df['Avg_STR_Units'] <= 0),
                    (df['Suggest_Units'] <= 0) & (df['MC_Fcst_STR_Units'] <= 0) & (df['Avg_STR_Units'] > 0),
                    (df['Suggest_Units'] > 0) & (df['MC_Fcst_STR_Units'] <= 0) & (df['Avg_STR_Units'] <= 0),
                    (df['Suggest_Units'] <= 0) & (df['MC_Fcst_STR_Units'] > 0) & (df['Avg_STR_Units'] <= 0),
                    (df['Suggest_Units'] <= 0) & (df['MC_Fcst_STR_Units'] <= 0) & (df['Avg_STR_Units'] <= 0)]
        # create results to calculate expected strs
        xtrress2 = [(.25 * df['Suggest_Units']) + (.25 * df['MC_Fcst_STR_Units']) + (.5 * df['Avg_STR_Units']),
                    (.33 * df['Suggest_Units']) + (.67 * df['Avg_STR_Units']),
                    (.33 * df['MC_Fcst_STR_Units']) + (.67 * df['Avg_STR_Units']),
                    (.5 * df['Suggest_Units']) + (.5 * df['MC_Fcst_STR_Units']),
                    df['Avg_STR_Units'],
                    df['Suggest_Units'],
                    df['MC_Fcst_STR_Units'],
                    999999]
        # create expected str series by averaging STRs, suggest, and LE
        df['STR_Baseline'] = np.select(xtrcons2, xtrress2, default=0)
        # dynamically check which baseline should be used for each ShipTo OSKU pair
        df['Baseline'] = np.where(df['Avg_STR_Units'] < df['MC_Baseline'], df['MC_Baseline'], df['STR_Baseline'])
        # compare each distributor forecast against their expected strs by week
        df['Egr_Flag1'] = np.where(df['DFcst_Wk1'] > df['Baseline'] * 2, 1, 0)
        df['Egr_Flag2'] = np.where(df['DFcst_Wk2'] > df['Baseline'] * 2, 1, 0)
        df['Egr_Flag3'] = np.where(df['DFcst_Wk3'] > df['Baseline'] * 2, 1, 0)
        df['Egr_Flag4'] = np.where(df['DFcst_Wk4'] > df['Baseline'] * 2, 1, 0)
        # create variable to sum egregious flag series
        egr_flags = df['Egr_Flag1'] + df['Egr_Flag2'] + df['Egr_Flag3'] + df['Egr_Flag4']
        # check if ShipTo-OSKU pair is considered an egregious forecaster -> 3 or more egregious forecast weeks
        df['Egr_Flag'] = np.where(egr_flags >= 3, 1, 0)
        # create conditions to curb forecasts by week
        egr_flag_con1 = (df['Egr_Flag1'] == 1) & (df['Egr_Flag'] == 1)
        egr_flag_con2 = (df['Egr_Flag2'] == 1) & (df['Egr_Flag'] == 1)
        egr_flag_con3 = (df['Egr_Flag3'] == 1) & (df['Egr_Flag'] == 1)
        egr_flag_con4 = (df['Egr_Flag4'] == 1) & (df['Egr_Flag'] == 1)
        # curb egregious forecasts in egregious week if ShipTo-OSKU pair is considered egregious
        df['DFcst_Wk1'] = np.where(egr_flag_con1, df['Baseline'] * 2, df['DFcst_Wk1'])
        df['DFcst_Wk2'] = np.where(egr_flag_con2, df['Baseline'] * 2, df['DFcst_Wk2'])
        df['DFcst_Wk3'] = np.where(egr_flag_con3, df['Baseline'] * 2, df['DFcst_Wk3'])
        df['DFcst_Wk4'] = np.where(egr_flag_con4, df['Baseline'] * 2, df['DFcst_Wk4'])
        # create cumulative forecast series
        df['Cm_DFcst_1_2'] = df['DFcst_Wk1'] + df['DFcst_Wk2']
        df['Cm_DFcst_1_3'] = df['DFcst_Wk1'] + df['DFcst_Wk2'] + df['DFcst_Wk3']
        df['Cm_DFcst_1_4'] = df['DFcst_Wk1'] + df['DFcst_Wk2'] + df['DFcst_Wk3'] + df['DFcst_Wk4']
        # create average forecast series
        df['Avg_DFcst'] = df[['DFcst_Wk1', 'DFcst_Wk2', 'DFcst_Wk3', 'DFcst_Wk4']].mean(axis=1)
        # reset index
        df.reset_index(drop=True, inplace=True)
        # fill blank values with zeros
        df = df.fillna(value=0)
        # correct series so there are no negative beginning inventories
        df['Act_Beg_Inv_Units'] = np.where(df['Act_Beg_Inv_Units'] < 0, 0, df['Act_Beg_Inv_Units'])
        # create variable for calculating ordered DOI
        pf_ordered_doi_start = df['Act_Beg_Inv_Units'] + df['Original Order (Units) 1'] - df['DFcst_Wk0']
        # create list of conditions to calculate ordered DOI
        doiweeks1 = [
            (pf_ordered_doi_start <= 0),
            (pf_ordered_doi_start <= df['DFcst_Wk1']),
            (pf_ordered_doi_start <= df['Cm_DFcst_1_2']),
            (pf_ordered_doi_start <= df['Cm_DFcst_1_3']),
            (pf_ordered_doi_start <= df['Cm_DFcst_1_4']),
            (pf_ordered_doi_start > df['Cm_DFcst_1_4']),
        ]
        # create list of ordered DOI calculation options
        weeks1 = [
            0,
            round(0 + (pf_ordered_doi_start / df['DFcst_Wk1']) * 7, 1),
            round(7 + ((pf_ordered_doi_start - df['DFcst_Wk1']) / df['DFcst_Wk2']) * 7, 1),
            round(14 + ((pf_ordered_doi_start - df['Cm_DFcst_1_2']) / df['DFcst_Wk3']) * 7, 1),
            round(21 + ((pf_ordered_doi_start - df['Cm_DFcst_1_3']) / df['DFcst_Wk4']) * 7, 1),
            round(28 + ((pf_ordered_doi_start - df['Cm_DFcst_1_4']) / df['Avg_DFcst']) * 7, 1),
        ]
        # create new series based on numpy select conditions
        df['Ordered DOI 1'] = np.select(doiweeks1, weeks1, default=0)
        # create condition to recalculate ordered DOI 0 when there was an order placed
        df['Ordered DOI 1'] = np.where((df['Ordered DOI 1'] == 0) & (df['Original Order (Units) 1'] > 0),
                                       round(0 + (df['Original Order (Units) 1'] / df['DFcst_Wk1']) * 7, 1),
                                       df['Ordered DOI 1'])
        # create condition to set any remaining ordered DOI == 0 as 0.00001 to allow downstream planning activities
        df['Ordered DOI 1'] = np.where(df['Ordered DOI 1'] == 0, 0.00001, df['Ordered DOI 1'])
        # create list of conditions to calculate current DOI
        doiweeks2 = [
            (df['Act_Beg_Inv_Units'] <= 0),
            (df['Act_Beg_Inv_Units'] <= df['DFcst_Wk1']),
            (df['Act_Beg_Inv_Units'] <= df['Cm_DFcst_1_2']),
            (df['Act_Beg_Inv_Units'] <= df['Cm_DFcst_1_3']),
            (df['Act_Beg_Inv_Units'] <= df['Cm_DFcst_1_4']),
            (df['Act_Beg_Inv_Units'] > df['Cm_DFcst_1_4'])
        ]
        # create list of current DOI calculation options
        weeks2 = [
            0,
            round(0 + (df['Act_Beg_Inv_Units'] / df['DFcst_Wk1']) * 7, 1),
            round(7 + ((df['Act_Beg_Inv_Units'] - df['DFcst_Wk1']) / df['DFcst_Wk2']) * 7, 1),
            round(14 + ((df['Act_Beg_Inv_Units'] - df['Cm_DFcst_1_2']) / df['DFcst_Wk3']) * 7, 1),
            round(21 + ((df['Act_Beg_Inv_Units'] - df['Cm_DFcst_1_3']) / df['DFcst_Wk4']) * 7, 1),
            round(28 + ((df['Act_Beg_Inv_Units'] - df['Cm_DFcst_1_4']) / df['Avg_DFcst']) * 7, 1),
        ]
        # create new series based on numpy select conditions
        df['Current DOI'] = np.select(doiweeks2, weeks2, default=0)
        # create list of conditions to calculate monthly pallet orders
        mpoweeks1 = [
            (df['Units_p_Pal'] <= 0),
            (df['Units_p_Pal'] <= df['DFcst_Wk1']),
            (df['Units_p_Pal'] <= df['Cm_DFcst_1_2']),
            (df['Units_p_Pal'] <= df['Cm_DFcst_1_3']),
            (df['Units_p_Pal'] <= df['Cm_DFcst_1_4']),
            (df['Units_p_Pal'] > df['Cm_DFcst_1_4'])
        ]
        # create list of current DOI calculation options
        mpoweeks2 = [
            0,
            round(0 + (df['Units_p_Pal'] / df['DFcst_Wk1']) * 7, 1),
            round(7 + ((df['Units_p_Pal'] - df['DFcst_Wk1']) / df['DFcst_Wk2']) * 7, 1),
            round(14 + ((df['Units_p_Pal'] - df['Cm_DFcst_1_2']) / df['DFcst_Wk3']) * 7, 1),
            round(21 + ((df['Units_p_Pal'] - df['Cm_DFcst_1_3']) / df['DFcst_Wk4']) * 7, 1),
            round(28 + ((df['Units_p_Pal'] - df['Cm_DFcst_1_4']) / df['Avg_DFcst']) * 7, 1),
        ]
        # create new series based on numpy select conditions
        df['Pal_DOI'] = np.select(mpoweeks1, mpoweeks2, default=0)
        # set all DOIs above 99 down to 99
        df['Ordered DOI 1'] = np.where(df['Ordered DOI 1'] > 99, 99, df['Ordered DOI 1'])
        df['Current DOI'] = np.where(df['Current DOI'] > 99, 99, df['Current DOI'])
        df['Pal_DOI'] = np.where(df['Pal_DOI'] > 99, 99, df['Pal_DOI'])
        # set all DOIs below 0 to 0
        df['Ordered DOI 1'] = np.where(df['Ordered DOI 1'] < 0, 0, df['Ordered DOI 1'])
        df['Current DOI'] = np.where(df['Current DOI'] < 0, 0, df['Current DOI'])
        df['Pal_DOI'] = np.where(df['Pal_DOI'] < 0, 0, df['Pal_DOI'])
        # flag as monthly pallet order if pal_DOI >= 28 and OSKU is not Contract and not a Keg and have an order
        df['MPO_Flag'] = np.where((df['Pal_DOI'] >= 28) &
                                  (df['Contract'] == 'N') &
                                  (df['Current DOI'] >= 42) &
                                  (df['Segment'].str.contains('KEG', case=False, na=0) == False) &
                                  ((df['Original Order (Units) 1'] > 0) |
                                   (df['Original Order (Units) 2'] > 0)), 1, 0)
        # create list to create series with
        df_series_list = ['Line', 'Cluster', 'Scenario 1', 'Target DOI 1', 'Target Conf (Units) 1', 'Pallet Adj 1',
                          'Confirm Vol (Units) 1', 'Ending DOI 1', 'Confirmed % 1', 'Change (Units) 1',
                          'Confirmed (Units) 1', 'Ordered DOI 2', 'Scenario 2', 'Target DOI 2',
                          'Target Conf (Units) 2', 'Pallet Adj 2', 'Confirm Vol (Units) 2', 'Ending DOI 2',
                          'Confirmed % 2', 'Change (Units) 2', 'Confirmed (Units) 2', 'SO Confirmed (Units) 1',
                          'SO Confirmed (Units) 2', 'Total Cut (Units)', 'Total On Tops (Units)', 'Cluster']
        # create empty series to be filled with formulas
        for dfseries in df_series_list:
            df[dfseries] = ''
        # replace 0s in Contract column with N
        df['Contract'] = np.where(df['Contract'] == 0, 'N', df['Contract'])
        # replace 0s in Mode column with Truck
        df['Mode'] = np.where(df['Mode'] == 0, 'TRUCK', df['Mode'])
        # replace 0s in run frequency with most common value
        r_freq_com = df['Run_Freq'].mode()
        df['Run_Freq'] = np.where(df['Run_Freq'] == 0, r_freq_com, df['Run_Freq'])
        # replace 0s in run frequency with STO if Line == 0 after first correction
        df['Run_Freq'] = np.where(df['Run_Freq'] == 0, 'STO', df['Run_Freq'])
        # create list of each region in the dataframe
        agg_key_list = df['Agg_Key'].unique().tolist()
        # create dataframe for the DOI table
        pf = pd.DataFrame()
        # create series in DOI table
        pf['Agg_Key'] = agg_key_list
        # create dataframe for merging
        mf = df[['Agg_Key', 'OSKU', 'Contract', 'Brand', 'Description', 'Segment', 'Run_Freq']]
        mp = md[['Agg_Key', 'Proc_Type']]
        # reduce dataframes into combined frame
        pf = reduce(lambda left, right: pd.merge(left, right, on=['Agg_Key'], how='left'), [pf, mf, mp])
        # drop duplicates in key column
        pf = pf.drop_duplicates(subset=['Agg_Key'])
        # create list of series to create
        pf_series_list = ['Line', 'Reject All 1', 'Confirm All 1', 'Confirm+ 1', 'Spill', 'Target DOI 1',
                          'OSKU DOI 1', 'Orders (BBLs) 1', 'Confirmed (BBLs) 1', 'Change (BBLs) 1', 'Remainder 1',
                          'Reject All 2', 'Confirm All 2', 'Confirm+ 2', 'Target DOI 2',
                          'OSKU DOI 2', 'Orders (BBLs) 2', 'Confirmed (BBLs) 2', 'Change (BBLs) 2', 'Remainder 2']
        # create blank series to be filled by formulas
        for pfseries in pf_series_list:
            pf[pfseries] = ''
        # rename columns
        pf = pf.rename(columns={'Run_Freq': 'Run Freq'})
        # create new series in the production dataframe
        rf['OSKU'] = rf['OSKU'].astype(int)
        rf['Agg_Key'] = rf['Plant'].astype(str) + '-' + rf['OSKU'].astype(str)
        rf['Plant_Key'] = rf['Plant'].astype(str) + '-' + rf['Line'].astype(str)
        # merge production dataframe with copied masterdata dataframe
        rf = pd.merge(rf, md_copy, on=['Agg_Key'], how='left')  # md_copy[['Agg_Key', 'MSKU']]
        # fill missing values with 0
        rf = rf.fillna(0)
        # format series
        rf['MSKU'] = rf['MSKU'].astype(int)
        # create new series in the production dataframe
        rf['MSKU_Key'] = rf['Plant'].astype(str) + '-' + rf['MSKU'].astype(str)
        # fill missing values
        rf.fillna(0, inplace=True)
        # create new dataframe for production lines
        cf1 = pd.DataFrame()
        # create new series in the line dataframe
        cf1['Line'] = rf['Plant'].astype(str) + '-' + rf['Line'].astype(str)
        # drop duplicate lines in line table
        cf1 = cf1.drop_duplicates(subset=['Line'])
        # create list of each region in the dataframe
        lineslist = cf1['Line'].tolist()
        # add values to end of list
        lineslist.append('STO')
        # create dataframe from list of lines
        cf = pd.DataFrame(lineslist, columns=['Line'])
        # create series in capacity dataframe
        cf['Line DOI 1'] = 99
        cf['Line DOI 2'] = 99
        # create series for each line
        cf['SP Cap (BBLs) 1'] = 99999
        cf['SP Cap (BBLs) 2'] = 99999
        # reindex dataframe
        cf = cf[['Line', 'Line DOI 1', 'Line DOI 2', 'SP Cap (BBLs) 1', 'SP Cap (BBLs) 2']]
        # create list to create series with
        cf_series_list = ['Capacity (BBLs) 1', 'Orders (BBLs) 1', 'Confirmed (BBLs) 1',
                          'Confirm % 1', 'Change (BBLs) 1', 'Remainder 1',
                          'Capacity (BBLs) 2', 'Orders (BBLs) 2', 'Confirmed (BBLs) 2',
                          'Confirm % 2', 'Change (BBLs) 2', 'Remainder 2']
        # create series from list
        for cfseries in cf_series_list:
            cf[cfseries] = ''
        # create new series
        df['Pair'] = df['ShipTo'].astype(str).str[:6] + '-' + df['OSKU'].astype(str).str[:6]
        # remove all periods from Pair series
        df['Pair'] = df['Pair'].str.replace('.', '')
        # create dataframe for the ShipTo tab
        sf = pd.DataFrame()
        sf_series_list = ['ShipTo', 'ShipTo_Name', 'DOI +-', 'DOI Cap', 'No Confirm+', 'Cluster', 'Status']
        for sf_series in sf_series_list:
            sf[sf_series] = ''
        # create dataframe for the OSKU tab
        st = pd.DataFrame()
        st_series_list = ['Plant', 'OSKU', 'Agg_Key', 'MSKU', 'Avail Inv (Units) 1', 'Avail Inv (BBLs) 1',
                          'Orders (BBLs) 1', 'Confirmed (BBLs) 1', 'Confirm % 1', 'Avail Vol (BBLs) 1',
                          'Avail Inv (Units) 2', 'Avail Inv (BBLs) 2', 'Orders (BBLs) 2', 'Confirmed (BBLs) 2',
                          'Confirm % 2', 'Avail Vol (BBLs) 2']
        for st_series in st_series_list:
            st[st_series] = ''
        # create dataframe for the Indirect tab
        ir = pd.DataFrame()
        ir_series_list = ['MSKU', 'Line', 'Brand', 'Segment', 'Order (BBLs) 1', 'Cut (BBLs) 1', 'Volume (BBLs) 1',
                          'Order (BBLs) 2', 'Cut (BBLs) 2', 'Volume (BBLs) 2']
        for ir_series in ir_series_list:
            ir[ir_series] = ''
        # create lists for run frequency master data
        rf_code_list = ['WKLY-X1', 'EVEN-X6', 'ODD-X7', '1ODD-X8', '1EVEN-X9', '2ODD-XA', '2EVEN-XB', '1EIGHT-XC',
                        '2EIGHT-XD', '3EIGHT-XE', '4EIGHT-XF', '5EIGHT-XG', '6EIGHT-XH', '7EIGHT-XJ', '8EIGHT-XK',
                        'SMMR_WKLY-Y8', 'SMMR_EVEN-YA', 'SMMR_1EVEN-YC', 'SMMR_2EVEN-YC', 'SMMR_ODD-YA', 'SMMR_1ODD-YB',
                        'SMMR_2ODD-YD', 'FALL_ODD-YG', 'FALL_1ODD-YJ', 'FALL_2ODD-YL', 'FALL_EVEN-YH', 'FLWN_EVEN-Y3',
                        'FLWN_2ODD-Y6', 'FLWN_1EVEN-Y5', 'FLWN_WKLY-Y1', 'FLWN_ODD-Y2', 'WNTR_EVEN-YQ', 'WNTR_WKLY-YN',
                        'WNTR_1EVEN-YS', 'WNTR_1ODD-YR', 'WNTR_ODD-YP', 'NEVER-X0', 'N1EVEN-X3', 'N1ODD-X2', 'N2EVEN-X5',
                        'N2ODD-X4', 'BM_SPRG-XL', 'BU_SMMR-XM', 'BM_FALL-XN', 'BM_WNTR-XP', 'LNKGL_SPRG-XQ',
                        'LNKGL_SMMR-XR', 'LNKGL_FALL-XS', 'LNKGL_WNTR-XT', '1AND2-XU', '2AND3-XV', '3AND4-XW', '4AND1-XX']
        rf_desc_list = ['Weekly', 'Even', 'Odd', 'First Odd', 'First Even', 'Second Odd', 'Second Even',
                        'Every 8 weeks, beginning in week 1', 'Every 8 weeks, beginning in week 2',
                        'Every 8 weeks, beginning in week 3', 'Every 8 weeks, beginning in week 4',
                        'Every 8 weeks, beginning in week 5', 'Every 8 weeks, beginning in week 6',
                        'Every 8 weeks, beginning in week 7', 'Every 8 weeks, beginning in week 8', 'Summer Weekly',
                        'Summer Even', 'Summer First Even', 'Summer Second Even', 'Summer Odd', 'Summer First Odd',
                        'Summer Second Odd', 'Fall Odd', 'Fall First Odd', 'Fall Second Odd', 'Fall Even',
                        'Fall Winter Even', 'Fall Winter Second Odd', 'Fall Winter First Even', 'Fall Winter Weekly',
                        'Fall Winter Odd', 'Winter Even', 'Winter Weekly', 'Winter First Even', 'Winter First Odd',
                        'Winter Odd', 'Never', 'All but First Even', 'All but First Odd', 'All but second even',
                        'All but second odd', 'Blue Moon Spring Weekly', 'Blue Moon Summer Weekly', 'Blue Moon Fall Weekly',
                        'Blue Moon Winter Weekly', "Leinie's Spring Weekly", "Leinie's Summer Weekly",
                        "Leinie's Fall Weekly", "Leinie's Winter Weekly", 'First and Second', 'Second and Third',
                        'Third and Fourth', 'Fourth and First']
        rf_adjs_list = [0, 7, 7, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 0, 7, 99, 99, 7, 99, 99, 7, 99, 99, 7, 99,
                        99, 99, 0, 7, 7, 0, 99, 99, 7, 99, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7]
        # zip lists of run frequency data
        run_freq_tuples = list(zip(rf_code_list, rf_desc_list, rf_adjs_list))
        # create dataframe from zipped list of weeks and fca accuracy
        ru = pd.DataFrame(run_freq_tuples, columns=['RF_Code', 'Description', 'Adjustment'])
        # rename columns
        pf = pf.rename(columns={'Tot_Beg_Inv_BBLs': 'Tot_Beg_Inv (BBLs)',
                                'Tot_Orig_Ord_BBLs': 'Tot_Orig_Ord (BBLs)',
                                'Proc_Type': 'Procurement'})
        # reindex dataframe for OSKU table
        pf = pf[['Agg_Key', 'OSKU', 'Run Freq', 'Brand', 'Description', 'Segment', 'Contract', 'Line',
                 'Reject All 1', 'Confirm All 1', 'Confirm+ 1', 'Spill', 'Target DOI 1', 'OSKU DOI 1',
                 'Orders (BBLs) 1', 'Confirmed (BBLs) 1', 'Change (BBLs) 1', 'Remainder 1',
                 'Reject All 2', 'Confirm All 2', 'Confirm+ 2', 'Target DOI 2', 'OSKU DOI 2',
                 'Orders (BBLs) 2', 'Confirmed (BBLs) 2', 'Change (BBLs) 2', 'Remainder 2', ]]
        # reindex dataframe for capacity table
        cf = cf[['Line', 'Line DOI 1', 'Line DOI 2', 'SP Cap (BBLs) 1', 'SP Cap (BBLs) 2',
                 'Capacity (BBLs) 1', 'Orders (BBLs) 1', 'Confirmed (BBLs) 1',
                 'Confirm % 1', 'Change (BBLs) 1', 'Remainder 1',
                 'Capacity (BBLs) 2', 'Orders (BBLs) 2', 'Confirmed (BBLs) 2',
                 'Confirm % 2', 'Change (BBLs) 2', 'Remainder 2']]
        # reindex dataframe for production table
        rf = rf[['Agg_Key', 'MSKU_Key', 'Plant_Key', 'OSKU']]
        # reindex dataframe for masterdata table
        md = md[['Agg_Key', 'Plant', 'OSKU', 'MSKU', 'Units_p_Pal', 'BBL_Conv', 'Proc_Type']]
        # check if production datafarme is empty and replace
        if rf.empty:
            # create dataframe for the Production tab
            rf = pd.DataFrame(np.array([['-', '-', '-', '-']]), columns=['Agg_Key', 'MSKU_Key', 'Plant_Key', 'OSKU'])
        # format series
        df['Original Order (Units) 1'] = df['Original Order (Units) 1'].astype(int)
        df['Original Order (Units) 2'] = df['Original Order (Units) 2'].astype(int)
        # fill blank values with zeros
        df = df.fillna(value=0)
        # create series for total original orders
        df['Original_Order_Sum'] = df['Original Order (Units) 1'] + df['Original Order (Units) 2']
        # sort dataframes by highest order volumes and alphabetically respectively
        df = df.sort_values(by='Original_Order_Sum', ascending=False)  # sort by highest total order volumes
        pf = pf.sort_values(by='Agg_Key', ascending=True)  # sort alphabetically
        cf = cf.sort_values(by='Line', ascending=True)  # sort alphabetically
        # reindex dataframe for MSKU table
        df = df[['Pair', 'Plant', 'ShipTo', 'OSKU', 'MSKU', 'Brand', 'Description', 'Segment', 'ShipTo_Name',
                 'State', 'Mode', 'SO 1', 'SO 2', 'Arrivals (Units) 1', 'Arrivals (Units) 2', 'MPO_Flag',
                 'Act_Beg_Inv_Units', 'DFcst_Wk0', 'Egr_Flag', 'DFcst_Wk1', 'DFcst_Wk2', 'DFcst_Wk3', 'DFcst_Wk4',
                 'Agg_Key', 'BBL_Conv', 'Units_p_Pal', 'Contract', 'Line', 'Run_Freq', 'Cluster',
                 'Current DOI', 'Ordered DOI 1', 'Original Order (Units) 1',
                 'Scenario 1', 'Target DOI 1', 'Target Conf (Units) 1', 'Pallet Adj 1', 'Confirm Vol (Units) 1',
                 'Ending DOI 1', 'Confirmed % 1', 'Change (Units) 1', 'Confirmed (Units) 1',
                 'Ordered DOI 2', 'Original Order (Units) 2', 'Scenario 2', 'Target DOI 2', 'Target Conf (Units) 2',
                 'Pallet Adj 2', 'Confirm Vol (Units) 2', 'Ending DOI 2', 'Confirmed % 2', 'Change (Units) 2',
                 'Confirmed (Units) 2', 'SO Confirmed (Units) 1', 'SO Confirmed (Units) 2',
                 'Total Cut (Units)', 'Total On Tops (Units)']]
        # reset index for each dataframe
        df.reset_index(drop=True, inplace=True)
        pf.reset_index(drop=True, inplace=True)
        cf.reset_index(drop=True, inplace=True)
        rf.reset_index(drop=True, inplace=True)
        # identify mpo_flagged orderers
        mpo_df = df[(df['MPO_Flag'] == 1)]
        # reindex dataframe
        mpo_df = mpo_df[['ShipTo', 'OSKU', 'Original Order (Units) 1', 'Original Order (Units) 2', 'Units_p_Pal']]
        # calculate in which week the volume should be placed
        mpo_conds = [
            (mpo_df['Original Order (Units) 1'] > 0) & (mpo_df['Original Order (Units) 2'] > 0),
            (mpo_df['Original Order (Units) 1'] > 0) & (mpo_df['Original Order (Units) 2'] <= 0),
            (mpo_df['Original Order (Units) 1'] <= 0) & (mpo_df['Original Order (Units) 2'] > 0),
        ]
        # calculate the choices for which week the volume should be placed
        mpo_choices = [1, 1, 2]
        # assign the week the volume should be placed
        mpo_df['Vol_Week'] = np.select(mpo_conds, mpo_choices, default=1)
        # convert series to lists
        mpo_shipto_list1 = mpo_df['ShipTo'].tolist()
        mpo_osku_list1 = mpo_df['OSKU'].tolist()
        mpo_uppal_list = mpo_df['Units_p_Pal'].tolist()
        mpo_vol_week_list = mpo_df['Vol_Week'].tolist()
        # create lists of dynamic length
        conf1_list1 = [None] * len(mpo_shipto_list1)
        conf2_list1 = [None] * len(mpo_shipto_list1)
        notes_list1 = ['Monthly Pallet Order'] * len(mpo_shipto_list1)
        # create lists for entering volume in the correct week
        mpo_min1_vol1 = []
        mpo_min2_vol1 = []
        # iterate through each pair to place volume in correct week
        for pair in range(0, len(mpo_shipto_list1)):
            # if volume allotted to week 1
            if (mpo_vol_week_list[pair] == 1) & (mpo_uppal_list[pair] > 0):
                # add one pallet of volume to the list
                mpo_min1_vol1.append(mpo_uppal_list[pair])
                # add None to the list in the other week
                mpo_min2_vol1.append('')
            # if volume allotted to week 2
            elif (mpo_vol_week_list[pair] == 2) & (mpo_uppal_list[pair] > 0):
                # add None to the list in the other week
                mpo_min1_vol1.append('')
                # add one pallet of volume to the list
                mpo_min2_vol1.append(mpo_uppal_list[pair])
        # identify unordered pairs where no volume was ordered in either week
        uno_df = df[(df['Original Order (Units) 1'] == 0) & (df['Original Order (Units) 2'] == 0)]
        # reindex dataframe
        uno_df = uno_df[['ShipTo', 'OSKU']]
        # convert series to lists
        mpo_shipto_list2 = uno_df['ShipTo'].tolist()
        mpo_osku_list2 = uno_df['OSKU'].tolist()
        # create lists of dynamic length
        conf1_list2 = [0] * len(mpo_shipto_list2)
        conf2_list2 = [0] * len(mpo_shipto_list2)
        mpo_min1_vol2 = [None] * len(mpo_shipto_list2)
        mpo_min2_vol2 = [None] * len(mpo_shipto_list2)
        notes_list2 = ['Unordered Pair'] * len(mpo_shipto_list2)
        # combine each of the lists into one list from mpo and unordered pairs
        mpo_shipto_list = mpo_shipto_list1 + mpo_shipto_list2
        mpo_osku_list = mpo_osku_list1 + mpo_osku_list2
        conf1_list = conf1_list1 + conf1_list2
        conf2_list = conf2_list1 + conf2_list2
        mpo_min1_vol = mpo_min1_vol1 + mpo_min1_vol2
        mpo_min2_vol = mpo_min2_vol1 + mpo_min2_vol2
        notes_list = notes_list1 + notes_list2
        # create dataframe for the Pair tab
        pv = pd.DataFrame()
        # create series from lists
        pv['ShipTo'] = mpo_shipto_list
        pv['OSKU'] = mpo_osku_list
        pv['Confirmed 1'] = conf1_list
        pv['Min Conf (Units) 1'] = mpo_min1_vol
        pv['Confirmed 2'] = conf2_list
        pv['Min Conf (Units) 2'] = mpo_min2_vol
        pv['Notes'] = notes_list
        pv_series_list = ['Pair', 'ShipTo_Name',
                          'DOI +- 1', 'Spill =>', 'Original Order (Units) 1',
                          'Confirmed (Units) 1', 'Diverted (Units) 1',
                          'DOI +- 2', 'Spill <=', 'Original Order (Units) 2',
                          'Confirmed (Units) 2', 'Diverted (Units) 2', ]
        for pv_series in pv_series_list:
            pv[pv_series] = ''
        # reindex dataframe
        pv = pv[['ShipTo', 'OSKU', 'Pair', 'ShipTo_Name',
                 'Confirmed 1', 'Diverted (Units) 1', 'DOI +- 1', 'Min Conf (Units) 1', 'Spill =>',
                 'Original Order (Units) 1', 'Confirmed (Units) 1', 'Spill <=',
                 'Confirmed 2', 'Diverted (Units) 2', 'DOI +- 2', 'Min Conf (Units) 2',
                 'Original Order (Units) 2', 'Confirmed (Units) 2', 'Notes']]
        # get list of plants from df dataframe
        plant_list = df['Plant'].astype(int).unique()
        # sort plant list smallest to largest
        plant_list.sort()
        # join items in list by delimiter
        joined_plant_list = '-'.join(map(str, plant_list))
        # create file name as a variable
        planner = savepath + foldername + ' Planner - (' + joined_plant_list + ').xlsx'
        # rewrite filepath with correct operating system separators
        planner = os_split_fixer(planner)
        # create a Pandas Excel writer using XlsxWriter as the engine
        writer = pd.ExcelWriter(planner, engine='xlsxwriter', options={'use_future_functions': True,
                                                                       'nan_inf_to_errors': True,
                                                                       'default_date_format': 'mm/dd/yyyy'})
        # create workbook object
        planbook = writer.book
        # Set the ratio between the worksheet tabs and the horizontal slider
        planbook.set_tab_ratio(75)
        # collect dataframe rows and columns
        dmax_row, dmax_col = df.shape  # orders
        pmax_row, pmax_col = pf.shape  # doi
        cmax_row, cmax_col = cf.shape  # lines
        # calculate starting row for the DOI table
        doi_row_start = 2 + cmax_row + 2
        # convert the dataframe to an XlsxWriter Excel object
        df.to_excel(writer, sheet_name='Orders', index=False)
        pf.to_excel(writer, sheet_name='DOI', index=False, startcol=0, startrow=doi_row_start)  # DOI
        cf.to_excel(writer, sheet_name='DOI', index=False, startcol=0, startrow=2)  # line
        sf.to_excel(writer, sheet_name='ShipTo', index=False)
        pv.to_excel(writer, sheet_name='Pair', index=False)
        st.to_excel(writer, sheet_name='OSKU', index=False)
        ir.to_excel(writer, sheet_name='Indirect', index=False)
        rf.to_excel(writer, sheet_name='Production', index=False)
        md.to_excel(writer, sheet_name='MasterData', index=False, startcol=0, startrow=0)
        ru.to_excel(writer, sheet_name='MasterData', index=False, startcol=8, startrow=0)
        # create worksheet objects
        ordr = writer.sheets['Orders']
        ldoi = writer.sheets['DOI']
        volp = writer.sheets['Pair']
        dois = writer.sheets['ShipTo']
        osku = writer.sheets['OSKU']
        indr = writer.sheets['Indirect']
        prod = writer.sheets['Production']
        mada = writer.sheets['MasterData']
        # get create date for workbook properties in list
        times = get_timestamps()
        # set workbook properties
        planbook.set_properties({
            'title': 'Confirmations Planner',
            'subject': 'Order Cycle Confirmations',
            'author': 'Joseph Arnson',
            'company': 'Molson Coors Beverage Company',
            'category': 'Customer Service Excellence',
            'keywords': 'Orders, Forecasts, Confirmations',
            'created': datetime.date(times[0], times[1], times[3]),
            'comments': 'Developed by Joseph Arnson\nIn conjunction with Customer Service\nSpecial thanks to Jacob Goike and Luke Wirsbinski\n\n(C) Copyright 2022, Molson Coors Beverage Company\nCreated with Python and XlsxWriter (C) Copyright 2013-2022, John McNamara.'})
        # format worksheets
        ordr.set_tab_color('#1496FF')
        ldoi.set_tab_color('#1496FF')
        volp.set_tab_color('#FFB000')
        dois.set_tab_color('#FFB000')
        osku.set_tab_color('#FFB000')
        indr.set_tab_color('#FFB000')
        prod.set_tab_color('#D8E0E4')
        mada.set_tab_color('#D8E0E4')
        ordr.set_zoom(100)  # set zoom % of worksheet
        ldoi.set_zoom(100)  # set zoom % of worksheet
        volp.set_zoom(100)  # set zoom % of worksheet
        dois.set_zoom(100)  # set zoom % of worksheet
        osku.set_zoom(100)  # set zoom % of worksheet
        indr.set_zoom(100)  # set zoom % of worksheet
        prod.set_zoom(100)  # set zoom % of worksheet
        mada.set_zoom(100)  # set zoom % of worksheet
        ordr.hide_gridlines(2)  # hide gridlines
        ldoi.hide_gridlines(2)  # hide gridlines
        volp.hide_gridlines(2)  # hide gridlines
        dois.hide_gridlines(2)  # hide gridlines
        osku.hide_gridlines(2)  # hide gridlines
        indr.hide_gridlines(2)  # hide gridlines
        prod.hide_gridlines(2)  # hide gridlines
        mada.hide_gridlines(2)  # hide gridlines
        ordr.freeze_panes(1, 6)  # freeze first five columns of DOI
        ldoi.freeze_panes(1 + doi_row_start, 3)  # freeze first three columns of DOI
        volp.freeze_panes(1, 0)  # freeze top row
        dois.freeze_panes(1, 0)  # freeze top row
        osku.freeze_panes(1, 0)  # freeze top row
        indr.freeze_panes(1, 0)  # freeze top row
        prod.freeze_panes(1, 0)  # freeze top row
        mada.freeze_panes(1, 0)  # freeze top row
        prod.hide()  # hide worksheet
        mada.hide()  # hide worksheet
        # turn off specific excel warnings
        ordr.ignore_errors({'calculated_column': 'A:BC'})
        ldoi.ignore_errors({'calculated_column': 'A:W'})
        dois.ignore_errors({'calculated_column': 'A:F'})
        volp.ignore_errors({'calculated_column': 'A:Q'})
        osku.ignore_errors({'calculated_column': 'A:P'})
        indr.ignore_errors({'calculated_column': 'A:J'})
        # create formatting methods for workbook - no background
        header_format = planbook.add_format({'align': 'left', 'bg_color': '#4F81BD', 'font_color': '#FFFFFF', 'bold': True})
        navy_header_format = planbook.add_format(
            {'align': 'left', 'bg_color': '#091F3F', 'font_color': '#FFFFFF', 'bold': True})
        orng_header_format = planbook.add_format(
            {'align': 'left', 'bg_color': '#FF6912', 'font_color': '#FFFFFF', 'bold': True})
        center_format = planbook.add_format({'align': 'center'})
        left_format = planbook.add_format({'align': 'left'})
        bblcv_format = planbook.add_format({'num_format': '#,##0.00000', 'align': 'right'})
        comma_format = planbook.add_format({'num_format': '#,##0.00', 'align': 'right'})
        doi_format = planbook.add_format({'num_format': '#,##0.0', 'align': 'right'})
        nodec_format = planbook.add_format({'num_format': '#,##0', 'align': 'right'})
        nocomma_format = planbook.add_format({'num_format': '0', 'align': 'left'})
        prcnt_format = planbook.add_format({'num_format': '0.00%', 'align': 'right'})
        # create formatting methods for workbook - blue background #9AC4F5
        b_center_format = planbook.add_format({'align': 'center', 'bg_color': '#9AC4F5'})
        b_left_format = planbook.add_format({'align': 'left', 'bg_color': '#9AC4F5'})
        b_comma_format = planbook.add_format({'num_format': '#,##0.00', 'align': 'right', 'bg_color': '#9AC4F5'})
        b_doi_format = planbook.add_format({'num_format': '#,##0.0', 'align': 'right', 'bg_color': '#9AC4F5'})
        b_nodec_format = planbook.add_format({'num_format': '#,##0', 'align': 'right', 'bg_color': '#9AC4F5'})
        b_prcnt_format = planbook.add_format({'num_format': '0.00%', 'align': 'right', 'bg_color': '#9AC4F5'})
        # create formatting methods for workbook - yellow background #FFE099
        y_center_format = planbook.add_format({'align': 'center', 'bg_color': '#FFE099'})
        y_comma_format = planbook.add_format({'num_format': '#,##0.00', 'align': 'right', 'bg_color': '#FFE099'})
        y_doi_format = planbook.add_format({'num_format': '#,##0.0', 'align': 'right', 'bg_color': '#FFE099'})
        y_nodec_format = planbook.add_format({'num_format': '#,##0', 'align': 'right', 'bg_color': '#FFE099'})
        y_nocomcen_format = planbook.add_format({'num_format': '0', 'align': 'center', 'bg_color': '#FFE099'})
        y_left_format = planbook.add_format({'align': 'left', 'bg_color': '#FFE099'})
        # create formatting for conditional formatting
        con_bad = planbook.add_format({'bg_color': '#FFC7CE', 'font_color': '#9C0006'})
        con_note = planbook.add_format({'bg_color': '#FCD5B4', 'font_color': '#974706'})
        con_midnote = planbook.add_format({'bg_color': '#FCD5B4', 'align': 'center', 'font_color': '#974706'})
        con_ntry = planbook.add_format({'bg_color': '#FFE099', 'font_color': '#000000', })
        # create table formatting
        format_excel(writer, md, 'MasterData', 'MADA', 'Table Style Medium 2', True)
        # group line table rows dynamically
        for ldoi_row in range(2, doi_row_start):
            ldoi.set_row(ldoi_row, None, None, {'level': 1})
        # format final group row for hiding
        ldoi.set_row(doi_row_start, None, None, {'collapsed': False})
        # get series as list for loop format entering - orders
        trns_mode_list = df['Mode'].tolist()
        dfcst_wk1_list = df['DFcst_Wk1'].tolist()
        dfcst_wk2_list = df['DFcst_Wk2'].tolist()
        dfcst_wk3_list = df['DFcst_Wk3'].tolist()
        dfcst_wk4_list = df['DFcst_Wk4'].tolist()
        contract_list = df['Contract'].tolist()
        run_freq_list = df['Run_Freq'].tolist()
        # get series as list for loop format entering - doi
        doi_os_list = pf['OSKU'].tolist()
        doi_rf_list = pf['Run Freq'].tolist()
        doi_cn_list = pf['Contract'].tolist()
        # format table entry columns - orders
        for i in range(1, dmax_row + 1):
            # format table based on used range
            ordr.write(i, 19, dfcst_wk1_list[i - 1], y_nodec_format)  # DFcst_Wk1
            ordr.write(i, 20, dfcst_wk2_list[i - 1], y_nodec_format)  # DFcst_Wk2
            ordr.write(i, 21, dfcst_wk3_list[i - 1], y_nodec_format)  # DFcst_Wk3
            ordr.write(i, 22, dfcst_wk4_list[i - 1], y_nodec_format)  # DFcst_Wk4
            ordr.write(i, 36, None, y_nodec_format)  # pallet adj 1
            ordr.write(i, 37, None, y_nodec_format)  # confirm vol units 1
            # ordr.write(i, 43, oorders2_list[i - 1], nodec_format)  # original orders (units) 2
            ordr.write(i, 47, None, y_nodec_format)  # pallet adj 2
            ordr.write(i, 48, None, y_nodec_format)  # confirm vol units 2
            ordr.write(i, 53, None, y_nodec_format)  # SO Confirmed (Units) 1
            ordr.write(i, 54, None, y_nodec_format)  # SO Confirmed (Units) 2
            # if Mode != 'TRUCK'
            if trns_mode_list[i - 1] != 'TRUCK':
                # if Mode != 'TRUCK' then format as note
                ordr.write(i, 10, trns_mode_list[i - 1], con_note)  # Mode
            # check if Contract == Y
            if contract_list[i - 1] == 'Y':
                # if Y then format as note
                ordr.write(i, 26, contract_list[i - 1], con_midnote)  # Contract
            # if Run_Freq != 'WKLY-X1'
            if run_freq_list[i - 1] != 'WKLY-X1':
                # if WKLY-X1 then format as note
                ordr.write(i, 28, run_freq_list[i - 1], con_note)  # Run_Freq
        # format table entry columns - line
        for i in range(2, cmax_row + 3):
            # format table based on used range
            ldoi.write(i, 1, 99, y_doi_format)  # line doi 1
            ldoi.write(i, 2, 99, y_doi_format)  # line doi 2
            ldoi.write(i, 3, 99999, y_comma_format)  # sp cap BBLs 1
            ldoi.write(i, 4, 99999, y_comma_format)  # sp cap BBLs 2
        # format table entry columns - doi
        for i in range(doi_row_start + 1, pmax_row + doi_row_start + 1):
            # format table based on used range
            ldoi.write(i, 1, doi_os_list[i - doi_row_start - 1], center_format)  # OSKU
            ldoi.write(i, 8, None, y_center_format)  # reject all 1
            ldoi.write(i, 9, None, y_center_format)  # confirm all 1
            ldoi.write(i, 10, None, y_center_format)  # confirm+ 1
            ldoi.write(i, 11, None, y_center_format)  # spill
            ldoi.write(i, 13, None, y_doi_format)  # OSKU DOI 1
            ldoi.write(i, 18, None, y_center_format)  # reject all 2
            ldoi.write(i, 19, None, y_center_format)  # confirm all 2
            ldoi.write(i, 20, None, y_center_format)  # confirm+ 2
            ldoi.write(i, 22, None, y_doi_format)  # OSKU DOI 2
            # if Run Freq != 'TRUCK'
            if doi_rf_list[i - doi_row_start - 1] != 'WKLY-X1':
                # if Mode != 'TRUCK' then format as note
                ldoi.write(i, 2, doi_rf_list[i - doi_row_start - 1], con_note)  # Run Freq
            # check if Contract == Y
            if doi_cn_list[i - doi_row_start - 1] == 'Y':
                # if Y then format as note
                ldoi.write(i, 6, doi_cn_list[i - doi_row_start - 1], con_midnote)  # Contract
            else:
                # if N then format as center
                ldoi.write(i, 6, doi_cn_list[i - doi_row_start - 1], center_format)  # Contract
        # loop through used rows and enter formats - pair tab
        for i in range(1, len(mpo_shipto_list) + 250 + 2):
            # enter data from lists
            try:
                # write format - Pair
                volp.write(i, 0, mpo_shipto_list[i - 1], y_nocomcen_format)
                volp.write(i, 1, mpo_osku_list[i - 1], y_nocomcen_format)
                volp.write(i, 4, conf1_list[i - 1], y_nodec_format)
                volp.write(i, 5, None, y_nodec_format)
                volp.write(i, 6, None, y_doi_format)
                volp.write(i, 7, mpo_min1_vol[i - 1], y_nodec_format)
                volp.write(i, 8, None, y_center_format)
                volp.write(i, 11, None, y_nodec_format)
                volp.write(i, 12, conf2_list[i - 1], y_nodec_format)
                volp.write(i, 13, None, y_nodec_format)
                volp.write(i, 14, None, y_doi_format)
                volp.write(i, 15, mpo_min2_vol[i - 1], y_nodec_format)
                volp.write(i, 18, notes_list[i - 1], y_left_format)
            # enter 250 blank rows after lists are finished
            except IndexError:
                # write format - Pair
                volp.write(i, 0, None, y_nocomcen_format)
                volp.write(i, 1, None, y_nocomcen_format)
                volp.write(i, 4, None, y_nodec_format)
                volp.write(i, 5, None, y_nodec_format)
                volp.write(i, 6, None, y_doi_format)
                volp.write(i, 7, None, y_nodec_format)
                volp.write(i, 8, None, y_center_format)
                volp.write(i, 11, None, y_nodec_format)
                volp.write(i, 12, None, y_nodec_format)
                volp.write(i, 13, None, y_nodec_format)
                volp.write(i, 14, None, y_doi_format)
                volp.write(i, 15, None, y_nodec_format)
                volp.write(i, 18, None, y_left_format)
        # loop through used rows and enter formats - all input tabs
        for i in range(1, 101):
            # write format - ShipTo
            dois.write(i, 0, None, y_nocomcen_format)
            dois.write(i, 2, None, y_doi_format)
            dois.write(i, 3, None, y_doi_format)
            dois.write(i, 4, None, y_center_format)
            dois.write(i, 5, None, y_left_format)
            # write format - OSKU
            osku.write(i, 0, None, y_nocomcen_format)
            osku.write(i, 1, None, y_nocomcen_format)
            osku.write(i, 4, None, y_nodec_format)
            osku.write(i, 5, None, y_comma_format)
            osku.write(i, 10, None, y_nodec_format)
            osku.write(i, 11, None, y_comma_format)
            # write format - INDR
            indr.write(i, 0, None, y_center_format)
            indr.write(i, 4, None, y_comma_format)
            indr.write(i, 5, None, y_comma_format)
            indr.write(i, 7, None, y_comma_format)
            indr.write(i, 8, None, y_comma_format)
        """create worksheet tables with formulas and entry columns"""
        # create table - ORDERS
        ordr.add_table(0, 0, df.shape[0], df.shape[1] - 1, {'name': 'ORDERS',
                                                            'style': 'Table Style Medium 2',
                                                            'autofilter': True,
                                                            'columns': [
                                                                {'header': 'Pair', 'header_format': left_format},
                                                                {'header': 'Plant', 'header_format': left_format},
                                                                {'header': 'ShipTo', 'header_format': left_format},
                                                                {'header': 'OSKU', 'header_format': left_format},
                                                                {'header': 'MSKU', 'header_format': left_format},
                                                                {'header': 'Brand', 'header_format': left_format},
                                                                {'header': 'Description', 'header_format': left_format},
                                                                {'header': 'Segment', 'header_format': left_format},
                                                                {'header': 'ShipTo_Name', 'header_format': left_format},
                                                                {'header': 'State', 'header_format': left_format},
                                                                {'header': 'Mode', 'header_format': left_format},
                                                                {'header': 'SO 1', 'header_format': navy_header_format},
                                                                {'header': 'SO 2', 'header_format': orng_header_format},
                                                                {'header': 'Arrivals (Units) 1',
                                                                 'header_format': navy_header_format},
                                                                {'header': 'Arrivals (Units) 2',
                                                                 'header_format': orng_header_format},
                                                                {'header': 'MPO_Flag', 'header_format': left_format},
                                                                {'header': 'Act_Beg_Inv_Units',
                                                                 'header_format': left_format},
                                                                {'header': 'DFcst_Wk0', 'header_format': left_format},
                                                                {'header': 'Egr_Flag', 'header_format': left_format},
                                                                {'header': 'DFcst_Wk1', 'header_format': left_format},
                                                                {'header': 'DFcst_Wk2', 'header_format': left_format},
                                                                {'header': 'DFcst_Wk3', 'header_format': left_format},
                                                                {'header': 'DFcst_Wk4', 'header_format': left_format},
                                                                {'header': 'Agg_Key', 'header_format': left_format},
                                                                {'header': 'BBL_Conv', 'header_format': left_format},
                                                                {'header': 'Units_p_Pal', 'header_format': left_format},
                                                                {'header': 'Contract', 'header_format': left_format},
                                                                {'header': 'Line', 'header_format': left_format,
                                                                 'format': b_center_format,
                                                                 'formula': '=IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("Line",DOI[#Headers],0)),"STO")'},
                                                                {'header': 'Run_Freq', 'header_format': left_format},
                                                                {'header': 'Cluster', 'header_format': left_format,
                                                                 'format': b_left_format,
                                                                 'formula': '=IF(IFERROR(INDEX(SHIPTO[],MATCH([@ShipTo],SHIPTO[ShipTo],0),MATCH("Cluster",SHIPTO[#Headers],0)),"-")=0,"-",IFERROR(INDEX(SHIPTO[],MATCH([@ShipTo],SHIPTO[ShipTo],0),MATCH("Cluster",SHIPTO[#Headers],0)),"-"))'},
                                                                {'header': 'Current DOI',
                                                                 'header_format': left_format},
                                                                {'header': 'Ordered DOI 1',
                                                                 'header_format': navy_header_format},
                                                                {'header': 'Original Order (Units) 1',
                                                                 'header_format': navy_header_format,
                                                                 'format': nodec_format},
                                                                {'header': 'Scenario 1',
                                                                 'header_format': navy_header_format,
                                                                 'format': b_center_format,
                                                                 'formula': '=IFERROR(_xlfn.IFS(IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Confirmed (Units) 1",PAIR[#Headers],0)),"-")<>"-","Pair Vol",IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("Reject All 1",DOI[#Headers],0)),0)<>0,"Reject All",AND(IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("Target DOI 1",DOI[#Headers],0)),"-")=99,IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("Line",DOI[#Headers],0)),"-")="STO"),"STO99",AND(IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("Target DOI 1",DOI[#Headers],0)),"-")=99,IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("Contract",DOI[#Headers],0)),"-")="Y"),"Con99",AND(IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("Confirm All 1",DOI[#Headers],0)),0)<>0,IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("Confirm+ 1",DOI[#Headers],0)),0)<>0,[@Contract]="N",IFERROR(INDEX(SHIPTO[],MATCH([@ShipTo],SHIPTO[ShipTo],0),MATCH("Status",SHIPTO[#Headers],0)),"-")<>"No Confirm+"),"Confirm All+",IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("Confirm All 1",DOI[#Headers],0)),0)<>0,"Confirm All",SUM(ORDERS[@[DFcst_Wk1]:[DFcst_Wk4]])=0,"No Forecast",AND(IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("Confirm+ 1",DOI[#Headers],0)),0)<>0,[@Contract]="N",IFERROR(INDEX(SHIPTO[],MATCH([@ShipTo],SHIPTO[ShipTo],0),MATCH("Status",SHIPTO[#Headers],0)),"-")<>"No Confirm+"),"Confirm+",TRUE,"Default"),0)'},
                                                                {'header': 'Target DOI 1',
                                                                 'header_format': navy_header_format,
                                                                 'format': b_doi_format,
                                                                 'formula': '=_xlfn.IFS(OR([@[Scenario 1]]="Reject All",[@[Scenario 1]]="No Forecast"),0,[@[Scenario 1]]="Confirm All+",MAX([@[Ordered DOI 1]],MIN(IF(IFERROR(INDEX(SHIPTO[],MATCH([@ShipTo],SHIPTO[ShipTo],0),MATCH("DOI CAP",SHIPTO[#Headers],0)),0)=0,99,IFERROR(INDEX(SHIPTO[],MATCH([@ShipTo],SHIPTO[ShipTo],0),MATCH("DOI CAP",SHIPTO[#Headers],0)),0)),IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("TARGET DOI 1",DOI[#Headers],0)),0)+IF([@Mode]="RAIL",7,0)+MAX(IFERROR(INDEX(SHIPTO[],MATCH([@ShipTo],SHIPTO[ShipTo],0),MATCH("DOI +-",SHIPTO[#Headers],0)),0),IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("DOI +- 1",PAIR[#Headers],0)),0)))),[@[Scenario 1]]="Confirm All",[@[Ordered DOI 1]],[@[Scenario 1]]="Confirm+",MIN(IF(IFERROR(INDEX(SHIPTO[],MATCH([@ShipTo],SHIPTO[ShipTo],0),MATCH("DOI CAP",SHIPTO[#Headers],0)),99)=0,99,IFERROR(INDEX(SHIPTO[],MATCH([@ShipTo],SHIPTO[ShipTo],0),MATCH("DOI CAP",SHIPTO[#Headers],0)),99)),IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("TARGET DOI 1",DOI[#Headers],0)),0)+IF([@Mode]="RAIL",7,0)+MAX(IFERROR(INDEX(SHIPTO[],MATCH([@ShipTo],SHIPTO[ShipTo],0),MATCH("DOI +-",SHIPTO[#Headers],0)),0),IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("DOI +- 1",PAIR[#Headers],0)),0))),TRUE,MIN([@[Ordered DOI 1]],IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("TARGET DOI 1",DOI[#Headers],0)),0)+IF([@Mode]="RAIL",7,0)+MAX(IFERROR(INDEX(SHIPTO[],MATCH([@ShipTo],SHIPTO[ShipTo],0),MATCH("DOI +-",SHIPTO[#Headers],0)),0),IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("DOI +- 1",PAIR[#Headers],0)),0))))'},
                                                                {'header': 'Target Conf (Units) 1',
                                                                 'header_format': navy_header_format,
                                                                 'format': b_nodec_format,
                                                                 'formula': '=IFERROR(MAX(0,_xlfn.IFS([@[Scenario 1]]="Pair Vol",IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Confirmed (Units) 1",PAIR[#Headers],0)),0),OR([@[Scenario 1]]="Reject All",[@[Scenario 1]]="No Forecast",[@[Target DOI 1]]=0),0,OR([@[Scenario 1]]="STO99",[@[Scenario 1]]="Con99",[@[Scenario 1]]="Confirm All"),[@[Original Order (Units) 1]],[@[Scenario 1]]="Confirm All+",MAX([@[Original Order (Units) 1]],FLOOR(MAX(IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Min Conf (Units) 1",PAIR[#Headers],0)),0),IF([@[Ordered DOI 1]]=[@[Target DOI 1]],[@[Original Order (Units) 1]],[@[Original Order (Units) 1]]+IFERROR(_xlfn.IFS([@[Target DOI 1]]<=0,0,[@[Target DOI 1]]<=7,[@[DFcst_Wk1]]*([@[Target DOI 1]]/7),[@[Target DOI 1]]<=14,[@[DFcst_Wk2]]*(([@[Target DOI 1]]-7)/7)+[@[DFcst_Wk1]],[@[Target DOI 1]]<=21,[@[DFcst_Wk3]]*(([@[Target DOI 1]]-14)/7)+SUM(ORDERS[@[DFcst_Wk1]:[DFcst_Wk2]]),[@[Target DOI 1]]<=28,[@[DFcst_Wk4]]*(([@[Target DOI 1]]-21)/7)+SUM(ORDERS[@[DFcst_Wk1]:[DFcst_Wk3]]),[@[Target DOI 1]]>28,AVERAGE(ORDERS[@[DFcst_Wk1]:[DFcst_Wk4]])*(([@[Target DOI 1]]-28)/7)+SUM(ORDERS[@[DFcst_Wk1]:[DFcst_Wk4]])),0)-([@[Act_Beg_Inv_Units]]+[@[Original Order (Units) 1]]-[@[DFcst_Wk0]]))),IF([@[Units_p_Pal]]=0,IFERROR(INDEX(MADA[],MATCH([@OSKU],MADA[OSKU],0),MATCH("Units_p_Pal",MADA[#Headers],0)),1),[@[Units_p_Pal]]))),TRUE,FLOOR(MAX(IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Min Conf (Units) 1",PAIR[#Headers],0)),0),IF([@[Ordered DOI 1]]=[@[Target DOI 1]],[@[Original Order (Units) 1]],[@[Original Order (Units) 1]]+IFERROR(_xlfn.IFS([@[Target DOI 1]]<=0,0,[@[Target DOI 1]]<=7,[@[DFcst_Wk1]]*([@[Target DOI 1]]/7),[@[Target DOI 1]]<=14,[@[DFcst_Wk2]]*(([@[Target DOI 1]]-7)/7)+[@[DFcst_Wk1]],[@[Target DOI 1]]<=21,[@[DFcst_Wk3]]*(([@[Target DOI 1]]-14)/7)+SUM(ORDERS[@[DFcst_Wk1]:[DFcst_Wk2]]),[@[Target DOI 1]]<=28,[@[DFcst_Wk4]]*(([@[Target DOI 1]]-21)/7)+SUM(ORDERS[@[DFcst_Wk1]:[DFcst_Wk3]]),[@[Target DOI 1]]>28,AVERAGE(ORDERS[@[DFcst_Wk1]:[DFcst_Wk4]])*(([@[Target DOI 1]]-28)/7)+SUM(ORDERS[@[DFcst_Wk1]:[DFcst_Wk4]])),0)-([@[Act_Beg_Inv_Units]]+[@[Original Order (Units) 1]]-[@[DFcst_Wk0]]))),IF([@[Units_p_Pal]]=0,IFERROR(INDEX(MADA,MATCH([@OSKU],MADA[OSKU],0),MATCH("Units_p_Pal",MADA[#Headers],0)),1),[@[Units_p_Pal]])))+([@[Pallet Adj 1]]*IF([@[Units_p_Pal]]=0,IFERROR(INDEX(MADA,MATCH([@OSKU],MADA[OSKU],0),MATCH("Units_p_Pal",MADA[#Headers],0)),1),[@[Units_p_Pal]]))),0)'},
                                                                {'header': 'Pallet Adj 1',
                                                                 'header_format': navy_header_format,
                                                                 'format': y_nodec_format},
                                                                {'header': 'Confirm Vol (Units) 1',
                                                                 'header_format': navy_header_format,
                                                                 'format': y_nodec_format},
                                                                {'header': 'Ending DOI 1',
                                                                 'header_format': navy_header_format,
                                                                 'format': b_doi_format,
                                                                 'formula': '=IFERROR(MAX(0,MIN(99,ROUND(IFERROR(_xlfn.IFS(IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)=0,0,IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)<=[@[DFcst_Wk1]],7*(IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)/[@[DFcst_Wk1]]),IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)<=SUM(ORDERS[@[DFcst_Wk1]:[DFcst_Wk2]]),7+7*((IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)-[@[DFcst_Wk1]])/[@[DFcst_Wk2]]),IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)<=SUM(ORDERS[@[DFcst_Wk1]:[DFcst_Wk3]]),14+7*((IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)-SUM(ORDERS[@[DFcst_Wk1]:[DFcst_Wk2]]))/[@[DFcst_Wk3]]),IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)<=SUM(ORDERS[@[DFcst_Wk1]:[DFcst_Wk4]]),21+7*((IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)-SUM(ORDERS[@[DFcst_Wk1]:[DFcst_Wk3]]))/[@[DFcst_Wk4]]),IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)>SUM(ORDERS[@[DFcst_Wk1]:[DFcst_Wk4]]),28+7*((IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)-SUM(ORDERS[@[DFcst_Wk1]:[DFcst_Wk4]]))/AVERAGE(ORDERS[@[DFcst_Wk1]:[DFcst_Wk4]]))),0),1))),0)'},
                                                                {'header': 'Confirmed % 1',
                                                                 'header_format': navy_header_format,
                                                                 'format': b_prcnt_format,
                                                                 'formula': '=IFERROR([@[Confirmed (Units) 1]]/[@[Original Order (Units) 1]],0)'},
                                                                {'header': 'Change (Units) 1',
                                                                 'header_format': navy_header_format,
                                                                 'format': b_nodec_format,
                                                                 'formula': '=IFERROR([@[Confirmed (Units) 1]]-[@[Original Order (Units) 1]],0)'},
                                                                {'header': 'Confirmed (Units) 1',
                                                                 'header_format': navy_header_format,
                                                                 'format': b_nodec_format,
                                                                 'formula': '=IFERROR(IF(ISBLANK([@[Confirm Vol (Units) 1]]),[@[Target Conf (Units) 1]],[@[Confirm Vol (Units) 1]]),0)'},
                                                                {'header': 'Ordered DOI 2',
                                                                 'header_format': orng_header_format,
                                                                 'format': b_doi_format,
                                                                 'formula': '=MAX(0.00001,IFERROR(MIN(99,ROUND(IFERROR(_xlfn.IFS(IFERROR(MAX(0,IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)+[@[Original Order (Units) 2]]+[@[Arrivals (Units) 2]]+IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Diverted (Units) 1",PAIR[#Headers],0)),0)-[@[DFcst_Wk1]]),0)<=0,0,IFERROR(MAX(0,IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)+[@[Original Order (Units) 2]]+[@[Arrivals (Units) 2]]+IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Diverted (Units) 1",PAIR[#Headers],0)),0)-[@[DFcst_Wk1]]),0)<=[@[DFcst_Wk2]],0+7*(IFERROR(MAX(0,IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)+[@[Original Order (Units) 2]]+[@[Arrivals (Units) 2]]+IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Diverted (Units) 1",PAIR[#Headers],0)),0)-[@[DFcst_Wk1]]),0)/[@[DFcst_Wk2]]),IFERROR(MAX(0,IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)+[@[Original Order (Units) 2]]+[@[Arrivals (Units) 2]]+IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Diverted (Units) 1",PAIR[#Headers],0)),0)-[@[DFcst_Wk1]]),0)<=SUM(ORDERS[@[DFcst_Wk2]:[DFcst_Wk3]]),7+7*((IFERROR(MAX(0,IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)+[@[Original Order (Units) 2]]+[@[Arrivals (Units) 2]]+IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Diverted (Units) 1",PAIR[#Headers],0)),0)-[@[DFcst_Wk1]]),0)-[@[DFcst_Wk2]])/[@[DFcst_Wk3]]),IFERROR(MAX(0,IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)+[@[Original Order (Units) 2]]+[@[Arrivals (Units) 2]]+IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Diverted (Units) 1",PAIR[#Headers],0)),0)-[@[DFcst_Wk1]]),0)<=SUM(ORDERS[@[DFcst_Wk2]:[DFcst_Wk4]]),14+7*((IFERROR(MAX(0,IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)+[@[Original Order (Units) 2]]+[@[Arrivals (Units) 2]]+IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Diverted (Units) 1",PAIR[#Headers],0)),0)-[@[DFcst_Wk1]]),0)-SUM(ORDERS[@[DFcst_Wk2]:[DFcst_Wk3]]))/[@[DFcst_Wk4]]),IFERROR(MAX(0,IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)+[@[Original Order (Units) 2]]+[@[Arrivals (Units) 2]]+IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Diverted (Units) 1",PAIR[#Headers],0)),0)-[@[DFcst_Wk1]]),0)>SUM(ORDERS[@[DFcst_Wk2]:[DFcst_Wk4]]),21+7*((IFERROR(MAX(0,IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)+[@[Original Order (Units) 2]]+[@[Arrivals (Units) 2]]+IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Diverted (Units) 1",PAIR[#Headers],0)),0)-[@[DFcst_Wk1]]),0)-SUM(ORDERS[@[DFcst_Wk2]:[DFcst_Wk4]]))/AVERAGE(ORDERS[@[DFcst_Wk2]:[DFcst_Wk4]]))),0),1)),0.00001))'},
                                                                {'header': 'Original Order (Units) 2',
                                                                 'header_format': orng_header_format,
                                                                 'format': nodec_format},
                                                                {'header': 'Scenario 2',
                                                                 'header_format': orng_header_format,
                                                                 'format': b_center_format,
                                                                 'formula': '=IFERROR(_xlfn.IFS(IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Confirmed (Units) 2",PAIR[#Headers],0)),"-")<>"-","Pair Vol",IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("Reject All 2",DOI[#Headers],0)),0)<>0,"Reject All",IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("Spill",DOI[#Headers],0)),0)<>0,"Spilled",AND(IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("Target DOI 2",DOI[#Headers],0)),"-")=99,IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("Line",DOI[#Headers],0)),"-")="STO"),"STO99",AND(IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("Target DOI 2",DOI[#Headers],0)),"-")=99,IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("Contract",DOI[#Headers],0)),"-")="Y"),"Con99",AND(IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("Confirm All 2",DOI[#Headers],0)),0)<>0,IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("Confirm+ 2",DOI[#Headers],0)),0)<>0,[@Contract]="N",IFERROR(INDEX(SHIPTO[],MATCH([@ShipTo],SHIPTO[ShipTo],0),MATCH("Status",SHIPTO[#Headers],0)),"-")<>"No Confirm+"),"Confirm All+",IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("Confirm All 2",DOI[#Headers],0)),0)<>0,"Confirm All",SUM(ORDERS[@[DFcst_Wk2]:[DFcst_Wk4]])=0,"No Forecast",AND(IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("Confirm+ 2",DOI[#Headers],0)),0)<>0,[@Contract]="N",IFERROR(INDEX(SHIPTO[],MATCH([@ShipTo],SHIPTO[ShipTo],0),MATCH("Status",SHIPTO[#Headers],0)),"-")<>"No Confirm+"),"Confirm+",TRUE,"Default"),0)'},
                                                                {'header': 'Target DOI 2',
                                                                 'header_format': orng_header_format,
                                                                 'format': b_doi_format,
                                                                 'formula': '=_xlfn.IFS(OR([@[Scenario 2]]="Reject All",[@[Scenario 2]]="No Forecast"),0,[@[Scenario 2]]="Confirm All+",MAX([@[Ordered DOI 2]],MIN(IF(IFERROR(INDEX(SHIPTO[],MATCH([@ShipTo],SHIPTO[ShipTo],0),MATCH("DOI CAP",SHIPTO[#Headers],0)),0)=0,99,IFERROR(INDEX(SHIPTO[],MATCH([@ShipTo],SHIPTO[ShipTo],0),MATCH("DOI CAP",SHIPTO[#Headers],0)),0)),IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("TARGET DOI 2",DOI[#Headers],0)),0)+IF([@Mode]="RAIL",7,0)+MAX(IFERROR(INDEX(SHIPTO[],MATCH([@ShipTo],SHIPTO[ShipTo],0),MATCH("DOI +-",SHIPTO[#Headers],0)),0),IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("DOI +- 2",PAIR[#Headers],0)),0)))),[@[Scenario 2]]="Confirm All",[@[Ordered DOI 2]],[@[Scenario 2]]="Confirm+",MIN(IF(IFERROR(INDEX(SHIPTO[],MATCH([@ShipTo],SHIPTO[ShipTo],0),MATCH("DOI CAP",SHIPTO[#Headers],0)),99)=0,99,IFERROR(INDEX(SHIPTO[],MATCH([@ShipTo],SHIPTO[ShipTo],0),MATCH("DOI CAP",SHIPTO[#Headers],0)),99)),IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("TARGET DOI 2",DOI[#Headers],0)),0)+IF([@Mode]="RAIL",7,0)+MAX(IFERROR(INDEX(SHIPTO[],MATCH([@ShipTo],SHIPTO[ShipTo],0),MATCH("DOI +-",SHIPTO[#Headers],0)),0),IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("DOI +- 2",PAIR[#Headers],0)),0))),TRUE,MIN([@[Ordered DOI 2]],IFERROR(INDEX(DOI[],MATCH([@[Agg_Key]],DOI[Agg_Key],0),MATCH("TARGET DOI 2",DOI[#Headers],0)),0)+IF([@Mode]="RAIL",7,0)+MAX(IFERROR(INDEX(SHIPTO[],MATCH([@ShipTo],SHIPTO[ShipTo],0),MATCH("DOI +-",SHIPTO[#Headers],0)),0),IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("DOI +- 2",PAIR[#Headers],0)),0))))'},
                                                                {'header': 'Target Conf (Units) 2',
                                                                 'header_format': orng_header_format,
                                                                 'format': b_nodec_format,
                                                                 'formula': '=IFERROR(MAX(0,_xlfn.IFS([@[Scenario 2]]="Pair Vol",IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Confirmed (Units) 2",PAIR[#Headers],0)),0),OR([@[Scenario 2]]="Reject All",[@[Scenario 2]]="No Forecast",[@[Target DOI 2]]=0),0,[@[Scenario 2]]="Spilled",MAX(0,SUM([@[Original Order (Units) 1]],[@[Original Order (Units) 2]])-[@[Confirmed (Units) 1]]),OR([@[Scenario 2]]="STO99",[@[Scenario 2]]="Con99",[@[Scenario 2]]="Confirm All"),[@[Original Order (Units) 2]],[@[Scenario 2]]="Confirm All+",MAX([@[Original Order (Units) 2]],FLOOR(MAX(IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Min Conf (Units) 2",PAIR[#Headers],0)),0),IF([@[Ordered DOI 2]]=[@[Target DOI 2]],[@[Original Order (Units) 2]],[@[Original Order (Units) 2]]+IFERROR(_xlfn.IFS([@[Target DOI 2]]<=0,0,[@[Target DOI 2]]<=7,[@[DFcst_Wk2]]*([@[Target DOI 2]]/7),[@[Target DOI 2]]<=14,[@[DFcst_Wk3]]*(([@[Target DOI 2]]-7)/7)+[@[DFcst_Wk2]],[@[Target DOI 2]]<=21,[@[DFcst_Wk4]]*(([@[Target DOI 2]]-14)/7)+SUM(ORDERS[@[DFcst_Wk2]:[DFcst_Wk3]]),[@[Target DOI 2]]>21,AVERAGE(ORDERS[@[DFcst_Wk2]:[DFcst_Wk4]])*(([@[Target DOI 2]]-21)/7)+SUM(ORDERS[@[DFcst_Wk2]:[DFcst_Wk4]])),0)-IFERROR(MAX(0,IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)+[@[Original Order (Units) 2]]+[@[Arrivals (Units) 2]]+IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Diverted (Units) 1",PAIR[#Headers],0)),0)-[@[DFcst_Wk1]]),0))),IF([@[Units_p_Pal]]=0,IFERROR(INDEX(MADA[],MATCH([@OSKU],MADA[OSKU],0),MATCH("Units_p_Pal",MADA[#Headers],0)),1),[@[Units_p_Pal]]))),TRUE,FLOOR(MAX(IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Min Conf (Units) 2",PAIR[#Headers],0)),0),IF([@[Ordered DOI 2]]=[@[Target DOI 2]],[@[Original Order (Units) 2]],[@[Original Order (Units) 2]]+IFERROR(_xlfn.IFS([@[Target DOI 2]]<=0,0,[@[Target DOI 2]]<=7,[@[DFcst_Wk2]]*([@[Target DOI 2]]/7),[@[Target DOI 2]]<=14,[@[DFcst_Wk3]]*(([@[Target DOI 2]]-7)/7)+[@[DFcst_Wk2]],[@[Target DOI 2]]<=21,[@[DFcst_Wk4]]*(([@[Target DOI 2]]-14)/7)+SUM(ORDERS[@[DFcst_Wk2]:[DFcst_Wk3]]),[@[Target DOI 2]]>21,AVERAGE(ORDERS[@[DFcst_Wk2]:[DFcst_Wk4]])*(([@[Target DOI 2]]-21)/7)+SUM(ORDERS[@[DFcst_Wk2]:[DFcst_Wk4]])),0)-IFERROR(MAX(0,IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)+[@[Original Order (Units) 2]]+[@[Arrivals (Units) 2]]+IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Diverted (Units) 1",PAIR[#Headers],0)),0)-[@[DFcst_Wk1]]),0))),IF([@[Units_p_Pal]]=0,IFERROR(INDEX(MADA[],MATCH([@OSKU],MADA[OSKU],0),MATCH("Units_p_Pal",MADA[#Headers],0)),1),[@[Units_p_Pal]])))+([@[Pallet Adj 2]]*IF([@[Units_p_Pal]]=0,IFERROR(INDEX(MADA[],MATCH([@OSKU],MADA[OSKU],0),MATCH("Units_p_Pal",MADA[#Headers],0)),1),[@[Units_p_Pal]]))),0)'},
                                                                {'header': 'Pallet Adj 2',
                                                                 'header_format': orng_header_format,
                                                                 'format': y_nodec_format},
                                                                {'header': 'Confirm Vol (Units) 2',
                                                                 'header_format': orng_header_format,
                                                                 'format': y_nodec_format},
                                                                {'header': 'Ending DOI 2',
                                                                 'header_format': orng_header_format,
                                                                 'format': b_doi_format,
                                                                 'formula': '=IFERROR(MAX(0,MIN(99,ROUND(IFERROR(_xlfn.IFS(IFERROR(MAX(0,IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)+[@[Confirmed (Units) 2]]+[@[Arrivals (Units) 2]]+IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Diverted (Units) 1",PAIR[#Headers],0)),0)-[@[DFcst_Wk1]]),0)<=0,0,IFERROR(MAX(0,IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)+[@[Confirmed (Units) 2]]+[@[Arrivals (Units) 2]]+IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Diverted (Units) 1",PAIR[#Headers],0)),0)-[@[DFcst_Wk1]]),0)<=[@[DFcst_Wk2]],0+7*(IFERROR(MAX(0,IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)+[@[Confirmed (Units) 2]]+[@[Arrivals (Units) 2]]+IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Diverted (Units) 1",PAIR[#Headers],0)),0)-[@[DFcst_Wk1]]),0)/[@[DFcst_Wk2]]),IFERROR(MAX(0,IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)+[@[Confirmed (Units) 2]]+[@[Arrivals (Units) 2]]+IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Diverted (Units) 1",PAIR[#Headers],0)),0)-[@[DFcst_Wk1]]),0)<=SUM(ORDERS[@[DFcst_Wk2]:[DFcst_Wk3]]),7+7*((IFERROR(MAX(0,IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)+[@[Confirmed (Units) 2]]+[@[Arrivals (Units) 2]]+IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Diverted (Units) 1",PAIR[#Headers],0)),0)-[@[DFcst_Wk1]]),0)-[@[DFcst_Wk2]])/[@[DFcst_Wk3]]),IFERROR(MAX(0,IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)+[@[Confirmed (Units) 2]]+[@[Arrivals (Units) 2]]+IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Diverted (Units) 1",PAIR[#Headers],0)),0)-[@[DFcst_Wk1]]),0)<=SUM(ORDERS[@[DFcst_Wk2]:[DFcst_Wk4]]),14+7*((IFERROR(MAX(0,IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)+[@[Confirmed (Units) 2]]+[@[Arrivals (Units) 2]]+IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Diverted (Units) 1",PAIR[#Headers],0)),0)-[@[DFcst_Wk1]]),0)-SUM(ORDERS[@[DFcst_Wk2]:[DFcst_Wk3]]))/[@[DFcst_Wk4]]),IFERROR(MAX(0,IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)+[@[Confirmed (Units) 2]]+[@[Arrivals (Units) 2]]+IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Diverted (Units) 1",PAIR[#Headers],0)),0)-[@[DFcst_Wk1]]),0)>SUM(ORDERS[@[DFcst_Wk2]:[DFcst_Wk4]]),21+7*((IFERROR(MAX(0,IFERROR(MAX(0,[@[Confirmed (Units) 1]]+[@[Act_Beg_Inv_Units]]-[@[DFcst_Wk0]]),0)+[@[Confirmed (Units) 2]]+[@[Arrivals (Units) 2]]+IFERROR(INDEX(PAIR[],MATCH([@Pair],PAIR[Pair],0),MATCH("Diverted (Units) 1",PAIR[#Headers],0)),0)-[@[DFcst_Wk1]]),0)-SUM(ORDERS[@[DFcst_Wk2]:[DFcst_Wk4]]))/AVERAGE(ORDERS[@[DFcst_Wk2]:[DFcst_Wk4]]))),0),1))),0)'},
                                                                {'header': 'Confirmed % 2',
                                                                 'header_format': orng_header_format,
                                                                 'format': b_prcnt_format,
                                                                 'formula': '=IFERROR([@[Confirmed (Units) 2]]/[@[Original Order (Units) 2]],0)'},
                                                                {'header': 'Change (Units) 2',
                                                                 'header_format': orng_header_format,
                                                                 'format': b_nodec_format,
                                                                 'formula': '=IFERROR([@[Confirmed (Units) 2]]-[@[Original Order (Units) 2]],0)'},
                                                                {'header': 'Confirmed (Units) 2',
                                                                 'header_format': orng_header_format,
                                                                 'format': b_nodec_format,
                                                                 'formula': '=IFERROR(IF(ISBLANK([@[Confirm Vol (Units) 2]]),[@[Target Conf (Units) 2]],[@[Confirm Vol (Units) 2]]),0)'},
                                                                {'header': 'SO Confirmed (Units) 1',
                                                                 'header_format': navy_header_format,
                                                                 'format': y_nodec_format},
                                                                {'header': 'SO Confirmed (Units) 2',
                                                                 'header_format': orng_header_format,
                                                                 'format': y_nodec_format},
                                                                {'header': 'Total Cut (Units)',
                                                                 'header_format': left_format, 'format': b_nodec_format,
                                                                 'formula': '=MAX(0,SUM([@[Original Order (Units) 1]],[@[Original Order (Units) 2]])-SUM([@[Confirmed (Units) 1]],[@[Confirmed (Units) 2]]))'},
                                                                {'header': 'Total On Tops (Units)',
                                                                 'header_format': left_format, 'format': b_nodec_format,
                                                                 'formula': '=MAX(0,SUM([@[Confirmed (Units) 1]],[@[Confirmed (Units) 2]])-SUM([@[Original Order (Units) 1]],[@[Original Order (Units) 2]]))'},
                                                            ]})
        # create table - DOI
        ldoi.add_table(doi_row_start, 0, pf.shape[0] + doi_row_start, pf.shape[1] - 1, {'name': 'DOI',
                                                                                        'style': 'Table Style Medium 2',
                                                                                        'autofilter': True,
                                                                                        'columns': [
                                                                                            {'header': 'Agg_Key',
                                                                                             'header_format': left_format,
                                                                                             'format': center_format},
                                                                                            {'header': 'OSKU',
                                                                                             'header_format': left_format,
                                                                                             'format': center_format},
                                                                                            {'header': 'Run Freq',
                                                                                             'header_format': left_format,
                                                                                             'format': left_format},
                                                                                            {'header': 'Brand',
                                                                                             'header_format': left_format,
                                                                                             'format': left_format},
                                                                                            {'header': 'Description',
                                                                                             'header_format': left_format,
                                                                                             'format': left_format},
                                                                                            {'header': 'Segment',
                                                                                             'header_format': left_format,
                                                                                             'format': left_format},
                                                                                            {'header': 'Contract',
                                                                                             'header_format': left_format,
                                                                                             'format': center_format},
                                                                                            {'header': 'Line',
                                                                                             'header_format': left_format,
                                                                                             'format': b_left_format,
                                                                                             'formula': '=IF(IFERROR(INDEX(MADA[],MATCH([@[Agg_Key]],MADA[Agg_Key],0),MATCH("Proc_Type",MADA[#Headers],0)),"-")="F",IFERROR(INDEX(PROD[],MATCH([@OSKU],PROD[OSKU],0),MATCH("Plant_Key",PROD[#Headers],0)),"STO"),IFERROR(INDEX(PROD[],MATCH([@[Agg_Key]],PROD[Agg_Key],0),MATCH("Plant_Key",PROD[#Headers],0)),"STO"))'},
                                                                                            {'header': 'Reject All 1',
                                                                                             'header_format': navy_header_format,
                                                                                             'format': y_center_format},
                                                                                            {'header': 'Confirm All 1',
                                                                                             'header_format': navy_header_format,
                                                                                             'format': y_center_format},
                                                                                            {'header': 'Confirm+ 1',
                                                                                             'header_format': navy_header_format,
                                                                                             'format': y_center_format},
                                                                                            {'header': 'Spill',
                                                                                             'header_format': navy_header_format,
                                                                                             'format': y_center_format},
                                                                                            {'header': 'Target DOI 1',
                                                                                             'header_format': navy_header_format,
                                                                                             'format': b_doi_format,
                                                                                             'formula': '=MAX(0,MIN(99,_xlfn.IFS(ISBLANK([@[OSKU DOI 1]])=FALSE,[@[OSKU DOI 1]],ISBLANK([@[Confirm+ 1]])=FALSE,IFERROR(INDEX(LINE[],MATCH([@Line],LINE[Line],0),MATCH("Line DOI 1",LINE[#Headers],0)),0),TRUE,IFERROR(INDEX(LINE[],MATCH([@Line],LINE[Line],0),MATCH("Line DOI 1",LINE[#Headers],0)),0)+IFERROR(INDEX(RNFQ[],MATCH([@[Run Freq]],RNFQ[RF_Code],0),MATCH("Adjustment",RNFQ[#Headers],0)),99))))'},
                                                                                            {'header': 'OSKU DOI 1',
                                                                                             'header_format': navy_header_format,
                                                                                             'format': y_doi_format},
                                                                                            {'header': 'Orders (BBLs) 1',
                                                                                             'header_format': navy_header_format,
                                                                                             'format': b_comma_format,
                                                                                             'formula': '=SUMPRODUCT(--(ORDERS[Agg_Key]=[@[Agg_Key]]),ORDERS[Original Order (Units) 1],ORDERS[BBL_Conv])'},
                                                                                            {'header': 'Confirmed (BBLs) 1',
                                                                                             'header_format': navy_header_format,
                                                                                             'format': b_comma_format,
                                                                                             'formula': '=SUMPRODUCT(--(ORDERS[Agg_Key]=[@[Agg_Key]]),ORDERS[Confirmed (Units) 1],ORDERS[BBL_Conv])'},
                                                                                            {'header': 'Change (BBLs) 1',
                                                                                             'header_format': navy_header_format,
                                                                                             'format': b_comma_format,
                                                                                             'formula': '=SUMPRODUCT(--(ORDERS[Agg_Key]=[@[Agg_Key]]),ORDERS[Change (Units) 1],ORDERS[BBL_Conv])'},
                                                                                            {'header': 'Remainder 1',
                                                                                             'header_format': navy_header_format,
                                                                                             'format': b_comma_format,
                                                                                             'formula': '=IFERROR(MIN(IFERROR(INDEX(LINE[],MATCH([@Line],LINE[Line],0),MATCH("Capacity (BBLs) 1",LINE[#Headers],0)),0),SUMPRODUCT(--(ORDERS[Line]=[@Line]),ORDERS[Confirmed (Units) 1],ORDERS[BBL_Conv]),IFERROR(INDEX(OSKU[],MATCH([@[Agg_Key]],OSKU[Agg_Key],0),MATCH("Avail Vol (BBLs) 1",OSKU[#Headers],0)),999999999))-SUMPRODUCT(--(ORDERS[Agg_Key]=[@[Agg_Key]]),ORDERS[Confirmed (Units) 1],ORDERS[BBL_Conv]),0)'},
                                                                                            {'header': 'Reject All 2',
                                                                                             'header_format': orng_header_format,
                                                                                             'format': y_center_format},
                                                                                            {'header': 'Confirm All 2',
                                                                                             'header_format': orng_header_format,
                                                                                             'format': y_center_format},
                                                                                            {'header': 'Confirm+ 2',
                                                                                             'header_format': orng_header_format,
                                                                                             'format': y_center_format},
                                                                                            {'header': 'Target DOI 2',
                                                                                             'header_format': orng_header_format,
                                                                                             'format': b_doi_format,
                                                                                             'formula': '=MAX(0,MIN(99,_xlfn.IFS(ISBLANK([@[OSKU DOI 2]])=FALSE,[@[OSKU DOI 2]],ISBLANK([@[Confirm+ 2]])=FALSE,IFERROR(INDEX(LINE[],MATCH([@Line],LINE[Line],0),MATCH("Line DOI 2",LINE[#Headers],0)),0),TRUE,IFERROR(INDEX(LINE[],MATCH([@Line],LINE[Line],0),MATCH("Line DOI 2",LINE[#Headers],0)),0)+IFERROR(INDEX(RNFQ[],MATCH([@[Run Freq]],RNFQ[RF_Code],0),MATCH("Adjustment",RNFQ[#Headers],0)),99))))'},
                                                                                            {'header': 'OSKU DOI 2',
                                                                                             'header_format': orng_header_format,
                                                                                             'format': y_doi_format},
                                                                                            {'header': 'Orders (BBLs) 2',
                                                                                             'header_format': orng_header_format,
                                                                                             'format': b_comma_format,
                                                                                             'formula': '=SUMPRODUCT(--(ORDERS[Agg_Key]=[@[Agg_Key]]),ORDERS[Original Order (Units) 2],ORDERS[BBL_Conv])'},
                                                                                            {'header': 'Confirmed (BBLs) 2',
                                                                                             'header_format': orng_header_format,
                                                                                             'format': b_comma_format,
                                                                                             'formula': '=SUMPRODUCT(--(ORDERS[Agg_Key]=[@[Agg_Key]]),ORDERS[Confirmed (Units) 2],ORDERS[BBL_Conv])'},
                                                                                            {'header': 'Change (BBLs) 2',
                                                                                             'header_format': orng_header_format,
                                                                                             'format': b_comma_format,
                                                                                             'formula': '=SUMPRODUCT(--(ORDERS[Agg_Key]=[@[Agg_Key]]),ORDERS[Change (Units) 2],ORDERS[BBL_Conv])'},
                                                                                            {'header': 'Remainder 2',
                                                                                             'header_format': orng_header_format,
                                                                                             'format': b_comma_format,
                                                                                             'formula': '=IFERROR(MIN(IFERROR(INDEX(LINE[],MATCH([@Line],LINE[Line],0),MATCH("Capacity (BBLs) 2",LINE[#Headers],0)),0),SUMPRODUCT(--(ORDERS[Line]=[@Line]),ORDERS[Confirmed (Units) 2],ORDERS[BBL_Conv]),IFERROR(INDEX(OSKU[],MATCH([@[Agg_Key]],OSKU[Agg_Key],0),MATCH("Avail Vol (BBLs) 2",OSKU[#Headers],0)),999999999))-SUMPRODUCT(--(ORDERS[Agg_Key]=[@[Agg_Key]]),ORDERS[Confirmed (Units) 2],ORDERS[BBL_Conv]),0)'},
                                                                                        ]})
        # create table - SUB
        ldoi.add_table('B1:E2', {'name': 'SUB',
                                 'style': 'Table Style Medium 2',
                                 'autofilter': False,
                                 'columns': [
                                     {'header': 'Confirmed (BBLs) 1', 'header_format': navy_header_format,
                                      'format': b_comma_format,
                                      'formula': '=SUBTOTAL(9,DOI[Confirmed (BBLs) 1])'},
                                     {'header': 'Confirmed (BBLs) 2', 'header_format': orng_header_format,
                                      'format': b_comma_format,
                                      'formula': '=SUBTOTAL(9,DOI[Confirmed (BBLs) 2])'},
                                     {'header': 'Change (BBLs) 1', 'header_format': navy_header_format,
                                      'format': b_comma_format,
                                      'formula': '=SUBTOTAL(9,DOI[Change (BBLs) 1])'},
                                     {'header': 'Change (BBLs) 2', 'header_format': orng_header_format,
                                      'format': b_comma_format,
                                      'formula': '=SUBTOTAL(9,DOI[Change (BBLs) 2])'},
                                 ]})
        # create table - LINE
        ldoi.add_table(2, 0, cf.shape[0] + 2, cf.shape[1] - 1, {'name': 'LINE',
                                                                'style': 'Table Style Medium 2',
                                                                'autofilter': True,
                                                                'columns': [
                                                                    {'header': 'Line',
                                                                     'header_format': left_format},
                                                                    {'header': 'Line DOI 1',
                                                                     'header_format': navy_header_format,
                                                                     'format': y_doi_format},
                                                                    {'header': 'Line DOI 2',
                                                                     'header_format': orng_header_format,
                                                                     'format': y_doi_format},
                                                                    {'header': 'SP Cap (BBLs) 1',
                                                                     'header_format': navy_header_format,
                                                                     'format': y_comma_format},
                                                                    {'header': 'SP Cap (BBLs) 2',
                                                                     'header_format': orng_header_format,
                                                                     'format': y_comma_format},
                                                                    {'header': 'Capacity (BBLs) 1',
                                                                     'header_format': navy_header_format,
                                                                     'format': b_comma_format,
                                                                     'formula': '=IF([@Line]="STO",MAX(SUMPRODUCT(--(ORDERS[Line]=[@Line]),ORDERS[Original Order (Units) 1],ORDERS[BBL_Conv]),SUMPRODUCT(--(ORDERS[Line]=[@Line]),ORDERS[Confirmed (Units) 1],ORDERS[BBL_Conv])),[@[SP Cap (BBLs) 1]]-SUMIFS(INDR[Volume (BBLs) 1],INDR[Line],[@Line]))'},
                                                                    {'header': 'Orders (BBLs) 1',
                                                                     'header_format': navy_header_format,
                                                                     'format': b_comma_format,
                                                                     'formula': '=SUMPRODUCT(--(ORDERS[Line]=[@Line]),ORDERS[Original Order (Units) 1],ORDERS[BBL_Conv])'},
                                                                    {'header': 'Confirmed (BBLs) 1',
                                                                     'header_format': navy_header_format,
                                                                     'format': b_comma_format,
                                                                     'formula': '=SUMPRODUCT(--(ORDERS[Line]=[@Line]),ORDERS[Confirmed (Units) 1],ORDERS[BBL_Conv])'},
                                                                    {'header': 'Confirm % 1',
                                                                     'header_format': navy_header_format,
                                                                     'format': b_prcnt_format,
                                                                     'formula': '=IFERROR([@[Confirmed (BBLs) 1]]/[@[Orders (BBLs) 1]],0)'},
                                                                    {'header': 'Change (BBLs) 1',
                                                                     'header_format': navy_header_format,
                                                                     'format': b_comma_format,
                                                                     'formula': '=IFERROR([@[Confirmed (BBLs) 1]]-[@[Orders (BBLs) 1]],0)'},
                                                                    {'header': 'Remainder 1',
                                                                     'header_format': navy_header_format,
                                                                     'format': b_comma_format,
                                                                     'formula': '=IFERROR([@[Capacity (BBLs) 1]]-[@[Confirmed (BBLs) 1]],0)'},
                                                                    {'header': 'Capacity (BBLs) 2',
                                                                     'header_format': orng_header_format,
                                                                     'format': b_comma_format,
                                                                     'formula': '=IF([@Line]="STO",MAX(SUMPRODUCT(--(ORDERS[Line]=[@Line]),ORDERS[Original Order (Units) 2],ORDERS[BBL_Conv]),SUMPRODUCT(--(ORDERS[Line]=[@Line]),ORDERS[Confirmed (Units) 2],ORDERS[BBL_Conv])),[@[SP Cap (BBLs) 2]]-SUMIFS(INDR[Volume (BBLs) 2],INDR[Line],[@Line]))'},
                                                                    {'header': 'Orders (BBLs) 2',
                                                                     'header_format': orng_header_format,
                                                                     'format': b_comma_format,
                                                                     'formula': '=SUMPRODUCT(--(ORDERS[Line]=[@Line]),ORDERS[Original Order (Units) 2],ORDERS[BBL_Conv])'},
                                                                    {'header': 'Confirmed (BBLs) 2',
                                                                     'header_format': orng_header_format,
                                                                     'format': b_comma_format,
                                                                     'formula': '=SUMPRODUCT(--(ORDERS[Line]=[@Line]),ORDERS[Confirmed (Units) 2],ORDERS[BBL_Conv])'},
                                                                    {'header': 'Confirm % 2',
                                                                     'header_format': orng_header_format,
                                                                     'format': b_prcnt_format,
                                                                     'formula': '=IFERROR([@[Confirmed (BBLs) 2]]/[@[Orders (BBLs) 2]],0)'},
                                                                    {'header': 'Change (BBLs) 2',
                                                                     'header_format': orng_header_format,
                                                                     'format': b_comma_format,
                                                                     'formula': '=IFERROR([@[Confirmed (BBLs) 2]]-[@[Orders (BBLs) 2]],0)'},
                                                                    {'header': 'Remainder 2',
                                                                     'header_format': orng_header_format,
                                                                     'format': b_comma_format,
                                                                     'formula': '=IFERROR([@[Capacity (BBLs) 2]]-[@[Confirmed (BBLs) 2]],0)'},
                                                                ]})
        # create table - SHIPTO
        dois.add_table(0, 0, 100, sf.shape[1] - 1, {'name': 'SHIPTO',
                                                    'style': 'Table Style Medium 2',
                                                    'autofilter': True,
                                                    'columns': [
                                                        {'header': 'ShipTo', 'header_format': left_format},
                                                        {'header': 'ShipTo_Name', 'header_format': left_format,
                                                         'format': b_left_format,
                                                         'formula': '=IF(COUNTIFS([ShipTo],[@ShipTo])>1,"[!] DUPLICATE SHIPTO LISTED",IFERROR(INDEX(ORDERS[],MATCH([@ShipTo],ORDERS[ShipTo],0),MATCH("ShipTo_Name",ORDERS[#Headers],0)),"-"))'},
                                                        {'header': 'DOI +-', 'header_format': left_format},
                                                        {'header': 'DOI Cap', 'header_format': left_format},
                                                        {'header': 'No Confirm+', 'header_format': left_format},
                                                        {'header': 'Cluster', 'header_format': left_format},
                                                        {'header': 'Status', 'header_format': left_format,
                                                         'format': b_left_format,
                                                         'formula': '=IF(OR(ISBLANK([@ShipTo]),ISBLANK([@[No Confirm+]])),"-","No Confirm+")'},
                                                    ]})
        # create table - VOLP
        volp.add_table(0, 0, 1 + len(mpo_shipto_list) + 250, pv.shape[1] - 1, {'name': 'PAIR',
                                                                               'style': 'Table Style Medium 2',
                                                                               'autofilter': True,
                                                                               'columns': [
                                                                                   {'header': 'ShipTo',
                                                                                    'header_format': left_format},
                                                                                   {'header': 'OSKU',
                                                                                    'header_format': left_format},
                                                                                   {'header': 'Pair',
                                                                                    'header_format': left_format,
                                                                                    'format': b_center_format,
                                                                                    'formula': '=IF(OR(ISBLANK([@ShipTo]),ISBLANK([@OSKU])),"-",_xlfn.CONCAT(VALUE([@ShipTo]),"-",VALUE([@OSKU])))'},
                                                                                   {'header': 'ShipTo_Name',
                                                                                    'header_format': left_format,
                                                                                    'format': b_left_format,
                                                                                    'formula': '=IF(AND([@Pair]<>"-",COUNTIFS([Pair],[@Pair])>1),"[!] DUPLICATE PAIR LISTED",IFERROR(INDEX(ORDERS[],MATCH([@Pair],ORDERS[Pair],0),MATCH("ShipTo_Name",ORDERS[#Headers],0)),"-"))'},
                                                                                   {'header': 'Confirmed 1',
                                                                                    'header_format': navy_header_format,
                                                                                    'format': y_nodec_format},
                                                                                   {'header': 'Diverted (Units) 1',
                                                                                    'header_format': navy_header_format,
                                                                                    'format': y_nodec_format},
                                                                                   {'header': 'DOI +- 1',
                                                                                    'header_format': navy_header_format,
                                                                                    'format': y_nodec_format},
                                                                                   {'header': 'Min Conf (Units) 1',
                                                                                    'header_format': navy_header_format,
                                                                                    'format': y_nodec_format},
                                                                                   {'header': 'Spill =>',
                                                                                    'header_format': navy_header_format,
                                                                                    'format': y_center_format},
                                                                                   {'header': 'Original Order (Units) 1',
                                                                                    'header_format': navy_header_format,
                                                                                    'format': b_nodec_format,
                                                                                    'formula': '=IF(OR(ISBLANK([@ShipTo]),ISBLANK([@OSKU])),"-",IFERROR(INDEX(ORDERS[],MATCH([@Pair],ORDERS[Pair],0),MATCH("Original Order (Units) 1",ORDERS[#Headers],0)),0))'},
                                                                                   {'header': 'Confirmed (Units) 1',
                                                                                    'header_format': navy_header_format,
                                                                                    'format': b_nodec_format,
                                                                                    'formula': '=_xlfn.IFS(AND(ISBLANK([@ShipTo]),ISBLANK([@OSKU])),"-",AND(ISBLANK([@[Confirmed 1]])=FALSE,ISBLANK([@[Spill =>]])=FALSE,ISBLANK([@[Confirmed 2]])=FALSE,ISBLANK([@[Spill <=]])=FALSE),"-",AND(ISBLANK([@[Spill <=]])=FALSE,ISBLANK([@[Confirmed 2]])=FALSE),MAX(0,SUM([@[Original Order (Units) 1]],[@[Original Order (Units) 2]])-IFERROR(VALUE([@[Confirmed 2]]),0)),AND(ISBLANK([@[Spill =>]])=FALSE,ISBLANK([@[Confirmed 1]])=FALSE),IFERROR(VALUE([@[Confirmed 1]]),0),ISBLANK([@[Confirmed 1]])=FALSE,IFERROR(VALUE([@[Confirmed 1]]),0),TRUE,"-")'},
                                                                                   {'header': 'Spill <=',
                                                                                    'header_format': orng_header_format,
                                                                                    'format': y_center_format},
                                                                                   {'header': 'Confirmed 2',
                                                                                    'header_format': orng_header_format,
                                                                                    'format': y_comma_format},
                                                                                   {'header': 'Diverted (Units) 2',
                                                                                    'header_format': orng_header_format,
                                                                                    'format': y_nodec_format},
                                                                                   {'header': 'DOI +- 2',
                                                                                    'header_format': orng_header_format,
                                                                                    'format': y_nodec_format},
                                                                                   {'header': 'Min Conf (Units) 2',
                                                                                    'header_format': orng_header_format,
                                                                                    'format': y_nodec_format},
                                                                                   {'header': 'Original Order (Units) 2',
                                                                                    'header_format': orng_header_format,
                                                                                    'format': b_nodec_format,
                                                                                    'formula': '=IF(OR(ISBLANK([@ShipTo]),ISBLANK([@OSKU])),"-",IFERROR(INDEX(ORDERS[],MATCH([@Pair],ORDERS[Pair],0),MATCH("Original Order (Units) 2",ORDERS[#Headers],0)),0))'},
                                                                                   {'header': 'Confirmed (Units) 2',
                                                                                    'header_format': orng_header_format,
                                                                                    'format': b_nodec_format,
                                                                                    'formula': '=_xlfn.IFS(AND(ISBLANK([@ShipTo]),ISBLANK([@OSKU])),"-",AND(ISBLANK([@[Confirmed 1]])=FALSE,ISBLANK([@[Spill =>]])=FALSE,ISBLANK([@[Confirmed 2]])=FALSE,ISBLANK([@[Spill <=]])=FALSE),"-",AND(ISBLANK([@[Spill <=]])=FALSE,ISBLANK([@[Confirmed 2]])=FALSE),IFERROR(VALUE([@[Confirmed 2]]),0),AND(ISBLANK([@[Spill =>]])=FALSE,ISBLANK([@[Confirmed 1]])=FALSE),MAX(0,SUM([@[Original Order (Units) 1]],[@[Original Order (Units) 2]])-IFERROR(VALUE([@[Confirmed 1]]),0)),ISBLANK([@[Confirmed 2]])=FALSE,IFERROR(VALUE([@[Confirmed 2]]),0),TRUE,"-")'},
                                                                                   {'header': 'Notes',
                                                                                    'header_format': left_format,
                                                                                    'format': left_format},
                                                                               ]})
        # create table - OSKU
        osku.add_table(0, 0, 100, st.shape[1] - 1, {'name': 'OSKU',
                                                    'style': 'Table Style Medium 2',
                                                    'autofilter': True,
                                                    'columns': [
                                                        {'header': 'Plant', 'header_format': left_format},
                                                        {'header': 'OSKU', 'header_format': left_format},
                                                        {'header': 'Agg_Key', 'header_format': left_format,
                                                         'format': b_center_format,
                                                         'formula': '=IF(OR(ISBLANK([@Plant]),ISBLANK([@OSKU])),"-",_xlfn.CONCAT(VALUE([@Plant]),"-",VALUE([@OSKU])))'},
                                                        {'header': 'MSKU', 'header_format': left_format,
                                                         'format': b_left_format,
                                                         'formula': '=IFERROR(INDEX(ORDERS[],MATCH([@[Agg_Key]],ORDERS[Agg_Key],0),MATCH("MSKU",ORDERS[#Headers],0)),"-")'},
                                                        {'header': 'Avail Inv (Units) 1',
                                                         'header_format': navy_header_format},
                                                        {'header': 'Avail Inv (BBLs) 1',
                                                         'header_format': navy_header_format},
                                                        {'header': 'Orders (BBLs) 1', 'header_format': navy_header_format,
                                                         'format': b_comma_format,
                                                         'formula': '=IF([@[Agg_Key]]="-","-",SUMIFS(ORDERS[Original Order (Units) 1],ORDERS[Agg_Key],[@[Agg_Key]])*IFERROR(INDEX(ORDERS[],MATCH([@[Agg_Key]],ORDERS[Agg_Key],0),MATCH("BBL_Conv",ORDERS[#Headers],0)),0))'},
                                                        {'header': 'Confirmed (BBLs) 1',
                                                         'header_format': navy_header_format,
                                                         'format': b_comma_format,
                                                         'formula': '=IF([@[Agg_Key]]="-","-",SUMIFS(ORDERS[Confirmed (Units) 1],ORDERS[Agg_Key],[@[Agg_Key]])*IFERROR(INDEX(ORDERS[],MATCH([@[Agg_Key]],ORDERS[Agg_Key],0),MATCH("BBL_Conv",ORDERS[#Headers],0)),0))'},
                                                        {'header': 'Confirm % 1', 'header_format': navy_header_format,
                                                         'format': b_prcnt_format,
                                                         'formula': '=IF([@[Agg_Key]]="-","-",IFERROR([@[Confirmed (BBLs) 1]]/[@[Orders (BBLs) 1]],0))'},
                                                        {'header': 'Avail Vol (BBLs) 1',
                                                         'header_format': navy_header_format,
                                                         'format': b_comma_format,
                                                         'formula': '=_xlfn.IFS([@[Agg_Key]]="-","-",ISBLANK([@[Avail Inv (BBLs) 1]]),IFERROR(INDEX(ORDERS[],MATCH([@[Agg_Key]],ORDERS[Agg_Key],0),MATCH("BBL_Conv",ORDERS[#Headers],0))*[@[Avail Inv (Units) 1]],"No Conversion"),TRUE,[@[Avail Inv (BBLs) 1]])'},
                                                        {'header': 'Avail Inv (Units) 2',
                                                         'header_format': orng_header_format},
                                                        {'header': 'Avail Inv (BBLs) 2',
                                                         'header_format': orng_header_format},
                                                        {'header': 'Orders (BBLs) 2', 'header_format': orng_header_format,
                                                         'format': b_comma_format,
                                                         'formula': '=IF([@[Agg_Key]]="-","-",SUMIFS(ORDERS[Original Order (Units) 2],ORDERS[Agg_Key],[@[Agg_Key]])*IFERROR(INDEX(ORDERS[],MATCH([@[Agg_Key]],ORDERS[Agg_Key],0),MATCH("BBL_Conv",ORDERS[#Headers],0)),0))'},
                                                        {'header': 'Confirmed (BBLs) 2',
                                                         'header_format': orng_header_format,
                                                         'format': b_comma_format,
                                                         'formula': '=IF([@[Agg_Key]]="-","-",SUMIFS(ORDERS[Confirmed (Units) 2],ORDERS[Agg_Key],[@[Agg_Key]])*IFERROR(INDEX(ORDERS[],MATCH([@[Agg_Key]],ORDERS[Agg_Key],0),MATCH("BBL_Conv",ORDERS[#Headers],0)),0))'},
                                                        {'header': 'Confirm % 2', 'header_format': orng_header_format,
                                                         'format': b_prcnt_format,
                                                         'formula': '=IF([@[Agg_Key]]="-","-",IFERROR([@[Confirmed (BBLs) 2]]/[@[Orders (BBLs) 2]],0))'},
                                                        {'header': 'Avail Vol (BBLs) 2',
                                                         'header_format': orng_header_format,
                                                         'format': b_comma_format,
                                                         'formula': '=_xlfn.IFS([@[Agg_Key]]="-","-",ISBLANK([@[Avail Inv (BBLs) 2]]),IFERROR(INDEX(ORDERS[],MATCH([@[Agg_Key]],ORDERS[Agg_Key],0),MATCH("BBL_Conv",ORDERS[#Headers],0))*[@[Avail Inv (Units) 2]],"No Conversion"),TRUE,[@[Avail Inv (BBLs) 2]])'},
                                                    ]})
        # create table - INDR
        indr.add_table(0, 0, 100, ir.shape[1] - 1, {'name': 'INDR',
                                                    'style': 'Table Style Medium 2',
                                                    'autofilter': True,
                                                    'columns': [
                                                        {'header': 'MSKU', 'header_format': left_format},
                                                        {'header': 'Line', 'header_format': left_format,
                                                         'format': b_left_format,
                                                         'formula': '=IF(ISBLANK([@MSKU]),"-",IFERROR(INDEX(PROD[],MATCH(INDEX(MADA[],MATCH(VALUE([@MSKU]),MADA[MSKU],0),MATCH("Agg_Key",MADA[#Headers],0)),PROD[Agg_Key],0),MATCH("Plant_Key",PROD[#Headers],0)),"STO"))'},
                                                        {'header': 'Brand', 'header_format': left_format,
                                                         'format': b_left_format,
                                                         'formula': '=IF(ISBLANK([@MSKU]),"-",IFERROR(INDEX(ORDERS[],MATCH(VALUE([@MSKU]),ORDERS[MSKU],0),MATCH("Brand",ORDERS[#Headers],0)),"-"))'},
                                                        {'header': 'Segment', 'header_format': left_format,
                                                         'format': b_left_format,
                                                         'formula': '=IF(ISBLANK([@MSKU]),"-",IFERROR(INDEX(ORDERS[],MATCH(VALUE([@MSKU]),ORDERS[MSKU],0),MATCH("Segment",ORDERS[#Headers],0)),"-"))'},
                                                        {'header': 'Order (BBLs) 1', 'header_format': navy_header_format},
                                                        {'header': 'Cut (BBLs) 1', 'header_format': navy_header_format},
                                                        {'header': 'Volume (BBLs) 1', 'header_format': navy_header_format,
                                                         'format': b_comma_format,
                                                         'formula': '=IF(ISBLANK([@MSKU]),"-",MAX(0,[@[Order (BBLs) 1]]-[@[Cut (BBLs) 1]]))'},
                                                        {'header': 'Order (BBLs) 2', 'header_format': orng_header_format},
                                                        {'header': 'Cut (BBLs) 2', 'header_format': orng_header_format},
                                                        {'header': 'Volume (BBLs) 2', 'header_format': orng_header_format,
                                                         'format': b_comma_format,
                                                         'formula': '=IF(ISBLANK([@MSKU]),"-",MAX(0,[@[Order (BBLs) 2]]-[@[Cut (BBLs) 2]]))'},
                                                    ]})
        # create table - PROD
        prod.add_table(0, 0, rf.shape[0], rf.shape[1] - 1, {'name': 'PROD',
                                                            'style': 'Table Style Medium 2',
                                                            'autofilter': True,
                                                            'columns': [
                                                                {'header': 'Agg_Key', 'header_format': left_format,
                                                                 'format': y_center_format},
                                                                {'header': 'MSKU_Key', 'header_format': left_format,
                                                                 'format': y_center_format},
                                                                {'header': 'Plant_Key',
                                                                 'header_format': left_format,
                                                                 'format': y_center_format},
                                                                {'header': 'OSKU', 'header_format': left_format,
                                                                 'format': center_format},
                                                            ]})
        # create table - RNFQ
        mada.add_table('$I$1:$K$54', {'name': 'RNFQ',
                                      'style': 'Table Style Medium 2',
                                      'autofilter': True,
                                      'columns': [
                                          {'header': 'RF_Code', 'header_format': left_format,
                                           'format': left_format},
                                          {'header': 'Description', 'header_format': left_format,
                                           'format': left_format},
                                          {'header': 'Adjustment', 'header_format': left_format,
                                           'format': nodec_format},
                                      ]})
        """set column formatting"""
        # Set the column width and format - MSKU
        ordr.set_column('A:A', 14, center_format)
        ordr.set_column('B:B', 8, center_format)
        ordr.set_column('C:C', 9, center_format)
        ordr.set_column('D:D', 8, center_format)
        ordr.set_column('E:E', 8, center_format)
        ordr.set_column('F:F', 18, left_format, {'hidden': 1, 'level': 1})
        ordr.set_column('G:G', 38, left_format, {'hidden': 1, 'level': 1})
        ordr.set_column('H:H', 14, left_format, {'hidden': 1, 'level': 1})
        ordr.set_column('I:I', 41, left_format, {'hidden': 1, 'level': 1})
        ordr.set_column('J:J', 14, left_format, {'hidden': 1, 'level': 1})
        ordr.set_column('K:K', 10, left_format, {'hidden': 1, 'level': 1})
        ordr.set_column('L:L', 10, nocomma_format, {'collapsed': 1})
        ordr.set_column('M:M', 10, nocomma_format)
        ordr.set_column('N:N', 18, nodec_format, {'hidden': 1, 'level': 1})
        ordr.set_column('O:O', 18, nodec_format, {'hidden': 1, 'level': 1})
        ordr.set_column('P:P', 11.57, center_format, {'hidden': 1, 'level': 1})
        ordr.set_column('Q:Q', 20, nodec_format, {'hidden': 1, 'level': 1})
        ordr.set_column('R:R', 13, nodec_format, {'hidden': 1, 'level': 1})
        ordr.set_column('S:S', 10, center_format, {'hidden': 1, 'level': 1})
        ordr.set_column('T:T', 13, nodec_format, {'hidden': 1, 'level': 1})
        ordr.set_column('U:U', 13, nodec_format, {'hidden': 1, 'level': 1})
        ordr.set_column('V:V', 13, nodec_format, {'hidden': 1, 'level': 1})
        ordr.set_column('W:W', 13, nodec_format, {'hidden': 1, 'level': 1})
        ordr.set_column('X:X', 11, center_format, {'hidden': 1, 'level': 1})
        ordr.set_column('Y:Y', 11.14, bblcv_format, {'hidden': 1, 'level': 1})
        ordr.set_column('Z:Z', 13.14, nodec_format, {'hidden': 1, 'level': 1})
        ordr.set_column('AA:AA', 10, center_format, {'collapsed': 1})
        ordr.set_column('AB:AB', 9, None)
        ordr.set_column('AC:AC', 16, left_format)
        ordr.set_column('AD:AD', 10, None)
        ordr.set_column('AE:AE', 13, doi_format)
        ordr.set_column('AF:AF', 15.29, doi_format, {'hidden': 1, 'level': 1})
        ordr.set_column('AG:AG', 23.57, nodec_format, {'hidden': 1, 'level': 1})
        ordr.set_column('AH:AH', 11.57, None, {'hidden': 1, 'level': 1})
        ordr.set_column('AI:AI', 13.29, None, {'hidden': 1, 'level': 1})
        ordr.set_column('AJ:AJ', 21, None, {'hidden': 1, 'level': 1})
        ordr.set_column('AK:AK', 12.57, None, {'hidden': 1, 'level': 1})
        ordr.set_column('AL:AL', 21.43, None, {'hidden': 1, 'level': 1})
        ordr.set_column('AM:AM', 13.71, None, {'hidden': 1, 'level': 1})
        ordr.set_column('AN:AN', 15.57, None, {'hidden': 1, 'level': 1})
        ordr.set_column('AO:AO', 17.29, None, {'hidden': 1, 'level': 1})
        ordr.set_column('AP:AP', 20.29, None, {'collapsed': 1})
        ordr.set_column('AQ:AQ', 15.29, None, {'hidden': 1, 'level': 1})
        ordr.set_column('AR:AR', 22.86, nodec_format, {'hidden': 1, 'level': 1})
        ordr.set_column('AS:AS', 11.57, None, {'hidden': 1, 'level': 1})
        ordr.set_column('AT:AT', 13.29, None, {'hidden': 1, 'level': 1})
        ordr.set_column('AU:AU', 21, None, {'hidden': 1, 'level': 1})
        ordr.set_column('AV:AV', 12.57, None, {'hidden': 1, 'level': 1})
        ordr.set_column('AW:AW', 21.43, None, {'hidden': 1, 'level': 1})
        ordr.set_column('AX:AX', 13.71, None, {'hidden': 1, 'level': 1})
        ordr.set_column('AY:AY', 15.57, None, {'hidden': 1, 'level': 1})
        ordr.set_column('AZ:AZ', 17.29, None, {'hidden': 1, 'level': 1})
        ordr.set_column('BA:BA', 20.29, None, {'collapsed': 1})
        ordr.set_column('BB:BB', 23.29, None, {'hidden': 1, 'level': 1})
        ordr.set_column('BC:BC', 23.29, None, {'hidden': 1, 'level': 1})
        ordr.set_column('BD:BD', 17.14, None, {'collapsed': 1})
        ordr.set_column('BE:BE', 21.43)
        # Set the column width and format - ldoi
        ldoi.set_column('A:A', 11)
        ldoi.set_column('B:B', 17.29)
        ldoi.set_column('C:C', 17.29)
        ldoi.set_column('D:D', 17.14, None, {'level': 1})
        ldoi.set_column('E:E', 36.57, None, {'level': 1})
        ldoi.set_column('F:F', 17.43, None, {'level': 1})
        ldoi.set_column('G:G', 18, None, {'level': 1})
        ldoi.set_column('H:H', 19.57, None, {'collapsed': 0})
        ldoi.set_column('I:I', 13.14, None, {'level': 1})
        ldoi.set_column('J:J', 16.57, None, {'level': 1})
        ldoi.set_column('K:K', 13.71, None, {'level': 1})
        ldoi.set_column('L:L', 17.43, None, {'level': 1})
        ldoi.set_column('M:M', 16, None, {'level': 1})
        ldoi.set_column('N:N', 19.57, None, {'level': 1})
        ldoi.set_column('O:O', 16, None, {'level': 1})
        ldoi.set_column('P:P', 19.57, None, {'level': 1})
        ldoi.set_column('Q:Q', 16.57, None, {'level': 1})
        ldoi.set_column('R:R', 14, None, {'collapsed': 0})
        ldoi.set_column('S:S', 12.43, None, {'level': 1})
        ldoi.set_column('T:T', 14, None, {'level': 1})
        ldoi.set_column('U:U', 12.14, None, {'level': 1})
        ldoi.set_column('V:V', 13.29, None, {'level': 1})
        ldoi.set_column('W:W', 12.71, None, {'level': 1})
        ldoi.set_column('X:X', 16, None, {'level': 1})
        ldoi.set_column('Y:Y', 19.57, None, {'level': 1})
        ldoi.set_column('Z:Z', 16.57, None, {'level': 1})
        ldoi.set_column('AA:AA', 13.71, None, {'collapsed': 0})
        # Set the column width and format - DOIS
        dois.set_column('A:A', 10)
        dois.set_column('B:B', 45)
        dois.set_column('C:C', 10)
        dois.set_column('D:D', 10)
        dois.set_column('E:E', 13.71)
        dois.set_column('F:F', 18)
        dois.set_column('G:G', 12)
        # Set the column width and format - PAIR
        volp.set_column('A:A', 10)
        volp.set_column('B:B', 10)
        volp.set_column('C:C', 16)
        volp.set_column('D:D', 45)
        volp.set_column('E:E', 13.42, None, {'level': 1})
        volp.set_column('F:F', 18.43, None, {'level': 1})
        volp.set_column('G:G', 12, None, {'level': 1})
        volp.set_column('H:H', 19, None, {'level': 1})
        volp.set_column('I:I', 9, None, {'level': 1})
        volp.set_column('J:J', 23.57, None, {'level': 1})
        volp.set_column('K:K', 20.29, None, {'collapsed': False})
        volp.set_column('L:L', 9, None, {'level': 1})
        volp.set_column('M:M', 13.43, None, {'level': 1})
        volp.set_column('N:N', 18.43, None, {'level': 1})
        volp.set_column('O:O', 12, None, {'level': 1})
        volp.set_column('P:P', 19, None, {'level': 1})
        volp.set_column('Q:Q', 23.57, None, {'level': 1})
        volp.set_column('R:R', 20.29, None, {'collapsed': False})
        volp.set_column('S:S', 30)
        # Set the column width and format - OSKU
        osku.set_column('A:A', 10)
        osku.set_column('B:B', 10)
        osku.set_column('C:C', 16)
        osku.set_column('D:D', 10)
        osku.set_column('E:E', 18.29, None, {'level': 1})
        osku.set_column('F:F', 17.57, None, {'level': 1})
        osku.set_column('G:G', 16.43, None, {'level': 1})
        osku.set_column('H:H', 19.57, None, {'level': 1})
        osku.set_column('I:I', 13.14, None, {'level': 1})
        osku.set_column('J:J', 17.86, None, {'collapsed': False})
        osku.set_column('K:K', 18.29, None, {'level': 1})
        osku.set_column('L:L', 17.5, None, {'level': 1})
        osku.set_column('M:M', 16.43, None, {'level': 1})
        osku.set_column('N:N', 19.57, None, {'level': 1})
        osku.set_column('O:O', 13.14, None, {'level': 1})
        osku.set_column('P:P', 17.86, None, {'collapsed': False})
        # Set the column width and format - INDR
        indr.set_column('A:A', 10)
        indr.set_column('B:B', 10)
        indr.set_column('C:C', 17.86)
        indr.set_column('D:D', 12.86)
        indr.set_column('E:E', 16)
        indr.set_column('F:F', 16)
        indr.set_column('G:G', 17)
        indr.set_column('H:H', 16)
        indr.set_column('I:I', 16)
        indr.set_column('J:J', 17)
        # Set the column width and format - PROD
        prod.set_column('A:A', 11)
        prod.set_column('B:B', 12.29)
        prod.set_column('C:C', 11.43)
        prod.set_column('D:D', 10)
        prod.set_column('E:E', 10)
        prod.set_column('F:F', 10)
        prod.set_column('G:G', 10)
        # Set the column width and format - MADA
        mada.set_column('A:A', 12)
        mada.set_column('B:B', 12)
        mada.set_column('C:C', 12)
        mada.set_column('D:D', 12)
        mada.set_column('E:E', 13.14)
        mada.set_column('F:F', 12)
        mada.set_column('G:G', 12)
        mada.set_column('I:I', 15.57)
        mada.set_column('J:J', 32.14)
        mada.set_column('K:K', 13)
        """create worksheet tables with formulas and entry columns"""
        # write in cell A2 - DOI
        ldoi.write('A1', None, header_format)
        ldoi.write('A2', 'Subtotals', header_format)
        # create conditional formatting - Orders table
        ordr.conditional_format(f"$AH$2:$AH${str(dmax_row + 1)}", {'type': 'cell',
                                                                   'criteria': '=',
                                                                   'value': '"No Forecast"',
                                                                   'format': con_bad})
        ordr.conditional_format(f"$AN$2:$AN${str(dmax_row + 1)}", {'type': 'data_bar', 'bar_color': '#63C384',
                                                                   'bar_direction': 'right', 'data_bar_2010': True,
                                                                   'min_type': 'num', 'max_type': 'num', 'min_value': 0,
                                                                   'max_value': 1})
        ordr.conditional_format(f"$AO$2:$AO${str(dmax_row + 1)}", {'type': 'icon_set',
                                                                   'icon_style': '3_traffic_lights',
                                                                   'icons': [
                                                                       {'criteria': '>=', 'type': 'number', 'value': 1},
                                                                       {'criteria': '>=', 'type': 'number', 'value': -1}
                                                                   ]})
        ordr.conditional_format(f"$AS$2:$AS${str(dmax_row + 1)}", {'type': 'cell',
                                                                   'criteria': '=',
                                                                   'value': '"No Forecast"',
                                                                   'format': con_bad})
        ordr.conditional_format(f"$AY$2:$AY${str(dmax_row + 1)}", {'type': 'data_bar', 'bar_color': '#63C384',
                                                                   'bar_direction': 'right', 'data_bar_2010': True,
                                                                   'min_type': 'num', 'max_type': 'num', 'min_value': 0,
                                                                   'max_value': 1})
        ordr.conditional_format(f"$AZ$2:$AZ${str(dmax_row + 1)}", {'type': 'icon_set',
                                                                   'icon_style': '3_traffic_lights',
                                                                   'icons': [
                                                                       {'criteria': '>=', 'type': 'number', 'value': 1},
                                                                       {'criteria': '>=', 'type': 'number', 'value': -1}
                                                                   ]})
        # create conditional formatting - Subtotal table
        ldoi.conditional_format(f"$D$2:$E$2", {'type': 'icon_set',
                                               'icon_style': '3_traffic_lights',
                                               'icons': [
                                                   {'criteria': '>=', 'type': 'number', 'value': 1},
                                                   {'criteria': '>=', 'type': 'number', 'value': -1}
                                               ]})
        # create conditional formatting - Line table
        ldoi.conditional_format(f"$I$4:$I${str(cmax_row + 3)}", {'type': 'data_bar', 'bar_color': '#63C384',
                                                                 'bar_direction': 'right', 'data_bar_2010': True,
                                                                 'min_type': 'num', 'max_type': 'num', 'min_value': 0,
                                                                 'max_value': 1})
        ldoi.conditional_format(f"$J$4:$J${str(cmax_row + 3)}", {'type': 'icon_set',
                                                                 'icon_style': '3_traffic_lights',
                                                                 'icons': [
                                                                     {'criteria': '>=', 'type': 'number', 'value': 1},
                                                                     {'criteria': '>=', 'type': 'number', 'value': -1}
                                                                 ]})
        ldoi.conditional_format(f"$K$4:$K${str(cmax_row + 3)}", {'type': 'icon_set',
                                                                 'icon_style': '3_symbols_circled',
                                                                 'icons': [
                                                                     {'criteria': '>=', 'type': 'number', 'value': 0},
                                                                     {'criteria': '<', 'type': 'number', 'value': 0}
                                                                 ]})
        ldoi.conditional_format(f"$O$4:$O${str(cmax_row + 3)}", {'type': 'data_bar', 'bar_color': '#63C384',
                                                                 'bar_direction': 'right', 'data_bar_2010': True,
                                                                 'min_type': 'num', 'max_type': 'num', 'min_value': 0,
                                                                 'max_value': 1})
        ldoi.conditional_format(f"$P$4:$P${str(cmax_row + 3)}", {'type': 'icon_set',
                                                                 'icon_style': '3_traffic_lights',
                                                                 'icons': [
                                                                     {'criteria': '>=', 'type': 'number', 'value': 1},
                                                                     {'criteria': '>=', 'type': 'number', 'value': -1}
                                                                 ]})
        ldoi.conditional_format(f"$Q$4:$Q${str(cmax_row + 3)}", {'type': 'icon_set',
                                                                 'icon_style': '3_symbols_circled',
                                                                 'icons': [
                                                                     {'criteria': '>=', 'type': 'number', 'value': 0},
                                                                     {'criteria': '<', 'type': 'number', 'value': 0}
                                                                 ]})
        # create conditional formatting - DOI table week 1
        ldoi.conditional_format(f"$I${str(2 + doi_row_start)}:$I${str(pmax_row + doi_row_start + 1)}", {'type': 'cell',
                                                                                                        'criteria': '=',
                                                                                                        'value': '""',
                                                                                                        'format': con_ntry})
        ldoi.conditional_format(f"$I${str(2 + doi_row_start)}:$I${str(pmax_row + doi_row_start + 1)}", {'type': 'cell',
                                                                                                        'criteria': '=',
                                                                                                        'value': 0,
                                                                                                        'format': con_bad})
        ldoi.conditional_format(f"$J${str(2 + doi_row_start)}:$J${str(pmax_row + doi_row_start + 1)}", {'type': 'cell',
                                                                                                        'criteria': '=',
                                                                                                        'value': '""',
                                                                                                        'format': con_ntry})
        ldoi.conditional_format(f"$J${str(2 + doi_row_start)}:$J${str(pmax_row + doi_row_start + 1)}", {'type': 'cell',
                                                                                                        'criteria': '=',
                                                                                                        'value': 0,
                                                                                                        'format': con_bad})
        ldoi.conditional_format(f"$K${str(2 + doi_row_start)}:$K${str(pmax_row + doi_row_start + 1)}", {'type': 'cell',
                                                                                                        'criteria': '=',
                                                                                                        'value': '""',
                                                                                                        'format': con_ntry})
        ldoi.conditional_format(f"$K${str(2 + doi_row_start)}:$K${str(pmax_row + doi_row_start + 1)}", {'type': 'cell',
                                                                                                        'criteria': '=',
                                                                                                        'value': 0,
                                                                                                        'format': con_bad})
        ldoi.conditional_format(f"$Q${str(2 + doi_row_start)}:$Q${str(pmax_row + doi_row_start + 1)}", {'type': 'icon_set',
                                                                                                        'icon_style': '3_traffic_lights',
                                                                                                        'icons': [
                                                                                                            {
                                                                                                                'criteria': '>=',
                                                                                                                'type': 'number',
                                                                                                                'value': 1},
                                                                                                            {
                                                                                                                'criteria': '>=',
                                                                                                                'type': 'number',
                                                                                                                'value': -1}
                                                                                                        ]})
        ldoi.conditional_format(f"$R${str(2 + doi_row_start)}:$R${str(pmax_row + doi_row_start + 1)}", {'type': 'icon_set',
                                                                                                        'icon_style': '3_symbols_circled',
                                                                                                        'icons': [
                                                                                                            {
                                                                                                                'criteria': '>=',
                                                                                                                'type': 'number',
                                                                                                                'value': 0},
                                                                                                            {
                                                                                                                'criteria': '<',
                                                                                                                'type': 'number',
                                                                                                                'value': 0}
                                                                                                        ]})
        # create conditional formatting - DOI table week 2
        ldoi.conditional_format(f"$S${str(2 + doi_row_start)}:$S${str(pmax_row + doi_row_start + 1)}", {'type': 'cell',
                                                                                                        'criteria': '=',
                                                                                                        'value': '""',
                                                                                                        'format': con_ntry})
        ldoi.conditional_format(f"$S${str(2 + doi_row_start)}:$S${str(pmax_row + doi_row_start + 1)}", {'type': 'cell',
                                                                                                        'criteria': '=',
                                                                                                        'value': 0,
                                                                                                        'format': con_bad})
        ldoi.conditional_format(f"$T${str(2 + doi_row_start)}:$T${str(pmax_row + doi_row_start + 1)}", {'type': 'cell',
                                                                                                        'criteria': '=',
                                                                                                        'value': '""',
                                                                                                        'format': con_ntry})
        ldoi.conditional_format(f"$T${str(2 + doi_row_start)}:$T${str(pmax_row + doi_row_start + 1)}", {'type': 'cell',
                                                                                                        'criteria': '=',
                                                                                                        'value': 0,
                                                                                                        'format': con_bad})
        ldoi.conditional_format(f"$U${str(2 + doi_row_start)}:$U${str(pmax_row + doi_row_start + 1)}", {'type': 'cell',
                                                                                                        'criteria': '=',
                                                                                                        'value': '""',
                                                                                                        'format': con_ntry})
        ldoi.conditional_format(f"$U${str(2 + doi_row_start)}:$U${str(pmax_row + doi_row_start + 1)}", {'type': 'cell',
                                                                                                        'criteria': '=',
                                                                                                        'value': 0,
                                                                                                        'format': con_bad})
        ldoi.conditional_format(f"$Z${str(2 + doi_row_start)}:$Z${str(pmax_row + doi_row_start + 1)}", {'type': 'icon_set',
                                                                                                        'icon_style': '3_traffic_lights',
                                                                                                        'icons': [
                                                                                                            {
                                                                                                                'criteria': '>=',
                                                                                                                'type': 'number',
                                                                                                                'value': 1},
                                                                                                            {
                                                                                                                'criteria': '>=',
                                                                                                                'type': 'number',
                                                                                                                'value': -1}
                                                                                                        ]})
        ldoi.conditional_format(f"$AA${str(2 + doi_row_start)}:$AA${str(pmax_row + doi_row_start + 1)}", {'type': 'icon_set',
                                 'icon_style': '3_symbols_circled',
                                 'icons': [
                                     {
                                         'criteria': '>=',
                                         'type': 'number',
                                         'value': 0},
                                     {
                                         'criteria': '<',
                                         'type': 'number',
                                         'value': 0}
                                 ]})
        # create conditional formatting - DOIS
        dois.conditional_format(f"$B$2:$B$501", {'type': 'cell',
                                                 'criteria': '=',
                                                 'value': '"[!] DUPLICATE SHIPTO LISTED"',
                                                 'format': con_bad})
        # create conditional formatting - VOLP
        volp.conditional_format(f"$D$2:$D${2 + 1 + len(mpo_shipto_list) + 250}", {'type': 'cell',
                                                                                  'criteria': '=',
                                                                                  'value': '"[!] DUPLICATE PAIR LISTED"',
                                                                                  'format': con_bad})
        # add in data validation - DOIS
        dois.data_validation('$D2:$D101', {'validate': 'decimal',
                                           'criteria': 'between',
                                           'minimum': 0.000001,
                                           'maximum': 99,
                                           'error_title': 'Input value not valid!',
                                           'error_message': 'Please enter a decimal or integer greater than 0 and less than or equal to 99.\n\n    0 < DOI Cap <= 99'})
        # set the active worksheet
        ldoi.activate()
    # create popup error message if any error is encountered
    except:
        # deleted newly created directory and any files inside
        shutil.rmtree(fpathname)
        # create pop up message warning
        ctypes.windll.user32.MessageBoxW(0,
                                         'An error has occurred. Please check the selected Weekly Confirmations - Data Source file and try again.',
                                         'Confirmations', 0)
    # check for errors when saving and closing
    while True:
        try:
            # save and close workbook
            writer.save()
            # workbook.close()
        except xlsxwriter.exceptions.FileCreateError:
            # create message box alert
            decision = ctypes.windll.user32.MessageBoxW(0, "Exception caught in workbook.close(): %s\n"
                                                           "Please close the file if it is open in Excel.\n"
                                                           "Try to write file again?", 'Confirmations Planner',
                                                        0x04)
            if decision != 2:
                continue
        break
    # create new unique folder name with plant list without overwriting
    fpathnewname = uniq_file_maker(f"{os.path.dirname(planner)} - ({joined_plant_list})")
    # rename the newly created directory
    os.rename(savepath, fpathnewname)
    # fix os separators
    cutfile = os_split_fixer(cutfile)
    olmfile = os_split_fixer(olmfile)
    arrfile = os_split_fixer(arrfile)
    # move the source files to the new directory
    shutil.move(cutfile, fpathnewname)
    shutil.move(olmfile, fpathnewname)
    shutil.move(arrfile, fpathnewname)
    # create message box alert
    ctypes.windll.user32.MessageBoxW(0, 'Confirmations Planner completed successfully!\n\n' +
                                     '    Processing Time:    ' + str(datetime.datetime.now() - begin_time)[2:7],
                                     'Confirmations Planner', 0)
    return


def conf_planner_scriptor():
    """Dynamically create sales order confirmation and rejection Winshuttle script files"""
    global chbox1
    # get checkbox status
    onefile = chbox1.get()
    # check if onefile is checked or not
    if onefile == 1:
        onefile = True
    else:
        onefile = False
    # get full timestamp
    full_timestamp = get_timestamps()[9][:-3]
    # select the source file
    Tk().withdraw()
    file = askopenfilename(title='Select a Confirmations Planner',
                           filetypes=[('Excel files', '.xlsx .xlsb .xlsm .xls')])
    # end function if no file is selected
    if not file:
        ctypes.windll.user32.MessageBoxW(0, 'No file selected.', 'Confirmations Winshuttle', 0)
        return
    # create directory name using week range from file name
    foldername = f"{str(os.path.basename(file).split(' ')[0])} SO Scripts - {full_timestamp}"
    # create new unique directory
    savepath, fpathname = uniq_dir_maker(foldername)
    # end function if no savepath is selected
    if not savepath:
        return
    # start timer
    begin_time = datetime.datetime.now()
    # create progress bar window
    progr_bar_win = Tk()
    # set window size
    progr_bar_win.geometry('575x85')
    # set window always on top
    progr_bar_win.attributes('-topmost', True)
    # name progress bar title window
    progr_bar_win.title('Script Creation')
    # create application icon
    mc_icon = resource_path('mc_icon.ico')
    progr_bar_win.iconbitmap(mc_icon)
    # Gets the requested values of the height and width
    windowWidth = progr_bar_win.winfo_reqwidth()
    windowHeight = progr_bar_win.winfo_reqheight()
    # Gets both half the screen width/height and window width/height
    positionRight = int(progr_bar_win.winfo_screenwidth() / 2 - windowWidth / 2)
    positionDown = int(progr_bar_win.winfo_screenheight() / 2 - windowHeight / 2)
    positionRight = int(positionRight - (windowWidth * 1.5)) + 15
    positionDown = int(positionDown - (windowHeight / 2)) + 120
    # Positions the window in the center of the page.
    progr_bar_win.geometry("+{}+{}".format(positionRight, positionDown))
    # create label widget for % completion and string variable for updating
    progr_l_p = Label(progr_bar_win, text='0%', font=('Inconsolata', 11), bg='#FFFFFF')
    progr_l_p.place(x=525, y=40, anchor='ne')
    # create label widget for naming stages and string variable for updating
    progr_l_s = Label(progr_bar_win, text='Initializing...', font=('Inconsolata', 11), bg='#FFFFFF')
    progr_l_s.place(x=35, y=40)
    # create progress bar widget
    progr_bar = Progressbar(progr_bar_win, orient=HORIZONTAL, length=500, mode='determinate')
    progr_bar.place(x=30, y=15)
    # start progress bar and update window
    progr_bar.start()
    progr_bar.update_idletasks()
    time.sleep(.3)
    # increase progress bar and update window
    p_step = random.randint(5, 12)
    progr_bar['value'] = p_step
    progr_l_p.config(text=str(p_step) + '%')
    progr_l_s.config(text='Creating Change Scripts')
    progr_bar.update_idletasks()
    time.sleep(.3)
    # check if selected file is already open
    try:
        # create sales order confirmations script files
        if onefile:
            try:
                l_conf, f_conf = conf_win_splitter_onefile(file, savepath, full_timestamp)
            except (ZeroDivisionError, UnboundLocalError):
                l_conf = 0
        else:
            try:
                l_conf, f_conf = conf_win_splitter(file, savepath, full_timestamp)
            except (ZeroDivisionError, UnboundLocalError):
                l_conf = 0
    except:
        # deleted newly created directory and any files inside
        shutil.rmtree(fpathname)
        # end the progress bar and update window
        progr_bar.stop()
        progr_bar.update_idletasks()
        # kill progress bar and window
        progr_bar.destroy()
        progr_bar_win.destroy()
        # create pop up message warning
        ctypes.windll.user32.MessageBoxW(0,
                                         'An error has occurred. Please check the following suggestions for the selected Confirmations Planner and try again\n\n  -  The file has been saved and closed\n  -  The file is not open or in use by another program\n  -  The worksheet names have not been changed \n\t(Orders, Changes)\n  -  The column names have not been changed \n\t(SO, OSKU, Original_Order (Units), Confirmed (Units))\n  -  The file is saved as in Excel Workbook format \n\t(.xlsx, .xlsb, .xlsm, .xls)',
                                         'Confirmations Winshuttle', 0)
        return
    # increase progress bar and update window
    p_step = random.randint(30, 40)
    progr_bar['value'] = p_step
    progr_l_p.config(text=str(p_step) + '%')
    progr_l_s.config(text='Creating Reject Scripts')
    progr_bar.update_idletasks()
    time.sleep(.3)
    # check if selected file is already open
    try:
        # create sales order rejection script files
        if onefile:
            try:
                l_rejd, f_rejd = rej_win_splitter_onefile(file, savepath, full_timestamp)
            except (ZeroDivisionError, UnboundLocalError):
                l_rejd = 0
        else:
            try:
                l_rejd, f_rejd = rej_win_splitter(file, savepath, full_timestamp)
            except (ZeroDivisionError, UnboundLocalError):
                l_rejd = 0
    except:
        # deleted newly created directory and any files inside
        shutil.rmtree(fpathname)
        # end the progress bar and update window
        progr_bar.stop()
        progr_bar.update_idletasks()
        # kill progress bar and window
        progr_bar.destroy()
        progr_bar_win.destroy()
        # create pop up message warning
        ctypes.windll.user32.MessageBoxW(0,
                                         'An error has occurred. Please check the following suggestions for the selected Confirmations Planner and try again\n\n  -  The file has been saved and closed\n  -  The file is not open or in use by another program\n  -  The worksheet names have not been changed \n\t(Orders, Changes)\n  -  The column names have not been changed \n\t(SO, OSKU, Original_Order (Units), Confirmed (Units))\n  -  The file is saved as in Excel Workbook format \n\t(.xlsx, .xlsb, .xlsm, .xls)',
                                         'Confirmations Winshuttle', 0)
        return
    # increase progress bar and update window
    p_step = random.randint(60, 70)
    progr_bar['value'] = p_step
    progr_l_p.config(text=str(p_step) + '%')
    progr_l_s.config(text='Creating Sales Order Add Scripts')
    progr_bar.update_idletasks()
    time.sleep(.3)
    # check if selected file is already open
    try:
        # create sales order confirmations script files
        if onefile:
            try:
                l_news, f_news = newso_win_splitter_onefile(file, savepath, full_timestamp)
            except (ZeroDivisionError, UnboundLocalError):
                l_news = 0
        else:
            try:
                l_news, f_news = newso_win_splitter(file, savepath, full_timestamp)
            except (ZeroDivisionError, UnboundLocalError):
                l_news = 0
    except:
        # deleted newly created directory and any files inside
        shutil.rmtree(fpathname)
        # end the progress bar and update window
        progr_bar.stop()
        progr_bar.update_idletasks()
        # kill progress bar and window
        progr_bar.destroy()
        progr_bar_win.destroy()
        # create pop up message warning
        ctypes.windll.user32.MessageBoxW(0,
                                         'An error has occurred. Please check the following suggestions for the selected Confirmations Planner and try again\n\n  -  The file has been saved and closed\n  -  The file is not open or in use by another program\n  -  The worksheet names have not been changed \n\t(Orders, Changes)\n  -  The column names have not been changed \n\t(SO, OSKU, Original_Order (Units), Confirmed (Units))\n  -  The file is saved as in Excel Workbook format \n\t(.xlsx, .xlsb, .xlsm, .xls)',
                                         'Confirmations Winshuttle', 0)
        return
    # increase progress bar and update window
    p_step = random.randint(75, 85)
    progr_bar['value'] = p_step
    progr_l_p.config(text=str(p_step) + '%')
    progr_l_s.config(text='Creating New Sales Order Scripts')
    progr_bar.update_idletasks()
    time.sleep(.3)
    # check if selected file is already open
    try:
        # create sales order confirmations script files
        if onefile:
            try:
                l_newc, f_newc = newconf_win_splitter_onefile(file, savepath, full_timestamp)
            except (ZeroDivisionError, UnboundLocalError):
                l_newc = 0
        else:
            try:
                l_newc, f_newc = newconf_win_splitter(file, savepath, full_timestamp)
            except (ZeroDivisionError, UnboundLocalError):
                l_newc = 0
    except:
        # deleted newly created directory and any files inside
        shutil.rmtree(fpathname)
        # end the progress bar and update window
        progr_bar.stop()
        progr_bar.update_idletasks()
        # kill progress bar and window
        progr_bar.destroy()
        progr_bar_win.destroy()
        # create pop up message warning
        ctypes.windll.user32.MessageBoxW(0,
                                         'An error has occurred. Please check the following suggestions for the selected Confirmations Planner and try again\n\n  -  The file has been saved and closed\n  -  The file is not open or in use by another program\n  -  The worksheet names have not been changed \n\t(Orders, Changes)\n  -  The column names have not been changed \n\t(SO, OSKU, Original_Order (Units), Confirmed (Units))\n  -  The file is saved as in Excel Workbook format \n\t(.xlsx, .xlsb, .xlsm, .xls)',
                                         'Confirmations Winshuttle', 0)
        return
    # increase progress bar and update window
    p_step = random.randint(90, 95)
    progr_bar['value'] = p_step
    progr_l_p.config(text=str(p_step) + '%')
    progr_l_s.config(text='Finalizing Winshuttle file creation')
    progr_bar.update_idletasks()
    time.sleep(.3)
    # increase progress bar and update window
    p_step = 100
    progr_bar['value'] = p_step
    progr_l_p.config(text=str(p_step) + '%')
    progr_bar.update_idletasks()
    time.sleep(.3)
    # end the progress bar and update window
    progr_bar.stop()
    progr_bar.update_idletasks()
    # kill progress bar and window
    progr_bar.destroy()
    progr_bar_win.destroy()
    # check if no files were created
    if l_conf == 0 and l_rejd == 0 and l_news == 0 and l_newc == 0:
        # deleted newly created directory and any files inside
        shutil.rmtree(fpathname)
        # create message box alert
        ctypes.windll.user32.MessageBoxW(0, 'There are no Sales Order scripts required to complete processing for the selected Confirmations Planner. No Sales Order scripts have been created.',
                                         'Scripts Completed', 0)
        return
    # if files were created
    else:
        # check if any new sales orders were created
        if l_newc > 0:
            # end timer and format result
            hours, minutes, seconds = str(datetime.datetime.now() - begin_time).split(' ')[-1].split('.')[0].split(':')
            s_time = hours + ':' + minutes + ':' + seconds
            # create message box alert
            ctypes.windll.user32.MessageBoxW(0, 'Your Sales Order scripts have been created successfully! New Sales Orders have been planned that you must create to complete processing Confirmations. Please be sure to enter the newly created Sales Orders numbers on the Orders worksheet of the Confirmations Planner in either the SO 1 or SO 2 columns depending on the associated week of the Sales Orders. These new numbers should overwrite a zero currently listed in that column associated with the Distributor-OSKU-Week pair. This will ensure future changes are captured correctly during any additional planning or Winshuttle activities.\n\n' +
                                             f"\tProcessing Time\t\t- " + str(s_time) + '\n' +
                                             f"\tLines Changed\t\t- " + str('{:,}'.format(l_conf)) + '\n' +
                                             f"\tLines Rejected\t\t- " + str('{:,}'.format(l_rejd)) + '\n' +
                                             f"\tLines Added\t\t- " + str('{:,}'.format(l_news)) + '\n' +
                                             f"\tNew Sales Orders\t\t- " + str('{:,}'.format(l_newc)),
                                             f"Scripts Completed", 0)
        # if no new sales orders were created
        else:
            # end timer and format result
            hours, minutes, seconds = str(datetime.datetime.now() - begin_time).split(' ')[-1].split('.')[0].split(':')
            s_time = hours + ':' + minutes + ':' + seconds
            # create message box alert
            ctypes.windll.user32.MessageBoxW(0, 'Your Sales Order scripts have been created successfully!\n\n' +
                                             f"\tProcessing Time\t\t- " + str(s_time) + '\n' +
                                             f"\tLines Changed\t\t- " + str('{:,}'.format(l_conf)) + '\n' +
                                             f"\tLines Rejected\t\t- " + str('{:,}'.format(l_rejd)) + '\n' +
                                             f"\tLines Added\t\t- " + str('{:,}'.format(l_news)) + '\n' +
                                             f"\tNew Sales Orders\t\t- " + str('{:,}'.format(l_newc)),
                                             f"Scripts Completed", 0)
        return


def unmatch_shuffle_sorting(source_list: list) -> list:
    """
    Function to rearrange numbers in array such that no two adjacent numbers are same.py

    Args:
        source_list (list): list of numbers to be rearranged

    Returns:
        list: list of numbers that have been shuffled to minimize the number of adjacent matching values
    """
    # check if source_list is numpy.ndarray
    if isinstance(source_list, np.ndarray):
        # convert numpy.ndarray to list
        source_list = source_list.tolist()
    # get frequency of each value in source_list and store in dictionary with key as value and value as frequency
    frequency_dict = {value: source_list.count(value) for value in source_list}
    # get key and values from sorted dictionary as a list of lists
    sorted_list = [[value, key] for key, value in frequency_dict.items()]
    # sort list by value
    sorted_list.sort()
    # reverse list order
    sorted_list.reverse()
    # create a list to store values of all zeroes with len(sort_column)
    sorted_values = [0] * len(source_list)
    # initialize previous value as -1 and it's frequency as -1
    prev_value = [-1, -1]
    # initialize index of sorted_values as 0
    sv_index = 0
    # iterate over sorted list while length of sorted list is greater than 0
    while len(sorted_list) != 0:
        # get first value from sorted list
        value = sorted_list[0]
        # pop value from sorted list
        sorted_list.pop(0)
        # insert value from first sublist in sorted_values at index sv_index
        sorted_values[sv_index] = value[1]
        # if remaining frequency of previous value is greater than 0 add it back to sorted list and sort by frequency
        if prev_value[0] > 0:
            # append previous value in sorted_list
            sorted_list.append(prev_value)
            # update sorted list frequency
            sorted_list.sort()
            # reverse sorted list order
            sorted_list.reverse()
        # update previous value by reducing the remaining frequency of the value by 1
        prev_value = [value[0] - 1, value[1]]
        # update sorted values index
        sv_index += 1
    # check if there are no zeroes in sorted_values, meaning the original list was fully shuffled
    if 0 not in sorted_values:
        return sorted_values
    # if original list could not be fully shuffled then attempt to shuffle remaining values recursively
    else:
        # remove all zeroes from sorted_values
        sorted_values = [x for x in sorted_values if x != 0]
        # get new frequency of each value in sorted values and store in dictionary
        sorted_freq_dict = {value: sorted_values.count(value) for value in sorted_values}
        # for each key in sorted_freq_dict check if it is in frequency_dict
        for key in sorted_freq_dict:
            # check if key is in frequency_dict
            if key in frequency_dict:
                # update sorted freq dictionary value with difference between key values
                sorted_freq_dict[key] = frequency_dict[key] - sorted_freq_dict[key]
        # remove keys with value 0 from sorted_freq_dict
        sorted_freq_dict = {key: value for key, value in sorted_freq_dict.items() if value != 0}
        # create list with each key in sorted_freq_dict listed by their value
        new_sublist = [[key] * sorted_freq_dict[key] for key in sorted_freq_dict]
        # convert list of lists to list
        unsorted_new_sublist = [item for sublist in new_sublist for item in sublist]
        # add new sublist to sorted_values
        unmatched_values = sorted_values + unsorted_new_sublist
        return unmatched_values


def match_shuffle_sorting(sord_list: list, osku_list: list, conf_list: list, third_col: str):
    """
    Take sales order dataframe and shuffle sales orders so as few as possible matching values are adjacent.
    Then match the sales order dataframe with the original sales order key and return the new sales order dataframe.

    Args:
        sord_list (list): list of sales order list
        osku_list (list): list of orderable SKU list from sales order
        conf_list (list): list of confirmed units for orderable SKU from sales order
        third_col (str): desired output name for the third column of sales order dataframe

    Returns:
        confirmed_results_df (pd.DataFrame): dataframe of sales orders, orderable SKUs, and confirmed units now shuffled
    """
    # check if all three lists are the same length
    if len(sord_list) != len(osku_list) or len(sord_list) != len(conf_list) or len(osku_list) != len(conf_list):
        # create windows popup messagebox that displays error message
        ctypes.windll.user32.MessageBoxW(0,
                                         'The number of Sales Orders, OSKUs, and Confirm (Units) values do not match. Please check the data and try again.',
                                         'Confirmations', 0)
        # error returns a blank dataframe
        return pd.DataFrame()
    # check if each source list is numpy.ndarray and convert
    if isinstance(sord_list, np.ndarray):
        # convert numpy.ndarray to list
        sord_list = sord_list.tolist()
    if isinstance(osku_list, np.ndarray):
        # convert numpy.ndarray to list
        osku_list = osku_list.tolist()
    if isinstance(conf_list, np.ndarray):
        # convert numpy.ndarray to list
        conf_list = conf_list.tolist()
    # initialize empty list to store the data
    confirmed_results_list = []
    # combine values at same index into one string in new list
    for i in range(len(sord_list)):
        confirmed_results_list.append(f"{sord_list[i]}-{osku_list[i]}-{conf_list[i]}")
    # shuffle sales order list
    sorted_sord_list = unmatch_shuffle_sorting(sord_list)
    # create a list to store values of all zeroes
    sorted_matched_results = [0] * len(sorted_sord_list)
    # for each value in sorted sales order list check where that sales order exists in the confirmed results list
    for i in range(len(sorted_sord_list)):
        # find where value in sorted sales order matches add the [sales order, OSKU, confirmed units] into sorted matched results
        for c in range(len(confirmed_results_list)):
            # check if the sorted sales order matches the confirmed sales order
            # also check if the order data already exists in the sorted matched results list
            if str(sorted_sord_list[i]) == confirmed_results_list[c].split('-')[0] \
                    and confirmed_results_list[c] not in sorted_matched_results:
                # create a list to store the values that are now sorted by sales order with found matches
                sorted_matched_results[i] = confirmed_results_list[c]
    # convert list of strings into list of split string lists
    sorted_matched_results = [x.split('-') for x in sorted_matched_results]
    # create pandas dataframe with list of lists
    confirmed_results_df = pd.DataFrame(sorted_matched_results, columns=['SO', 'OSKU', third_col])
    # format the dataframe as all integers
    confirmed_results_df = confirmed_results_df.apply(pd.to_numeric, downcast='integer', errors='ignore')
    return confirmed_results_df


def conf_win_splitter(file: str, savepath: str, timestamp: str) -> list:
    """
    Create Winshuttle scripts for changing OSKU volume on a sales order split automatically into multiple files.

    Conditions
    1. Sales Order does not equal 0
    2. Original Order is greater 0
    3. Confirmed units is greater than 0
    4. Confirmed units does not equal Original Order
    """
    # turn off warnings
    pd.options.mode.chained_assignment = None
    warnings.filterwarnings('ignore', category=UserWarning)
    warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)
    # read Excel files to dataframes and fill blanks with 0
    df = pd.read_excel(file, sheet_name='Orders', na_values=0)
    # check if dataframe is empty
    if df.empty:
        return [0, 0]
    # reindex dataframe
    df = df[['SO 1', 'Original Order (Units) 1', 'Confirmed (Units) 1',
             'SO 2', 'Original Order (Units) 2', 'Confirmed (Units) 2',
             'Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description']]
    # only keep valid table rows and remove any data entered by a user below the table
    df.dropna(axis=0, how='all', subset=['Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description'], inplace=True)
    # fill blanks with 0
    df = df.fillna(0)
    # create new dataframes from orders dataframe
    wf1 = df[['SO 1', 'OSKU', 'Confirmed (Units) 1', 'Original Order (Units) 1']]
    wf2 = df[['SO 2', 'OSKU', 'Confirmed (Units) 2', 'Original Order (Units) 2']]
    # rename columns
    wf1 = wf1.rename(columns={'SO 1': 'SO', 'Confirmed (Units) 1': 'Confirmed',
                              'Original Order (Units) 1': 'Original Order'})
    wf2 = wf2.rename(columns={'SO 2': 'SO', 'Confirmed (Units) 2': 'Confirmed',
                              'Original Order (Units) 2': 'Original Order'})
    # concatenate both dataframes
    wf = pd.concat([wf1, wf2], ignore_index=True)
    # only keep rows that meet above conditions
    wf = wf[(wf['SO'] != 0)]
    wf = wf[(wf['Original Order'] > 0)]
    wf = wf[(wf['Confirmed'] > 0)]
    wf = wf[(wf['Confirmed'] != wf['Original Order'])]
    # check if dataframe is empty
    if wf.empty:
        return [0, 0]
    # convert series back to lists
    salo_series = wf['SO'].tolist()
    osku_series = wf['OSKU'].tolist()
    conf_series = wf['Confirmed'].tolist()
    # shuffle the sales order dataframe
    df = match_shuffle_sorting(salo_series, osku_series, conf_series, 'Confirmed (Units)').reset_index(drop=True)
    # dynamically calculate number of dataframe splits
    splits = math.ceil(df.shape[0] / math.ceil(math.sqrt(df.shape[0]) * 8))
    # create original splits and new splits var
    osplits = splits
    # create blank series for Winshuttle processing
    df['RUN LOG'] = ''
    df['VALIDATE LOG'] = ''
    # reindex dataframe
    df = df[['RUN LOG', 'VALIDATE LOG', 'SO', 'OSKU', 'Confirmed (Units)']]
    # rename columns
    df = df.rename(columns={'SO': 'Sales Document VBAK-VBELN', 'OSKU': 'Material Number RV45A-PO_MATNR',
                            'Confirmed (Units)': 'Cumulative order quantity in sales units RV45A-KWMENG'})
    # create loop variables
    valu_flag = 0
    n_loops = 0
    # split dataframe into multiple sub-arrays to create Winshuttle scripts
    while True:
        # iterate loop variable
        n_loops += 1
        try:
            # split dataframe into list of roughly equal sized dataframes
            frames = np.array_split(df, splits)
        except ValueError:
            # decrease splits until array_split stops returning an error
            while valu_flag == 0:
                try:
                    # split dataframe into list of roughly equal sized dataframes
                    frames = np.array_split(df, splits)
                    # update flag and exit while loop
                    valu_flag = 1
                    break
                # if array_split returns error increase splits and retry
                except ValueError:
                    # update number of splits
                    splits = osplits - n_loops
        # end while loop when dataframe has been properly split
        break
    # check if original splits var was adjusted
    if splits != osplits:
        # add one split back to correct from final loop
        splits += 1
    # create new file for each dataframe in list
    for n, frame in enumerate(frames):
        # convert numpy array to dataframe
        rf = pd.DataFrame(frame)
        # create file name as a variable
        scriptfile = f"{savepath}SO CHG - ({str(n + 1).zfill(len(str(splits)))}-{str(splits)}) - {timestamp}.xlsx"
        # rewrite filepath with correct operating system separators
        scriptfile = os_split_fixer(scriptfile)
        # create a Pandas Excel writer using XlsxWriter as the engine
        writer = pd.ExcelWriter(scriptfile, engine='xlsxwriter')
        # create workbook and worksheet objects
        scriptbook = writer.book
        # write each DataFrame to a specific sheet and reset the index
        rf.to_excel(writer, sheet_name='Script', index=False)
        script = writer.sheets['Script']
        # create formatting methods for workbook
        comma_format = scriptbook.add_format({'num_format': '#,##0', 'align': 'right'})
        nomma_format = scriptbook.add_format({'num_format': '#0', 'align': 'right'})
        left_format = scriptbook.add_format({'align': 'left'})
        center_format = scriptbook.add_format({'align': 'center'})
        header_format = scriptbook.add_format({'text_wrap': True, 'align': 'center'})
        header_format.set_bg_color('#9AC4F5')  # Winshuttle blue
        header_format.set_font_color('#FFFFFF')  # white
        header_format.set_align('vcenter')
        header_format.set_bold(True)
        # freeze first row
        script.freeze_panes(1, 0)
        # set row and column formatting on Script worksheet
        script.set_row(0, 75)
        script.write('A1', 'RUN LOG', header_format)
        script.write('B1', 'VALIDATE LOG', header_format)
        script.write('C1', 'Sales Document VBAK-VBELN', header_format)
        script.write('D1', 'Material Number RV45A-PO_MATNR', header_format)
        script.write('E1', 'Cumulative order quantity in sales units RV45A-KWMENG', header_format)
        script.set_column('A:A', 45, left_format)
        script.set_column('B:B', 14, nomma_format)
        script.set_column('C:C', 14, center_format)
        script.set_column('D:D', 14, center_format)
        script.set_column('E:E', 14, comma_format)
        # save and close workbook
        writer.save()
    # turn chained assignment warning back on
    pd.options.mode.chained_assignment = 'warn'
    return [df.shape[0], splits]


def conf_win_splitter_onefile(file: str, savepath: str, timestamp: str) -> list:
    """
    Create Winshuttle scripts for changing OSKU volume on a sales order all in one file.

    Conditions
    1. Sales Order does not equal 0
    2. Original Order is greater 0
    3. Confirmed units is greater than 0
    4. Confirmed units does not equal Original Order
    """
    # turn off warnings
    pd.options.mode.chained_assignment = None
    warnings.filterwarnings('ignore', category=UserWarning)
    warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)
    # read Excel files to dataframes and fill blanks with 0
    df = pd.read_excel(file, sheet_name='Orders', na_values=0)
    # check if dataframe is empty
    if df.empty:
        return [0, 0]
    # reindex dataframe
    df = df[['SO 1', 'Original Order (Units) 1', 'Confirmed (Units) 1',
             'SO 2', 'Original Order (Units) 2', 'Confirmed (Units) 2',
             'Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description']]
    # only keep valid table rows and remove any data entered by a user below the table
    df.dropna(axis=0, how='all', subset=['Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description'], inplace=True)
    # fill blanks with 0
    df = df.fillna(0)
    # create new dataframes from orders dataframe
    wf1 = df[['SO 1', 'OSKU', 'Confirmed (Units) 1', 'Original Order (Units) 1']]
    wf2 = df[['SO 2', 'OSKU', 'Confirmed (Units) 2', 'Original Order (Units) 2']]
    # rename columns
    wf1 = wf1.rename(columns={'SO 1': 'SO', 'Confirmed (Units) 1': 'Confirmed',
                              'Original Order (Units) 1': 'Original Order'})
    wf2 = wf2.rename(columns={'SO 2': 'SO', 'Confirmed (Units) 2': 'Confirmed',
                              'Original Order (Units) 2': 'Original Order'})
    # concatenate both dataframes
    wf = pd.concat([wf1, wf2], ignore_index=True)
    # only keep rows that meet above conditions
    wf = wf[(wf['SO'] != 0)]
    wf = wf[(wf['Original Order'] > 0)]
    wf = wf[(wf['Confirmed'] > 0)]
    wf = wf[(wf['Confirmed'] != wf['Original Order'])]
    # check if dataframe is empty
    if wf.empty:
        return [0, 0]
    # convert series back to lists
    salo_series = wf['SO'].tolist()
    osku_series = wf['OSKU'].tolist()
    conf_series = wf['Confirmed'].tolist()
    # shuffle the sales order dataframe
    df = match_shuffle_sorting(salo_series, osku_series, conf_series, 'Confirmed (Units)').reset_index(drop=True)
    # create blank series for Winshuttle processing
    df['RUN LOG'] = ''
    df['VALIDATE LOG'] = ''
    # reindex dataframe
    df = df[['RUN LOG', 'VALIDATE LOG', 'SO', 'OSKU', 'Confirmed (Units)']]
    # rename columns
    df = df.rename(columns={'SO': 'Sales Document VBAK-VBELN', 'OSKU': 'Material Number RV45A-PO_MATNR',
                            'Confirmed (Units)': 'Cumulative order quantity in sales units RV45A-KWMENG'})
    # create file name as a variable
    scriptfile = f"{savepath}SO CHG - (1 of 1) - {timestamp}.xlsx"
    # rewrite filepath with correct operating system separators
    scriptfile = os_split_fixer(scriptfile)
    # create a Pandas Excel writer using XlsxWriter as the engine
    writer = pd.ExcelWriter(scriptfile, engine='xlsxwriter')
    # create workbook and worksheet objects
    scriptbook = writer.book
    # write each DataFrame to a specific sheet and reset the index
    df.to_excel(writer, sheet_name='Script', index=False)
    script = writer.sheets['Script']
    # create formatting methods for workbook
    comma_format = scriptbook.add_format({'num_format': '#,##0', 'align': 'right'})
    nomma_format = scriptbook.add_format({'num_format': '#0', 'align': 'right'})
    left_format = scriptbook.add_format({'align': 'left'})
    center_format = scriptbook.add_format({'align': 'center'})
    header_format = scriptbook.add_format({'text_wrap': True, 'align': 'center'})
    header_format.set_bg_color('#9AC4F5')  # Winshuttle blue
    header_format.set_font_color('#FFFFFF')  # white
    header_format.set_align('vcenter')
    header_format.set_bold(True)
    # freeze first row
    script.freeze_panes(1, 0)
    # set row and column formatting on Script worksheet
    script.set_row(0, 75)
    script.write('A1', 'RUN LOG', header_format)
    script.write('B1', 'VALIDATE LOG', header_format)
    script.write('C1', 'Sales Document VBAK-VBELN', header_format)
    script.write('D1', 'Material Number RV45A-PO_MATNR', header_format)
    script.write('E1', 'Cumulative order quantity in sales units RV45A-KWMENG', header_format)
    script.set_column('A:A', 45, left_format)
    script.set_column('B:B', 14, nomma_format)
    script.set_column('C:C', 14, center_format)
    script.set_column('D:D', 14, center_format)
    script.set_column('E:E', 14, comma_format)
    # save and close workbook
    writer.save()
    # turn chained assignment warning back on
    pd.options.mode.chained_assignment = 'warn'
    return [df.shape[0], 1]


def rej_win_splitter(file: str, savepath: str, timestamp: str) -> list:
    """
    Create Winshuttle scripts for rejecting an OSKU on a sales order split automatically into multiple files.

    Conditions
    1. Sales Order does not equal 0
    2. Original Order is greater 0
    3. Confirmed units is 0
    """
    # turn off warnings
    pd.options.mode.chained_assignment = None
    warnings.filterwarnings('ignore', category=UserWarning)
    warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)
    # read Excel files to dataframes and fill blanks with 0
    df = pd.read_excel(file, sheet_name='Orders', na_values=0)
    # check if dataframe is empty
    if df.empty:
        return [0, 0]
    # reindex dataframe
    df = df[['SO 1', 'Original Order (Units) 1', 'Confirmed (Units) 1',
             'SO 2', 'Original Order (Units) 2', 'Confirmed (Units) 2',
             'Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description']]
    # only keep valid table rows and remove any data entered by a user below the table
    df.dropna(axis=0, how='all', subset=['Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description'], inplace=True)
    # fill blanks with 0
    df = df.fillna(0)
    # create new dataframes from orders dataframe
    wf1 = df[['SO 1', 'OSKU', 'Confirmed (Units) 1', 'Original Order (Units) 1']]
    wf2 = df[['SO 2', 'OSKU', 'Confirmed (Units) 2', 'Original Order (Units) 2']]
    # rename columns
    wf1 = wf1.rename(
        columns={'SO 1': 'SO', 'Confirmed (Units) 1': 'Confirmed', 'Original Order (Units) 1': 'Original Order'})
    wf2 = wf2.rename(
        columns={'SO 2': 'SO', 'Confirmed (Units) 2': 'Confirmed', 'Original Order (Units) 2': 'Original Order'})
    # concatenate both dataframes
    wf = pd.concat([wf1, wf2], ignore_index=True)
    # only keep rows that meet above conditions
    wf = wf[(wf['SO'] != 0)]
    wf = wf[(wf['Original Order'] > 0)]
    wf = wf[(wf['Confirmed'] == 0)]
    # check if dataframe is empty
    if wf.empty:
        return [0, 0]
    # convert series back to lists
    salo_series = wf['SO'].tolist()
    osku_series = wf['OSKU'].tolist()
    conf_series = wf['Confirmed'].tolist()
    # shuffle the sales order dataframe
    df = match_shuffle_sorting(salo_series, osku_series, conf_series, 'Confirmed (Units)').reset_index(drop=True)
    # dynamically calculate number of dataframe splits
    splits = math.ceil(df.shape[0] / math.ceil(math.sqrt(df.shape[0]) * 8))
    # create original splits and new splits var
    osplits = splits
    # create series
    df['Rej'] = 'Z1'
    df['RUN LOG'] = ''
    df['VALIDATE LOG'] = ''
    # reindex dataframe
    df = df[['RUN LOG', 'VALIDATE LOG', 'SO', 'OSKU', 'Rej']]
    # rename columns
    df = df.rename(columns={'SO': 'Sales Document VBAK-VBELN', 'OSKU': 'Material Number RV45A-PO_MATNR',
                            'Rej': 'Reason for rejection of quotations and sales orders VBAP-ABGRU'})
    # create loop variables
    valu_flag = 0
    n_loops = 0
    # split dataframe into multiple sub-arrays to create Winshuttle scripts
    while True:
        # iterate loop variable
        n_loops += 1
        try:
            # split dataframe into list of roughly equal sized dataframes
            frames = np.array_split(df, splits)
        except ValueError:
            # decrease splits until array_split stops returning an error
            while valu_flag == 0:
                try:
                    # split dataframe into list of roughly equal sized dataframes
                    frames = np.array_split(df, splits)
                    # update flag and exit while loop
                    valu_flag = 1
                    break
                # if array_split returns error increase splits and retry
                except ValueError:
                    # update number of splits
                    splits = osplits - n_loops
        # end while loop when dataframe has been properly split
        break
    # check if original splits var was adjusted
    if splits != osplits:
        # add one split back to correct from final loop
        splits += 1
    # create new file for each dataframe in list
    for n, frame in enumerate(frames):
        # convert numpy array to dataframe
        rf = pd.DataFrame(frame)
        # create file name as a variable
        scriptfile = f"{savepath}SO REJ - ({str(n + 1).zfill(len(str(splits)))}-{str(splits)}) - {timestamp}.xlsx"
        # rewrite filepath with correct operating system separators
        scriptfile = os_split_fixer(scriptfile)
        # create a Pandas Excel writer using XlsxWriter as the engine
        writer = pd.ExcelWriter(scriptfile, engine='xlsxwriter')
        # create workbook and worksheet objects
        scriptbook = writer.book
        # write each DataFrame to a specific sheet and reset the index
        rf.to_excel(writer, sheet_name='Script', index=False)
        script = writer.sheets['Script']
        # create formatting methods for workbook
        comma_format = scriptbook.add_format({'num_format': '#,##0', 'align': 'right'})
        nomma_format = scriptbook.add_format({'num_format': '#0', 'align': 'right'})
        left_format = scriptbook.add_format({'align': 'left'})
        center_format = scriptbook.add_format({'align': 'center'})
        header_format = scriptbook.add_format({'text_wrap': True, 'align': 'center', })
        header_format.set_bg_color('#DA9694')
        header_format.set_font_color('#FFFFFF')  # white
        header_format.set_align('vcenter')
        header_format.set_bold(True)
        # freeze first row
        script.freeze_panes(1, 0)
        # set row and column formatting on Script worksheet
        script.set_row(0, 75)
        script.write('A1', 'RUN LOG', header_format)
        script.write('B1', 'VALIDATE LOG', header_format)
        script.write('C1', 'Sales Document VBAK-VBELN', header_format)
        script.write('D1', 'Material Number RV45A-PO_MATNR', header_format)
        script.write('E1', 'Reason for rejection of quotations and sales orders VBAP-ABGRU', header_format)
        script.set_column('A:A', 45, left_format)
        script.set_column('B:B', 14, nomma_format)
        script.set_column('C:D', 14, center_format)
        script.set_column('E:E', 14, center_format)
        # save and close workbook
        writer.save()
    # turn chained assignment warning back on
    pd.options.mode.chained_assignment = 'warn'
    return [df.shape[0], splits]


def rej_win_splitter_onefile(file: str, savepath: str, timestamp: str) -> list:
    """
    Create Winshuttle scripts for rejecting an OSKU on a sales order all in one file.

    Conditions
    1. Sales Order does not equal 0
    2. Original Order is greater 0
    3. Confirmed units is 0
    """
    # turn off warnings
    pd.options.mode.chained_assignment = None
    warnings.filterwarnings('ignore', category=UserWarning)
    warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)
    # read Excel files to dataframes and fill blanks with 0
    df = pd.read_excel(file, sheet_name='Orders', na_values=0)
    # check if dataframe is empty
    if df.empty:
        return [0, 0]
    # reindex dataframe
    df = df[['SO 1', 'Original Order (Units) 1', 'Confirmed (Units) 1',
             'SO 2', 'Original Order (Units) 2', 'Confirmed (Units) 2',
             'Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description']]
    # only keep valid table rows and remove any data entered by a user below the table
    df.dropna(axis=0, how='all', subset=['Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description'], inplace=True)
    # fill blanks with 0
    df = df.fillna(0)
    # create new dataframes from orders dataframe
    wf1 = df[['SO 1', 'OSKU', 'Confirmed (Units) 1', 'Original Order (Units) 1']]
    wf2 = df[['SO 2', 'OSKU', 'Confirmed (Units) 2', 'Original Order (Units) 2']]
    # rename columns
    wf1 = wf1.rename(
        columns={'SO 1': 'SO', 'Confirmed (Units) 1': 'Confirmed', 'Original Order (Units) 1': 'Original Order'})
    wf2 = wf2.rename(
        columns={'SO 2': 'SO', 'Confirmed (Units) 2': 'Confirmed', 'Original Order (Units) 2': 'Original Order'})
    # concatenate both dataframes
    wf = pd.concat([wf1, wf2], ignore_index=True)
    # only keep rows that meet above conditions
    wf = wf[(wf['SO'] != 0)]
    wf = wf[(wf['Original Order'] > 0)]
    wf = wf[(wf['Confirmed'] == 0)]
    # check if dataframe is empty
    if wf.empty:
        return [0, 0]
    # convert series back to lists
    salo_series = wf['SO'].tolist()
    osku_series = wf['OSKU'].tolist()
    conf_series = wf['Confirmed'].tolist()
    # shuffle the sales order dataframe
    df = match_shuffle_sorting(salo_series, osku_series, conf_series, 'Confirmed (Units)').reset_index(drop=True)
    # create series
    df['Rej'] = 'Z1'
    df['RUN LOG'] = ''
    df['VALIDATE LOG'] = ''
    # reindex dataframe
    df = df[['RUN LOG', 'VALIDATE LOG', 'SO', 'OSKU', 'Rej']]
    # rename columns
    df = df.rename(columns={'SO': 'Sales Document VBAK-VBELN', 'OSKU': 'Material Number RV45A-PO_MATNR',
                            'Rej': 'Reason for rejection of quotations and sales ordersVBAP-ABGRU'})
    # create file name as a variable
    scriptfile = f"{savepath}SO REJ - (1 of 1) - {timestamp}.xlsx"
    # rewrite filepath with correct operating system separators
    scriptfile = os_split_fixer(scriptfile)
    # create a Pandas Excel writer using XlsxWriter as the engine
    writer = pd.ExcelWriter(scriptfile, engine='xlsxwriter')
    # create workbook and worksheet objects
    scriptbook = writer.book
    # write each DataFrame to a specific sheet and reset the index
    df.to_excel(writer, sheet_name='Script', index=False)
    script = writer.sheets['Script']
    # create formatting methods for workbook
    comma_format = scriptbook.add_format({'num_format': '#,##0', 'align': 'right'})
    nomma_format = scriptbook.add_format({'num_format': '#0', 'align': 'right'})
    left_format = scriptbook.add_format({'align': 'left'})
    center_format = scriptbook.add_format({'align': 'center'})
    header_format = scriptbook.add_format({'text_wrap': True, 'align': 'center', })
    header_format.set_bg_color('#DA9694')
    header_format.set_font_color('#FFFFFF')  # white
    header_format.set_align('vcenter')
    header_format.set_bold(True)
    # freeze first row
    script.freeze_panes(1, 0)
    # set row and column formatting on Script worksheet
    script.set_row(0, 75)
    script.write('A1', 'RUN LOG', header_format)
    script.write('B1', 'VALIDATE LOG', header_format)
    script.write('C1', 'Sales Document VBAK-VBELN', header_format)
    script.write('D1', 'Material Number RV45A-PO_MATNR', header_format)
    script.write('E1', 'Reason for rejection of quotations and sales orders VBAP-ABGRU', header_format)
    script.set_column('A:A', 45, left_format)
    script.set_column('B:B', 14, nomma_format)
    script.set_column('C:D', 14, center_format)
    script.set_column('E:E', 14, comma_format)
    # save and close workbook
    writer.save()
    # turn chained assignment warning back on
    pd.options.mode.chained_assignment = 'warn'
    return [df.shape[0], 1]


def newconf_win_splitter(file: str, savepath: str, timestamp: str) -> list:
    """
    Create Winshuttle scripts for creating new sales orders and adding an OSKU split automatically into multiple files.

    Conditions
    1. Sales Order is 0
    2. Confirmed units is greater than 0
    """
    # turn off warnings
    pd.options.mode.chained_assignment = None
    warnings.filterwarnings('ignore', category=UserWarning)
    warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)
    # read Excel files to dataframes and fill blanks with 0
    df = pd.read_excel(file, sheet_name='Orders', na_values=0)
    # check if dataframe is empty
    if df.empty:
        return [0, 0]
    # reindex dataframe
    df = df[['SO 1', 'Original Order (Units) 1', 'Confirmed (Units) 1',
             'SO 2', 'Original Order (Units) 2', 'Confirmed (Units) 2',
             'Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description', 'Mode']]
    # only keep valid table rows and remove any data entered by a user below the table
    df.dropna(axis=0, how='all', subset=['Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description'], inplace=True)
    # fill blanks with 0
    df = df.fillna(0)
    # create new dataframes from orders dataframe
    wf1 = df[['SO 1', 'OSKU', 'Confirmed (Units) 1', 'Original Order (Units) 1', 'ShipTo', 'Plant', 'Mode']]
    wf2 = df[['SO 2', 'OSKU', 'Confirmed (Units) 2', 'Original Order (Units) 2', 'ShipTo', 'Plant', 'Mode']]
    # get date three and four weeks from now
    wp3_today = datetime.datetime.today() + datetime.timedelta(days=21)
    wp4_today = datetime.datetime.today() + datetime.timedelta(days=28)
    # create yr-wk three and four weeks from now
    wf1['Yr-Wk'] = wp3_today.strftime('%Y') + '-' + wp3_today.strftime('%W')
    wf2['Yr-Wk'] = wp4_today.strftime('%Y') + '-' + wp4_today.strftime('%W')
    # rename columns
    wf1 = wf1.rename(columns={'SO 1': 'SO', 'Confirmed (Units) 1': 'Confirmed (Units)',
                              'Original Order (Units) 1': 'Original Order'})
    wf2 = wf2.rename(columns={'SO 2': 'SO', 'Confirmed (Units) 2': 'Confirmed (Units)',
                              'Original Order (Units) 2': 'Original Order'})
    # concatenate both dataframes
    df = pd.concat([wf1, wf2], ignore_index=True)
    # only keep rows that meet above conditions
    df = df[(df['SO'] == 0)]
    df = df[(df['Confirmed (Units)'] > 0)]
    # check if dataframe is empty
    if df.empty:
        return [0, 0]
    # dynamically calculate number of dataframe splits
    splits = math.ceil(df.shape[0] / math.ceil(math.sqrt(df.shape[0]) * 8))
    # create original splits and new splits var
    osplits = splits
    # create blank series for Winshuttle processing
    df['RUN LOG'] = ''
    df['VALIDATE LOG'] = ''
    df['SO'] = ''
    # reindex dataframe
    df = df[['RUN LOG', 'VALIDATE LOG', 'ShipTo', 'Plant', 'Yr-Wk', 'Mode', 'SO', 'OSKU', 'Confirmed (Units)']]
    # rename columns
    df = df.rename(columns={'SO': 'Sales Document VBAK-VBELN', 'OSKU': 'Material Number RV45A-PO_MATNR',
                            'Confirmed (Units)': 'Cumulative order quantity in sales units RV45A-KWMENG',
                            'Mode': 'SO Transportation Mode'})
    # create loop variables
    valu_flag = 0
    n_loops = 0
    # split dataframe into multiple sub-arrays to create Winshuttle scripts
    while True:
        # iterate loop variable
        n_loops += 1
        try:
            # split dataframe into list of roughly equal sized dataframes
            frames = np.array_split(df, splits)
        except ValueError:
            # decrease splits until array_split stops returning an error
            while valu_flag == 0:
                try:
                    # split dataframe into list of roughly equal sized dataframes
                    frames = np.array_split(df, splits)
                    # update flag and exit while loop
                    valu_flag = 1
                    break
                # if array_split returns error increase splits and retry
                except ValueError:
                    # update number of splits
                    splits = osplits - n_loops
        # end while loop when dataframe has been properly split
        break
    # check if original splits var was adjusted
    if splits != osplits:
        # add one split back to correct from final loop
        splits += 1
    # create new file for each dataframe in list
    for n, frame in enumerate(frames):
        # convert numpy array to dataframe
        rf = pd.DataFrame(frame)
        # create file name as a variable
        scriptfile = f"{savepath}New SO - ({str(n + 1).zfill(len(str(splits)))}-{str(splits)}) - {timestamp}.xlsx"
        # rewrite filepath with correct operating system separators
        scriptfile = os_split_fixer(scriptfile)
        # create a Pandas Excel writer using XlsxWriter as the engine
        writer = pd.ExcelWriter(scriptfile, engine='xlsxwriter')
        # create workbook and worksheet objects
        scriptbook = writer.book
        # write each DataFrame to a specific sheet and reset the index
        rf.to_excel(writer, sheet_name='Script', index=False)
        script = writer.sheets['Script']
        # create formatting methods for workbook
        comma_format = scriptbook.add_format({'num_format': '#,##0', 'align': 'right'})
        nomma_format = scriptbook.add_format({'num_format': '#0', 'align': 'right'})
        left_format = scriptbook.add_format({'align': 'left'})
        center_format = scriptbook.add_format({'align': 'center'})
        header_format = scriptbook.add_format({'text_wrap': True, 'align': 'center'})
        header_format.set_bg_color('#FCD5B4')  # peach
        header_format.set_font_color('#FFFFFF')  # white
        header_format.set_align('vcenter')
        header_format.set_bold(True)
        # freeze first row
        script.freeze_panes(1, 0)
        # set row and column formatting on Script worksheet
        script.set_row(0, 75)
        script.write('A1', 'RUN LOG', header_format)
        script.write('B1', 'VALIDATE LOG', header_format)
        script.write('C1', 'ShipTo', header_format)
        script.write('D1', 'Plant', header_format)
        script.write('E1', 'Yr-Wk', header_format)
        script.write('F1', 'SO Transportation Mode', header_format)
        script.write('G1', 'Sales Document VBAK-VBELN', header_format)
        script.write('H1', 'Material Number RV45A-PO_MATNR', header_format)
        script.write('I1', 'Cumulative order quantity in sales units RV45A-KWMENG', header_format)
        script.set_column('A:A', 45, left_format)
        script.set_column('B:B', 14, nomma_format)
        script.set_column('C:C', 14, center_format)
        script.set_column('D:D', 14, center_format)
        script.set_column('E:E', 14, center_format)
        script.set_column('F:F', 14, center_format)
        script.set_column('G:G', 14, center_format)
        script.set_column('H:H', 14, center_format)
        script.set_column('I:I', 14, comma_format)
        # save and close workbook
        writer.save()
    # turn chained assignment warning back on
    pd.options.mode.chained_assignment = 'warn'
    return [df.shape[0], splits]


def newconf_win_splitter_onefile(file: str, savepath: str, timestamp: str) -> list:
    """
    Create Winshuttle scripts for creating new sales orders and adding an OSKU all in one file.

    Conditions
    1. Sales Order is 0
    2. Confirmed units is greater than 0
    """
    # turn off warnings
    pd.options.mode.chained_assignment = None
    warnings.filterwarnings('ignore', category=UserWarning)
    warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)
    # read Excel files to dataframes and fill blanks with 0
    df = pd.read_excel(file, sheet_name='Orders', na_values=0)
    # check if dataframe is empty
    if df.empty:
        return [0, 0]
    # reindex dataframe
    df = df[['SO 1', 'Original Order (Units) 1', 'Confirmed (Units) 1',
             'SO 2', 'Original Order (Units) 2', 'Confirmed (Units) 2',
             'Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description', 'Mode']]
    # only keep valid table rows and remove any data entered by a user below the table
    df.dropna(axis=0, how='all', subset=['Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description'], inplace=True)
    # fill blanks with 0
    df = df.fillna(0)
    # create new dataframes from orders dataframe
    wf1 = df[['SO 1', 'OSKU', 'Confirmed (Units) 1', 'Original Order (Units) 1', 'ShipTo', 'Plant', 'Mode']]
    wf2 = df[['SO 2', 'OSKU', 'Confirmed (Units) 2', 'Original Order (Units) 2', 'ShipTo', 'Plant', 'Mode']]
    # get date three and four weeks from now
    wp3_today = datetime.datetime.today() + datetime.timedelta(days=21)
    wp4_today = datetime.datetime.today() + datetime.timedelta(days=28)
    # create yr-wk three and four weeks from now
    wf1['Yr-Wk'] = wp3_today.strftime('%Y') + '-' + wp3_today.strftime('%W')
    wf2['Yr-Wk'] = wp4_today.strftime('%Y') + '-' + wp4_today.strftime('%W')
    # rename columns
    wf1 = wf1.rename(columns={'SO 1': 'SO', 'Confirmed (Units) 1': 'Confirmed (Units)',
                              'Original Order (Units) 1': 'Original Order'})
    wf2 = wf2.rename(columns={'SO 2': 'SO', 'Confirmed (Units) 2': 'Confirmed (Units)',
                              'Original Order (Units) 2': 'Original Order'})
    # concatenate both dataframes
    df = pd.concat([wf1, wf2], ignore_index=True)
    # only keep rows that meet above conditions
    df = df[(df['SO'] == 0)]
    df = df[(df['Confirmed (Units)'] > 0)]
    # check if dataframe is empty
    if df.empty:
        return [0, 0]
    # create blank series for Winshuttle processing
    df['RUN LOG'] = ''
    df['VALIDATE LOG'] = ''
    df['SO'] = ''
    # reindex dataframe
    df = df[['RUN LOG', 'VALIDATE LOG', 'ShipTo', 'Plant', 'Yr-Wk', 'Mode', 'SO', 'OSKU', 'Confirmed (Units)']]
    # rename columns
    df = df.rename(columns={'SO': 'Sales Document VBAK-VBELN', 'OSKU': 'Material Number RV45A-PO_MATNR',
                            'Confirmed (Units)': 'Cumulative order quantity in sales units RV45A-KWMENG',
                            'Mode': 'SO Transportation Mode'})
    # create file name as a variable
    scriptfile = f"{savepath}New SO - (1 of 1) - {timestamp}.xlsx"
    # rewrite filepath with correct operating system separators
    scriptfile = os_split_fixer(scriptfile)
    # create a Pandas Excel writer using XlsxWriter as the engine
    writer = pd.ExcelWriter(scriptfile, engine='xlsxwriter')
    # create workbook and worksheet objects
    scriptbook = writer.book
    # write each DataFrame to a specific sheet and reset the index
    df.to_excel(writer, sheet_name='Script', index=False)
    script = writer.sheets['Script']
    # create formatting methods for workbook
    comma_format = scriptbook.add_format({'num_format': '#,##0', 'align': 'right'})
    nomma_format = scriptbook.add_format({'num_format': '#0', 'align': 'right'})
    left_format = scriptbook.add_format({'align': 'left'})
    center_format = scriptbook.add_format({'align': 'center'})
    header_format = scriptbook.add_format({'text_wrap': True, 'align': 'center'})
    header_format.set_bg_color('#FCD5B4')  # peach
    header_format.set_font_color('#FFFFFF')  # white
    header_format.set_align('vcenter')
    header_format.set_bold(True)
    # freeze first row
    script.freeze_panes(1, 0)
    # set row and column formatting on Script worksheet
    script.set_row(0, 75)
    script.write('A1', 'RUN LOG', header_format)
    script.write('B1', 'VALIDATE LOG', header_format)
    script.write('C1', 'ShipTo', header_format)
    script.write('D1', 'Plant', header_format)
    script.write('E1', 'Yr-Wk', header_format)
    script.write('F1', 'SO Transportation Mode', header_format)
    script.write('G1', 'Sales Document VBAK-VBELN', header_format)
    script.write('H1', 'Material Number RV45A-PO_MATNR', header_format)
    script.write('I1', 'Cumulative order quantity in sales units RV45A-KWMENG', header_format)
    script.set_column('A:A', 45, left_format)
    script.set_column('B:B', 14, nomma_format)
    script.set_column('C:C', 14, center_format)
    script.set_column('D:D', 14, center_format)
    script.set_column('E:E', 14, center_format)
    script.set_column('F:F', 14, center_format)
    script.set_column('G:G', 14, nomma_format)
    script.set_column('H:H', 14, center_format)
    script.set_column('I:I', 14, comma_format)
    # save and close workbook
    writer.save()
    # turn chained assignment warning back on
    pd.options.mode.chained_assignment = 'warn'
    return [df.shape[0], 1]


def newso_win_splitter(file: str, savepath: str, timestamp: str) -> list:
    """
    Create Winshuttle scripts for adding a new OSKU to a sales order split automatically into multiple files.

    Conditions
    1. Sales Order does not equal 0
    2. Original Order is 0
    3. Confirmed units is greater than 0
    """
    # turn off warnings
    pd.options.mode.chained_assignment = None
    warnings.filterwarnings('ignore', category=UserWarning)
    warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)
    # read Excel files to dataframes and fill blanks with 0
    df = pd.read_excel(file, sheet_name='Orders', na_values=0)
    # check if dataframe is empty
    if df.empty:
        return [0, 0]
    # reindex dataframe
    df = df[['SO 1', 'Original Order (Units) 1', 'Confirmed (Units) 1',
             'SO 2', 'Original Order (Units) 2', 'Confirmed (Units) 2',
             'Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description']]
    # only keep valid table rows and remove any data entered by a user below the table
    df.dropna(axis=0, how='all', subset=['Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description'], inplace=True)
    # fill blanks with 0
    df = df.fillna(0)
    # combine confirmed units and plant in a string
    df['Combined 1'] = df['Confirmed (Units) 1'].astype(str) + ';' + df['Plant'].astype(str)
    df['Combined 2'] = df['Confirmed (Units) 2'].astype(str) + ';' + df['Plant'].astype(str)
    # create new dataframes from orders dataframe
    wf1 = df[['SO 1', 'OSKU', 'Confirmed (Units) 1', 'Original Order (Units) 1', 'Combined 1']]
    wf2 = df[['SO 2', 'OSKU', 'Confirmed (Units) 2', 'Original Order (Units) 2', 'Combined 2']]
    # rename columns
    wf1 = wf1.rename(columns={'SO 1': 'SO', 'Confirmed (Units) 1': 'Confirmed',
                              'Original Order (Units) 1': 'Original Order', 'Combined 1': 'Combined'})
    wf2 = wf2.rename(columns={'SO 2': 'SO', 'Confirmed (Units) 2': 'Confirmed',
                              'Original Order (Units) 2': 'Original Order', 'Combined 2': 'Combined'})
    # concatenate both dataframes
    wf = pd.concat([wf1, wf2], ignore_index=True)
    # only keep rows that meet above conditions
    wf = wf[(wf['SO'] != 0)]
    wf = wf[(wf['Original Order'] == 0)]
    wf = wf[(wf['Confirmed'] > 0)]
    # check if dataframe is empty
    if wf.empty:
        return [0, 0]
    # convert series back to lists
    salo_series = wf['SO'].tolist()
    osku_series = wf['OSKU'].tolist()
    conf_series = wf['Combined'].tolist()
    # shuffle the sales order dataframe
    df = match_shuffle_sorting(salo_series, osku_series, conf_series, 'Combined').reset_index(drop=True)
    # dynamically calculate number of dataframe splits
    splits = math.ceil(df.shape[0] / math.ceil(math.sqrt(df.shape[0]) * 8))
    # create original splits and new splits var
    osplits = splits
    # create blank series for Winshuttle processing
    df['RUN LOG'] = ''
    df['VALIDATE LOG'] = ''
    # reindex dataframe
    df = df[['RUN LOG', 'VALIDATE LOG', 'SO', 'OSKU', 'Combined']]
    # create new series by splitting Combined series
    df['Confirmed (Units)'] = df['Combined'].str.split(';', expand=True)[0]
    df['Plant'] = df['Combined'].str.split(';', expand=True)[1]
    # reindex dataframe
    df = df[['RUN LOG', 'VALIDATE LOG', 'SO', 'OSKU', 'Confirmed (Units)', 'Plant']]
    # format series as integers
    df['Confirmed (Units)'] = df['Confirmed (Units)'].astype(float).astype(int)
    df['Plant'] = df['Plant'].astype(float).astype(int)
    # rename columns
    df = df.rename(columns={'SO': 'Sales Document VBAK-VBELN', 'OSKU': 'Material Number RV45A-PO_MATNR',
                            'Confirmed (Units)': 'Cumulative order quantity in sales units RV45A-KWMENG',
                            'Plant': 'Plant (Own or External) VBAP-WERKS'})
    # create loop variables
    valu_flag = 0
    n_loops = 0
    # split dataframe into multiple sub-arrays to create Winshuttle scripts
    while True:
        # iterate loop variable
        n_loops += 1
        try:
            # split dataframe into list of roughly equal sized dataframes
            frames = np.array_split(df, splits)
        except ValueError:
            # decrease splits until array_split stops returning an error
            while valu_flag == 0:
                try:
                    # split dataframe into list of roughly equal sized dataframes
                    frames = np.array_split(df, splits)
                    # update flag and exit while loop
                    valu_flag = 1
                    break
                # if array_split returns error increase splits and retry
                except ValueError:
                    # update number of splits
                    splits = osplits - n_loops
        # end while loop when dataframe has been properly split
        break
    # check if original splits var was adjusted
    if splits != osplits:
        # add one split back to correct from final loop
        splits += 1
    # create new file for each dataframe in list
    for n, frame in enumerate(frames):
        # convert numpy array to dataframe
        rf = pd.DataFrame(frame)
        # create file name as a variable
        scriptfile = f"{savepath}Add OSKU - ({str(n + 1).zfill(len(str(splits)))}-{str(splits)}) - {timestamp}.xlsx"
        # rewrite filepath with correct operating system separators
        scriptfile = os_split_fixer(scriptfile)
        # create a Pandas Excel writer using XlsxWriter as the engine
        writer = pd.ExcelWriter(scriptfile, engine='xlsxwriter')
        # create workbook and worksheet objects
        scriptbook = writer.book
        # write each DataFrame to a specific sheet and reset the index
        rf.to_excel(writer, sheet_name='Script', index=False)
        script = writer.sheets['Script']
        # create formatting methods for workbook
        comma_format = scriptbook.add_format({'num_format': '#,##0', 'align': 'right'})
        nomma_format = scriptbook.add_format({'num_format': '#0', 'align': 'right'})
        nodec_format = scriptbook.add_format({'num_format': '#,##0', 'align': 'right'})
        left_format = scriptbook.add_format({'align': 'left'})
        center_format = scriptbook.add_format({'align': 'center'})
        header_format = scriptbook.add_format({'text_wrap': True, 'align': 'center'})
        header_format.set_bg_color('#CCC0DA')  # purple
        header_format.set_font_color('#FFFFFF')  # white
        header_format.set_align('vcenter')
        header_format.set_bold(True)
        # freeze first row
        script.freeze_panes(1, 0)
        # set row and column formatting on Script worksheet
        script.set_row(0, 75)
        script.write('A1', 'RUN LOG', header_format)
        script.write('B1', 'VALIDATE LOG', header_format)
        script.write('C1', 'Sales Document VBAK-VBELN', header_format)
        script.write('D1', 'Material Number RV45A-PO_MATNR', header_format)
        script.write('E1', 'Cumulative order quantity in sales units RV45A-KWMENG', header_format)
        script.write('F1', 'Plant (Own or External) VBAP-WERKS', header_format)
        script.set_column('A:A', 45, left_format)
        script.set_column('B:B', 14, nomma_format)
        script.set_column('C:D', 14, center_format)
        script.set_column('E:E', 14, comma_format)
        script.set_column('F:F', 14, center_format)
        # save and close workbook
        writer.save()
    # turn chained assignment warning back on
    pd.options.mode.chained_assignment = 'warn'
    return [df.shape[0], splits]


def newso_win_splitter_onefile(file: str, savepath: str, timestamp: str) -> list:
    """
    Create Winshuttle script for adding a new OSKU to a sales order all in one file.

    Conditions
    1. Sales Order does not equal 0
    2. Original Order is 0
    3. Confirmed units is greater than 0
    """
    # turn off warnings
    pd.options.mode.chained_assignment = None
    warnings.filterwarnings('ignore', category=UserWarning)
    warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)
    # read Excel files to dataframes and fill blanks with 0
    df = pd.read_excel(file, sheet_name='Orders', na_values=0)
    # check if dataframe is empty
    if df.empty:
        return [0, 0]
    # reindex dataframe
    df = df[['SO 1', 'Original Order (Units) 1', 'Confirmed (Units) 1',
             'SO 2', 'Original Order (Units) 2', 'Confirmed (Units) 2',
             'Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description']]
    # only keep valid table rows and remove any data entered by a user below the table
    df.dropna(axis=0, how='all', subset=['Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description'], inplace=True)
    # fill blanks with 0
    df = df.fillna(0)
    # combine confirmed units and plant in a string
    df['Combined 1'] = df['Confirmed (Units) 1'].astype(str) + ';' + df['Plant'].astype(str)
    df['Combined 2'] = df['Confirmed (Units) 2'].astype(str) + ';' + df['Plant'].astype(str)
    # create new dataframes from orders dataframe
    wf1 = df[['SO 1', 'OSKU', 'Confirmed (Units) 1', 'Original Order (Units) 1', 'Combined 1']]
    wf2 = df[['SO 2', 'OSKU', 'Confirmed (Units) 2', 'Original Order (Units) 2', 'Combined 2']]
    # rename columns
    wf1 = wf1.rename(columns={'SO 1': 'SO', 'Confirmed (Units) 1': 'Confirmed',
                              'Original Order (Units) 1': 'Original Order', 'Combined 1': 'Combined'})
    wf2 = wf2.rename(columns={'SO 2': 'SO', 'Confirmed (Units) 2': 'Confirmed',
                              'Original Order (Units) 2': 'Original Order', 'Combined 2': 'Combined'})
    # concatenate both dataframes
    wf = pd.concat([wf1, wf2], ignore_index=True)
    # only keep rows that meet above conditions
    wf = wf[(wf['SO'] != 0)]
    wf = wf[(wf['Original Order'] == 0)]
    wf = wf[(wf['Confirmed'] > 0)]
    # check if dataframe is empty
    if wf.empty:
        return [0, 0]
    # convert series back to lists
    salo_series = wf['SO'].tolist()
    osku_series = wf['OSKU'].tolist()
    conf_series = wf['Combined'].tolist()
    # shuffle the sales order dataframe
    df = match_shuffle_sorting(salo_series, osku_series, conf_series, 'Combined').reset_index(drop=True)
    # create blank series for Winshuttle processing
    df['RUN LOG'] = ''
    df['VALIDATE LOG'] = ''
    # reindex dataframe
    df = df[['RUN LOG', 'VALIDATE LOG', 'SO', 'OSKU', 'Combined']]
    # create new series by splitting Combined series
    df['Confirmed (Units)'] = df['Combined'].str.split(';', expand=True)[0]
    df['Plant'] = df['Combined'].str.split(';', expand=True)[1]
    # format series as integers
    df['Confirmed (Units)'] = df['Confirmed (Units)'].astype(float).astype(int)
    df['Plant'] = df['Plant'].astype(float).astype(int)
    # reindex dataframe
    df = df[['RUN LOG', 'VALIDATE LOG', 'SO', 'OSKU', 'Confirmed (Units)', 'Plant']]
    # rename columns
    df = df.rename(columns={'SO': 'Sales Document VBAK-VBELN', 'OSKU': 'Material Number RV45A-PO_MATNR',
                            'Confirmed (Units)': 'Cumulative order quantity in sales units RV45A-KWMENG',
                            'Plant': 'Plant (Own or External) VBAP-WERKS'})
    # create file name as a variable
    scriptfile = f"{savepath}Add OSKU - (1 of 1) - {timestamp}.xlsx"
    # rewrite filepath with correct operating system separators
    scriptfile = os_split_fixer(scriptfile)
    # create a Pandas Excel writer using XlsxWriter as the engine
    writer = pd.ExcelWriter(scriptfile, engine='xlsxwriter')
    # create workbook and worksheet objects
    scriptbook = writer.book
    # write each DataFrame to a specific sheet and reset the index
    df.to_excel(writer, sheet_name='Script', index=False)
    script = writer.sheets['Script']
    # create formatting methods for workbook
    comma_format = scriptbook.add_format({'num_format': '#,##0', 'align': 'right'})
    nomma_format = scriptbook.add_format({'num_format': '#0', 'align': 'right'})
    left_format = scriptbook.add_format({'align': 'left'})
    center_format = scriptbook.add_format({'align': 'center'})
    header_format = scriptbook.add_format({'text_wrap': True, 'align': 'center'})
    header_format.set_bg_color('#CCC0DA')  # purple
    header_format.set_font_color('#FFFFFF')  # white
    header_format.set_align('vcenter')
    header_format.set_bold(True)
    # freeze first row
    script.freeze_panes(1, 0)
    # set row and column formatting on Script worksheet
    script.set_row(0, 75)
    script.write('A1', 'RUN LOG', header_format)
    script.write('B1', 'VALIDATE LOG', header_format)
    script.write('C1', 'Sales Document VBAK-VBELN', header_format)
    script.write('D1', 'Material Number RV45A-PO_MATNR', header_format)
    script.write('E1', 'Cumulative order quantity in sales units RV45A-KWMENG', header_format)
    script.write('F1', 'Plant (Own or External) VBAP-WERKS', header_format)
    script.set_column('A:A', 45, left_format)
    script.set_column('B:B', 14, nomma_format)
    script.set_column('C:D', 14, center_format)
    script.set_column('E:E', 14, comma_format)
    script.set_column('F:F', 14, center_format)
    # save and close workbook
    writer.save()
    # turn chained assignment warning back on
    pd.options.mode.chained_assignment = 'warn'
    return [df.shape[0], 1]


def chng_conf_planner_scriptor():
    """Dynamically create sales order confirmation and rejection Winshuttle script files"""
    global chbox4
    # get checkbox status
    onefile = chbox4.get()
    # check if onefile is checked or not
    if onefile == 1:
        onefile = True
    else:
        onefile = False
    # get full timestamp
    full_timestamp = get_timestamps()[9][:-3]
    # select the source file
    Tk().withdraw()
    file = askopenfilename(title='Select a Confirmations Planner',
                           filetypes=[('Excel files', '.xlsx .xlsb .xlsm .xls')])
    # end function if no file is selected
    if not file:
        ctypes.windll.user32.MessageBoxW(0, 'No file selected.', 'Confirmations Winshuttle', 0)
        return
    # create directory name using week range from file name
    foldername = f"{str(os.path.basename(file).split(' ')[0])} SO CHG Scripts - {full_timestamp}"
    # create new unique directory
    savepath, fpathname = uniq_dir_maker(foldername)
    # end function if no savepath is selected
    if not savepath:
        return
    # start timer
    begin_time = datetime.datetime.now()
    # read in file as dataframe to check for missing sales orders
    df = pd.read_excel(file, sheet_name='Orders')
    # count how many 0s are listed in df series SO 1 or SO 2
    so_zeros = df['SO 1'].isin([0]).sum()
    so_zeros += df['SO 2'].isin([0]).sum()
    # if there are any 0s in the SO 1 or SO 2 columns, send message to ask if user wants to continue or not
    if so_zeros > 0:
        # create pop up message with yes or no option
        answer = ctypes.windll.user32.MessageBoxW(0,
                                                  'There are missing Sales Orders listed in the selected Confirmations Planner that may affect Winshuttle results. If you have already created new Sales Orders, please make sure you enter them in the correct week to ensure they are updated correctly by any further Winshuttle processing. Do you want to continue?',
                                                  'Confirmations Winshuttle', 4)
        # if user selects no, end function
        if answer == 7:
            # deleted newly created directory and any files inside
            shutil.rmtree(fpathname)
            # create message box alert
            ctypes.windll.user32.MessageBoxW(0,
                                             'Winshuttle processing cancelled.',
                                             'Scripts Cancelled', 0)
            return
    # create progress bar window
    progr_bar_win = Tk()
    # set window size
    progr_bar_win.geometry('575x85')
    # set window always on top
    progr_bar_win.attributes('-topmost', True)
    # name progress bar title window
    progr_bar_win.title('Script Creation')
    # create application icon
    mc_icon = resource_path('mc_icon.ico')
    progr_bar_win.iconbitmap(mc_icon)
    # Gets the requested values of the height and width
    windowWidth = progr_bar_win.winfo_reqwidth()
    windowHeight = progr_bar_win.winfo_reqheight()
    # Gets both half the screen width/height and window width/height
    positionRight = int(progr_bar_win.winfo_screenwidth() / 2 - windowWidth / 2)
    positionDown = int(progr_bar_win.winfo_screenheight() / 2 - windowHeight / 2)
    positionRight = int(positionRight - (windowWidth * 1.5)) + 15
    positionDown = int(positionDown - (windowHeight / 2)) + 120
    # Positions the window in the center of the page.
    progr_bar_win.geometry("+{}+{}".format(positionRight, positionDown))
    # create label widget for % completion and string variable for updating
    progr_l_p = Label(progr_bar_win, text='0%', font=('Inconsolata', 11), bg='#FFFFFF')
    progr_l_p.place(x=525, y=40, anchor='ne')
    # create label widget for naming stages and string variable for updating
    progr_l_s = Label(progr_bar_win, text='Initializing...', font=('Inconsolata', 11), bg='#FFFFFF')
    progr_l_s.place(x=35, y=40)
    # create progress bar widget
    progr_bar = Progressbar(progr_bar_win, orient=HORIZONTAL, length=500, mode='determinate')
    progr_bar.place(x=30, y=15)
    # start progress bar and update window
    progr_bar.start()
    progr_bar.update_idletasks()
    time.sleep(.3)
    # increase progress bar and update window
    p_step = random.randint(5, 12)
    progr_bar['value'] = p_step
    progr_l_p.config(text=str(p_step) + '%')
    progr_l_s.config(text='Creating Change Scripts')
    progr_bar.update_idletasks()
    time.sleep(.3)
    # check if selected file is already open
    try:
        # create sales order confirmations script files
        if onefile:
            try:
                l_conf, f_conf = chng_conf_win_splitter_onefile(file, savepath, full_timestamp)
            except (ZeroDivisionError, UnboundLocalError):
                l_conf = 0
        else:
            try:
                l_conf, f_conf = chng_conf_win_splitter(file, savepath, full_timestamp)
            except (ZeroDivisionError, UnboundLocalError):
                l_conf = 0
    except:
        # deleted newly created directory and any files inside
        shutil.rmtree(fpathname)
        # end the progress bar and update window
        progr_bar.stop()
        progr_bar.update_idletasks()
        # kill progress bar and window
        progr_bar.destroy()
        progr_bar_win.destroy()
        # create pop up message warning
        ctypes.windll.user32.MessageBoxW(0,
                                         'An error has occurred. Please check the following suggestions for the selected Confirmations Planner and try again\n\n  -  The file has been saved and closed\n  -  The file is not open or in use by another program\n  -  The worksheet names have not been changed \n\t(Orders, Changes)\n  -  The column names have not been changed \n\t(SO, OSKU, Original_Order (Units), Confirmed (Units))\n  -  The file is saved as in Excel Workbook format \n\t(.xlsx, .xlsb, .xlsm, .xls)',
                                         'Confirmations Winshuttle', 0)
        return
    # increase progress bar and update window
    p_step = random.randint(25, 40)
    progr_bar['value'] = p_step
    progr_l_p.config(text=str(p_step) + '%')
    progr_l_s.config(text='Creating Reject Scripts')
    progr_bar.update_idletasks()
    time.sleep(.3)
    # check if selected file is already open
    try:
        # create sales order rejection script files
        if onefile:
            try:
                l_rejd, f_rejd = chng_rej_win_splitter_onefile(file, savepath, full_timestamp)
            except (ZeroDivisionError, UnboundLocalError):
                l_rejd = 0
        else:
            try:
                l_rejd, f_rejd = chng_rej_win_splitter(file, savepath, full_timestamp)
            except (ZeroDivisionError, UnboundLocalError):
                l_rejd = 0
    except:
        # deleted newly created directory and any files inside
        shutil.rmtree(fpathname)
        # end the progress bar and update window
        progr_bar.stop()
        progr_bar.update_idletasks()
        # kill progress bar and window
        progr_bar.destroy()
        progr_bar_win.destroy()
        # create pop up message warning
        ctypes.windll.user32.MessageBoxW(0,
                                         'An error has occurred. Please check the following suggestions for the selected Confirmations Planner and try again\n\n  -  The file has been saved and closed\n  -  The file is not open or in use by another program\n  -  The worksheet names have not been changed \n\t(Orders, Changes)\n  -  The column names have not been changed \n\t(SO, OSKU, Original_Order (Units), Confirmed (Units))\n  -  The file is saved as in Excel Workbook format \n\t(.xlsx, .xlsb, .xlsm, .xls)',
                                         'Confirmations Winshuttle', 0)
        return
    p_step = random.randint(60, 75)
    progr_bar['value'] = p_step
    progr_l_p.config(text=str(p_step) + '%')
    progr_l_s.config(text='Creating Unreject Scripts')
    progr_bar.update_idletasks()
    time.sleep(.3)
    # create sales order unrejection script files
    try:
        if onefile:
            try:
                u_rejd, n_rejd = chng_unrej_win_splitter_onefile(file, savepath, full_timestamp)
            except (ZeroDivisionError, UnboundLocalError):
                u_rejd = 0
        else:
            try:
                u_rejd, n_rejd = chng_unrej_win_splitter(file, savepath, full_timestamp)
            except (ZeroDivisionError, UnboundLocalError):
                u_rejd = 0
    except:
        # deleted newly created directory and any files inside
        shutil.rmtree(fpathname)
        # end the progress bar and update window
        progr_bar.stop()
        progr_bar.update_idletasks()
        # kill progress bar and window
        progr_bar.destroy()
        progr_bar_win.destroy()
        # create pop up message warning
        ctypes.windll.user32.MessageBoxW(0,
                                         'An error has occurred. Please check the following suggestions for the selected Confirmations Planner and try again\n\n  -  The file has been saved and closed\n  -  The file is not open or in use by another program\n  -  The worksheet names have not been changed \n\t(Orders, Changes)\n  -  The column names have not been changed \n\t(SO, OSKU, Original_Order (Units), Confirmed (Units))\n  -  The file is saved as in Excel Workbook format \n\t(.xlsx, .xlsb, .xlsm, .xls)',
                                         'Confirmations Winshuttle', 0)
        return
    # increase progress bar and update window
    p_step = random.randint(90, 95)
    progr_bar['value'] = p_step
    progr_l_p.config(text=str(p_step) + '%')
    progr_l_s.config(text='Finalizing Winshuttle files')
    progr_bar.update_idletasks()
    time.sleep(.3)
    # increase progress bar and update window
    p_step = 100
    progr_bar['value'] = p_step
    progr_l_p.config(text=str(p_step) + '%')
    progr_bar.update_idletasks()
    time.sleep(.3)
    # end the progress bar and update window
    progr_bar.stop()
    progr_bar.update_idletasks()
    # kill progress bar and window
    progr_bar.destroy()
    progr_bar_win.destroy()
    # check if no files were created
    if l_conf == 0 and l_rejd == 0 and u_rejd == 0:
        # deleted newly created directory and any files inside
        shutil.rmtree(fpathname)
        # create message box alert
        ctypes.windll.user32.MessageBoxW(0, 'There are no planned Sales Order changes listed in the selected Confirmations Planner. No Sales Order scripts have been created.',
                                         'Scripts Completed', 0)
        return
    # if files were created
    else:
        # end timer and format result
        hours, minutes, seconds = str(datetime.datetime.now() - begin_time).split(' ')[-1].split('.')[0].split(':')
        s_time = hours + ':' + minutes + ':' + seconds
        # create message box alert
        ctypes.windll.user32.MessageBoxW(0, 'Your Sales Order change scripts have been created successfully!\n\n' +
                                         f"\tProcessing Time\t\t- " + str(s_time) + '\n' +
                                         f"\tLines Changed\t\t- " + str('{:,}'.format(l_conf)) + '\n' +
                                         f"\tLines Rejected\t\t- " + str('{:,}'.format(l_rejd)) + '\n' +
                                         f"\tLines Added\t\t- " + str('{:,}'.format(u_rejd)),
                                         f"Scripts Completed", 0)
        return


def chng_conf_win_splitter(file: str, savepath: str, timestamp: str) -> list:
    """
    Create Winshuttle scripts for changing OSKU volume on a sales order split automatically into multiple files.

    Conditions
    1. Sales Order does not equal 0
    2. SO Confirmed units is greater 0
    3. Confirmed units is greater than 0
    4. Confirmed units does not equal Original Order
    """
    # turn off warnings
    pd.options.mode.chained_assignment = None
    warnings.filterwarnings('ignore', category=UserWarning)
    warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)
    # read Excel files to dataframes
    df = pd.read_excel(file, sheet_name='Orders')
    # check if dataframe is empty
    if df.empty:
        return [0, 0]
    # reindex dataframe
    df = df[['SO 1', 'SO Confirmed (Units) 1', 'Confirmed (Units) 1',
             'SO 2', 'SO Confirmed (Units) 2', 'Confirmed (Units) 2',
             'Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description']]
    # only keep valid table rows and remove any data entered by a user below the table
    df.dropna(axis=0, how='all', subset=['Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description'], inplace=True)
    # create new dataframes from orders dataframe
    wf1 = df[['SO 1', 'OSKU', 'Confirmed (Units) 1', 'SO Confirmed (Units) 1']]
    wf2 = df[['SO 2', 'OSKU', 'Confirmed (Units) 2', 'SO Confirmed (Units) 2']]
    # rename columns
    wf1 = wf1.rename(columns={'SO 1': 'SO', 'Confirmed (Units) 1': 'Confirmed',
                              'SO Confirmed (Units) 1': 'Original Order'})
    wf2 = wf2.rename(columns={'SO 2': 'SO', 'Confirmed (Units) 2': 'Confirmed',
                              'SO Confirmed (Units) 2': 'Original Order'})
    # concatenate both dataframes
    wf = pd.concat([wf1, wf2], ignore_index=True)
    # remove rows where nothing is listed in the SO Confirmed columns
    wf = wf[wf['Original Order'].notnull()]
    # fill blanks with 0
    wf = wf.fillna(0)
    # only keep rows that meet above conditions
    wf = wf[(wf['SO'] != 0)]
    wf = wf[(wf['Original Order'] > 0)]
    wf = wf[(wf['Confirmed'] > 0)]
    wf = wf[(wf['Confirmed'] != wf['Original Order'])]
    # check if dataframe is empty
    if wf.empty:
        return [0, 0]
    # convert series back to lists
    salo_series = wf['SO'].tolist()
    osku_series = wf['OSKU'].tolist()
    conf_series = wf['Confirmed'].tolist()
    # shuffle the sales order dataframe
    df = match_shuffle_sorting(salo_series, osku_series, conf_series, 'Confirmed (Units)').reset_index(drop=True)
    # dynamically calculate number of dataframe splits
    splits = math.ceil(df.shape[0] / math.ceil(math.sqrt(df.shape[0]) * 8))
    # create original splits and new splits var
    osplits = splits
    # create blank series for Winshuttle processing
    df['RUN LOG'] = ''
    df['VALIDATE LOG'] = ''
    # reindex dataframe
    df = df[['RUN LOG', 'VALIDATE LOG', 'SO', 'OSKU', 'Confirmed (Units)']]
    # rename columns
    df = df.rename(columns={'SO': 'Sales Document VBAK-VBELN', 'OSKU': 'Material Number RV45A-PO_MATNR',
                            'Confirmed (Units)': 'Cumulative order quantity in sales units RV45A-KWMENG'})
    # create loop variables
    valu_flag = 0
    n_loops = 0
    # split dataframe into multiple sub-arrays to create Winshuttle scripts
    while True:
        # iterate loop variable
        n_loops += 1
        try:
            # split dataframe into list of roughly equal sized dataframes
            frames = np.array_split(df, splits)
        except ValueError:
            # decrease splits until array_split stops returning an error
            while valu_flag == 0:
                try:
                    # split dataframe into list of roughly equal sized dataframes
                    frames = np.array_split(df, splits)
                    # update flag and exit while loop
                    valu_flag = 1
                    break
                # if array_split returns error increase splits and retry
                except ValueError:
                    # update number of splits
                    splits = osplits - n_loops
        # end while loop when dataframe has been properly split
        break
    # check if original splits var was adjusted
    if splits != osplits:
        # add one split back to correct from final loop
        splits += 1
    # create new file for each dataframe in list
    for n, frame in enumerate(frames):
        # convert numpy array to dataframe
        rf = pd.DataFrame(frame)
        # create file name as a variable
        scriptfile = f"{savepath}SO CHG - ({str(n + 1).zfill(len(str(splits)))}-{str(splits)}) - {timestamp}.xlsx"
        # rewrite filepath with correct operating system separators
        scriptfile = os_split_fixer(scriptfile)
        # create a Pandas Excel writer using XlsxWriter as the engine
        writer = pd.ExcelWriter(scriptfile, engine='xlsxwriter')
        # create workbook and worksheet objects
        scriptbook = writer.book
        # write each DataFrame to a specific sheet and reset the index
        rf.to_excel(writer, sheet_name='Script', index=False)
        script = writer.sheets['Script']
        # create formatting methods for workbook
        comma_format = scriptbook.add_format({'num_format': '#,##0', 'align': 'right'})
        nomma_format = scriptbook.add_format({'num_format': '#0', 'align': 'right'})
        left_format = scriptbook.add_format({'align': 'left'})
        center_format = scriptbook.add_format({'align': 'center'})
        header_format = scriptbook.add_format({'text_wrap': True, 'align': 'center'})
        header_format.set_bg_color('#9AC4F5')  # Winshuttle blue
        header_format.set_font_color('#FFFFFF')  # white
        header_format.set_align('vcenter')
        header_format.set_bold(True)
        # freeze first row
        script.freeze_panes(1, 0)
        # set row and column formatting on Script worksheet
        script.set_row(0, 75)
        script.write('A1', 'RUN LOG', header_format)
        script.write('B1', 'VALIDATE LOG', header_format)
        script.write('C1', 'Sales Document VBAK-VBELN', header_format)
        script.write('D1', 'Material Number RV45A-PO_MATNR', header_format)
        script.write('E1', 'Cumulative order quantity in sales units RV45A-KWMENG', header_format)
        script.set_column('A:A', 45, left_format)
        script.set_column('B:B', 14, nomma_format)
        script.set_column('C:C', 14, center_format)
        script.set_column('D:D', 14, center_format)
        script.set_column('E:E', 14, comma_format)
        # save and close workbook
        writer.save()
    # turn chained assignment warning back on
    pd.options.mode.chained_assignment = 'warn'
    return [df.shape[0], splits]


def chng_conf_win_splitter_onefile(file: str, savepath: str, timestamp: str) -> list:
    """
    Create Winshuttle scripts for changing OSKU volume on a sales order split automatically into multiple files.

    Conditions
    1. Sales Order does not equal 0
    2. SO Confirmed units is greater 0
    3. Confirmed units is greater than 0
    4. Confirmed units does not equal Original Order
    """
    # turn off warnings
    pd.options.mode.chained_assignment = None
    warnings.filterwarnings('ignore', category=UserWarning)
    warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)
    # read Excel files to dataframes
    df = pd.read_excel(file, sheet_name='Orders')
    # check if dataframe is empty
    if df.empty:
        return [0, 0]
    # reindex dataframe
    df = df[['SO 1', 'SO Confirmed (Units) 1', 'Confirmed (Units) 1',
             'SO 2', 'SO Confirmed (Units) 2', 'Confirmed (Units) 2',
             'Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description']]
    # only keep valid table rows and remove any data entered by a user below the table
    df.dropna(axis=0, how='all', subset=['Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description'], inplace=True)
    # create new dataframes from orders dataframe
    wf1 = df[['SO 1', 'OSKU', 'Confirmed (Units) 1', 'SO Confirmed (Units) 1']]
    wf2 = df[['SO 2', 'OSKU', 'Confirmed (Units) 2', 'SO Confirmed (Units) 2']]
    # rename columns
    wf1 = wf1.rename(columns={'SO 1': 'SO', 'Confirmed (Units) 1': 'Confirmed',
                              'SO Confirmed (Units) 1': 'Original Order'})
    wf2 = wf2.rename(columns={'SO 2': 'SO', 'Confirmed (Units) 2': 'Confirmed',
                              'SO Confirmed (Units) 2': 'Original Order'})
    # concatenate both dataframes
    wf = pd.concat([wf1, wf2], ignore_index=True)
    # remove rows where nothing is listed in the SO Confirmed columns
    wf = wf[wf['Original Order'].notnull()]
    # fill blanks with 0
    wf = wf.fillna(0)
    # only keep rows that meet above conditions
    wf = wf[(wf['SO'] != 0)]
    wf = wf[(wf['Original Order'] > 0)]
    wf = wf[(wf['Confirmed'] > 0)]
    wf = wf[(wf['Confirmed'] != wf['Original Order'])]
    # check if dataframe is empty
    if wf.empty:
        return [0, 0]
    # convert series back to lists
    salo_series = wf['SO'].tolist()
    osku_series = wf['OSKU'].tolist()
    conf_series = wf['Confirmed'].tolist()
    # shuffle the sales order dataframe
    df = match_shuffle_sorting(salo_series, osku_series, conf_series, 'Confirmed (Units)').reset_index(drop=True)
    # create blank series for Winshuttle processing
    df['RUN LOG'] = ''
    df['VALIDATE LOG'] = ''
    # reindex dataframe
    df = df[['RUN LOG', 'VALIDATE LOG', 'SO', 'OSKU', 'Confirmed (Units)']]
    # rename columns
    df = df.rename(columns={'SO': 'Sales Document VBAK-VBELN', 'OSKU': 'Material Number RV45A-PO_MATNR',
                            'Confirmed (Units)': 'Cumulative order quantity in sales units RV45A-KWMENG'})
    # create file name as a variable
    scriptfile = f"{savepath}SO CHG - (1 of 1) - {timestamp}.xlsx"
    # rewrite filepath with correct operating system separators
    scriptfile = os_split_fixer(scriptfile)
    # create a Pandas Excel writer using XlsxWriter as the engine
    writer = pd.ExcelWriter(scriptfile, engine='xlsxwriter')
    # create workbook and worksheet objects
    scriptbook = writer.book
    # write each DataFrame to a specific sheet and reset the index
    df.to_excel(writer, sheet_name='Script', index=False)
    script = writer.sheets['Script']
    # create formatting methods for workbook
    comma_format = scriptbook.add_format({'num_format': '#,##0', 'align': 'right'})
    nomma_format = scriptbook.add_format({'num_format': '#0', 'align': 'right'})
    left_format = scriptbook.add_format({'align': 'left'})
    center_format = scriptbook.add_format({'align': 'center'})
    header_format = scriptbook.add_format({'text_wrap': True, 'align': 'center'})
    header_format.set_bg_color('#9AC4F5')  # Winshuttle blue
    header_format.set_font_color('#FFFFFF')  # white
    header_format.set_align('vcenter')
    header_format.set_bold(True)
    # freeze first row
    script.freeze_panes(1, 0)
    # set row and column formatting on Script worksheet
    script.set_row(0, 75)
    script.write('A1', 'RUN LOG', header_format)
    script.write('B1', 'VALIDATE LOG', header_format)
    script.write('C1', 'Sales Document VBAK-VBELN', header_format)
    script.write('D1', 'Material Number RV45A-PO_MATNR', header_format)
    script.write('E1', 'Cumulative order quantity in sales units RV45A-KWMENG', header_format)
    script.set_column('A:A', 45, left_format)
    script.set_column('B:B', 14, nomma_format)
    script.set_column('C:C', 14, center_format)
    script.set_column('D:D', 14, center_format)
    script.set_column('E:E', 14, comma_format)
    # save and close workbook
    writer.save()
    # turn chained assignment warning back on
    pd.options.mode.chained_assignment = 'warn'
    return [df.shape[0], 1]


def chng_rej_win_splitter(file: str, savepath: str, timestamp: str) -> list:
    """
    Create Winshuttle scripts for rejecting an OSKU on a sales order split automatically into multiple files.

    Conditions
    1. Sales Order does not equal 0
    2. SO Confirmed units is greater 0
    3. Confirmed units is 0
    """
    # turn off warnings
    pd.options.mode.chained_assignment = None
    warnings.filterwarnings('ignore', category=UserWarning)
    warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)
    # read Excel files to dataframes
    df = pd.read_excel(file, sheet_name='Orders')
    # check if dataframe is empty
    if df.empty:
        return [0, 0]
    # reindex dataframe
    df = df[['SO 1', 'SO Confirmed (Units) 1', 'Confirmed (Units) 1',
             'SO 2', 'SO Confirmed (Units) 2', 'Confirmed (Units) 2',
             'Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description']]
    # only keep valid table rows and remove any data entered by a user below the table
    df.dropna(axis=0, how='all', subset=['Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description'], inplace=True)
    # create new dataframes from orders dataframe
    wf1 = df[['SO 1', 'OSKU', 'Confirmed (Units) 1', 'SO Confirmed (Units) 1']]
    wf2 = df[['SO 2', 'OSKU', 'Confirmed (Units) 2', 'SO Confirmed (Units) 2']]
    # rename columns
    wf1 = wf1.rename(
        columns={'SO 1': 'SO', 'Confirmed (Units) 1': 'Confirmed', 'SO Confirmed (Units) 1': 'Original Order'})
    wf2 = wf2.rename(
        columns={'SO 2': 'SO', 'Confirmed (Units) 2': 'Confirmed', 'SO Confirmed (Units) 2': 'Original Order'})
    # concatenate both dataframes
    wf = pd.concat([wf1, wf2], ignore_index=True)
    # remove rows where nothing is listed in the SO Confirmed columns
    wf = wf[wf['Original Order'].notnull()]
    # fill blanks with 0
    wf = wf.fillna(0)
    # only keep rows that meet above conditions
    wf = wf[(wf['SO'] != 0)]
    wf = wf[(wf['Original Order'] > 0)]
    wf = wf[(wf['Confirmed'] <= 0)]
    # check if dataframe is empty
    if wf.empty:
        return [0, 0]
    # convert series back to lists
    salo_series = wf['SO'].tolist()
    osku_series = wf['OSKU'].tolist()
    conf_series = wf['Confirmed'].tolist()
    # shuffle the sales order dataframe
    df = match_shuffle_sorting(salo_series, osku_series, conf_series, 'Confirmed (Units)').reset_index(drop=True)
    # dynamically calculate number of dataframe splits
    splits = math.ceil(df.shape[0] / math.ceil(math.sqrt(df.shape[0]) * 8))
    # create original splits and new splits var
    osplits = splits
    # create series
    df['Rej'] = 'Z1'
    df['RUN LOG'] = ''
    df['VALIDATE LOG'] = ''
    # reindex dataframe
    df = df[['RUN LOG', 'VALIDATE LOG', 'SO', 'OSKU', 'Rej']]
    # rename columns
    df = df.rename(columns={'SO': 'Sales Document VBAK-VBELN', 'OSKU': 'Material Number RV45A-PO_MATNR',
                            'Rej': 'Reason for rejection of quotations and sales orders VBAP-ABGRU'})
    # create loop variables
    valu_flag = 0
    n_loops = 0
    # split dataframe into multiple sub-arrays to create Winshuttle scripts
    while True:
        # iterate loop variable
        n_loops += 1
        try:
            # split dataframe into list of roughly equal sized dataframes
            frames = np.array_split(df, splits)
        except ValueError:
            # decrease splits until array_split stops returning an error
            while valu_flag == 0:
                try:
                    # split dataframe into list of roughly equal sized dataframes
                    frames = np.array_split(df, splits)
                    # update flag and exit while loop
                    valu_flag = 1
                    break
                # if array_split returns error increase splits and retry
                except ValueError:
                    # update number of splits
                    splits = osplits - n_loops
        # end while loop when dataframe has been properly split
        break
    # check if original splits var was adjusted
    if splits != osplits:
        # add one split back to correct from final loop
        splits += 1
    # create new file for each dataframe in list
    for n, frame in enumerate(frames):
        # convert numpy array to dataframe
        rf = pd.DataFrame(frame)
        # create file name as a variable
        scriptfile = f"{savepath}SO REJ - ({str(n + 1).zfill(len(str(splits)))}-{str(splits)}) - {timestamp}.xlsx"
        # rewrite filepath with correct operating system separators
        scriptfile = os_split_fixer(scriptfile)
        # create a Pandas Excel writer using XlsxWriter as the engine
        writer = pd.ExcelWriter(scriptfile, engine='xlsxwriter')
        # create workbook and worksheet objects
        scriptbook = writer.book
        # write each DataFrame to a specific sheet and reset the index
        rf.to_excel(writer, sheet_name='Script', index=False)
        script = writer.sheets['Script']
        # create formatting methods for workbook
        comma_format = scriptbook.add_format({'num_format': '#,##0', 'align': 'right'})
        nomma_format = scriptbook.add_format({'num_format': '#0', 'align': 'right'})
        left_format = scriptbook.add_format({'align': 'left'})
        center_format = scriptbook.add_format({'align': 'center'})
        header_format = scriptbook.add_format({'text_wrap': True, 'align': 'center', })
        header_format.set_bg_color('#DA9694')
        header_format.set_font_color('#FFFFFF')  # white
        header_format.set_align('vcenter')
        header_format.set_bold(True)
        # freeze first row
        script.freeze_panes(1, 0)
        # set row and column formatting on Script worksheet
        script.set_row(0, 75)
        script.write('A1', 'RUN LOG', header_format)
        script.write('B1', 'VALIDATE LOG', header_format)
        script.write('C1', 'Sales Document VBAK-VBELN', header_format)
        script.write('D1', 'Material Number RV45A-PO_MATNR', header_format)
        script.write('E1', 'Reason for rejection of quotations and sales orders VBAP-ABGRU', header_format)
        script.set_column('A:A', 45, left_format)
        script.set_column('B:B', 14, nomma_format)
        script.set_column('C:D', 14, center_format)
        script.set_column('E:E', 14, center_format)
        # save and close workbook
        writer.save()
    # turn chained assignment warning back on
    pd.options.mode.chained_assignment = 'warn'
    return [df.shape[0], splits]


def chng_rej_win_splitter_onefile(file: str, savepath: str, timestamp: str) -> list:
    """
    Create Winshuttle scripts for rejecting an OSKU on a sales order all in one file.

    Conditions
    1. Sales Order does not equal 0
    2. SO Confirmed units is greater 0
    3. Confirmed units is 0
    """
    # turn off warnings
    pd.options.mode.chained_assignment = None
    warnings.filterwarnings('ignore', category=UserWarning)
    warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)
    # read Excel files to dataframes
    df = pd.read_excel(file, sheet_name='Orders')
    # check if dataframe is empty
    if df.empty:
        return [0, 0]
    # reindex dataframe
    df = df[['SO 1', 'SO Confirmed (Units) 1', 'Confirmed (Units) 1',
             'SO 2', 'SO Confirmed (Units) 2', 'Confirmed (Units) 2',
             'Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description']]
    # only keep valid table rows and remove any data entered by a user below the table
    df.dropna(axis=0, how='all', subset=['Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description'], inplace=True)
    # create new dataframes from orders dataframe
    wf1 = df[['SO 1', 'OSKU', 'Confirmed (Units) 1', 'SO Confirmed (Units) 1']]
    wf2 = df[['SO 2', 'OSKU', 'Confirmed (Units) 2', 'SO Confirmed (Units) 2']]
    # rename columns
    wf1 = wf1.rename(
        columns={'SO 1': 'SO', 'Confirmed (Units) 1': 'Confirmed', 'SO Confirmed (Units) 1': 'Original Order'})
    wf2 = wf2.rename(
        columns={'SO 2': 'SO', 'Confirmed (Units) 2': 'Confirmed', 'SO Confirmed (Units) 2': 'Original Order'})
    # concatenate both dataframes
    wf = pd.concat([wf1, wf2], ignore_index=True)
    # remove rows where nothing is listed in the SO Confirmed columns
    wf = wf[wf['Original Order'].notnull()]
    # fill blanks with 0
    wf = wf.fillna(0)
    # only keep rows that meet above conditions
    wf = wf[(wf['SO'] != 0)]
    wf = wf[(wf['Original Order'] > 0)]
    wf = wf[(wf['Confirmed'] <= 0)]
    # check if dataframe is empty
    if wf.empty:
        return [0, 0]
    # convert series back to lists
    salo_series = wf['SO'].tolist()
    osku_series = wf['OSKU'].tolist()
    conf_series = wf['Confirmed'].tolist()
    # shuffle the sales order dataframe
    df = match_shuffle_sorting(salo_series, osku_series, conf_series, 'Confirmed (Units)').reset_index(drop=True)
    # create series
    df['Rej'] = 'Z1'
    df['RUN LOG'] = ''
    df['VALIDATE LOG'] = ''
    # reindex dataframe
    df = df[['RUN LOG', 'VALIDATE LOG', 'SO', 'OSKU', 'Rej']]
    # rename columns
    df = df.rename(columns={'SO': 'Sales Document VBAK-VBELN', 'OSKU': 'Material Number RV45A-PO_MATNR',
                            'Rej': 'Reason for rejection of quotations and sales ordersVBAP-ABGRU'})
    # create file name as a variable
    scriptfile = f"{savepath}SO REJ - (1 of 1) - {timestamp}.xlsx"
    # rewrite filepath with correct operating system separators
    scriptfile = os_split_fixer(scriptfile)
    # create a Pandas Excel writer using XlsxWriter as the engine
    writer = pd.ExcelWriter(scriptfile, engine='xlsxwriter')
    # create workbook and worksheet objects
    scriptbook = writer.book
    # write each DataFrame to a specific sheet and reset the index
    df.to_excel(writer, sheet_name='Script', index=False)
    script = writer.sheets['Script']
    # create formatting methods for workbook
    comma_format = scriptbook.add_format({'num_format': '#,##0', 'align': 'right'})
    nomma_format = scriptbook.add_format({'num_format': '#0', 'align': 'right'})
    left_format = scriptbook.add_format({'align': 'left'})
    center_format = scriptbook.add_format({'align': 'center'})
    header_format = scriptbook.add_format({'text_wrap': True, 'align': 'center', })
    header_format.set_bg_color('#DA9694')
    header_format.set_font_color('#FFFFFF')  # white
    header_format.set_align('vcenter')
    header_format.set_bold(True)
    # freeze first row
    script.freeze_panes(1, 0)
    # set row and column formatting on Script worksheet
    script.set_row(0, 75)
    script.write('A1', 'RUN LOG', header_format)
    script.write('B1', 'VALIDATE LOG', header_format)
    script.write('C1', 'Sales Document VBAK-VBELN', header_format)
    script.write('D1', 'Material Number RV45A-PO_MATNR', header_format)
    script.write('E1', 'Reason for rejection of quotations and sales orders VBAP-ABGRU', header_format)
    script.set_column('A:A', 45, left_format)
    script.set_column('B:B', 14, nomma_format)
    script.set_column('C:D', 14, center_format)
    script.set_column('E:E', 14, comma_format)
    # save and close workbook
    writer.save()
    # turn chained assignment warning back on
    pd.options.mode.chained_assignment = 'warn'
    return [df.shape[0], 1]


def chng_unrej_win_splitter(file: str, savepath: str, timestamp: str) -> list:
    """
    Create Winshuttle scripts for unrejecting and changing OSKU volume on a sales order split automatically into multiple files.

    Conditions
    1. Sales Order does not equal 0
    2. SO Confirmed units is 0
    3. Confirmed units is greater than 0
    """
    # turn off warnings
    pd.options.mode.chained_assignment = None
    warnings.filterwarnings('ignore', category=UserWarning)
    warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)
    # read Excel files to dataframes
    df = pd.read_excel(file, sheet_name='Orders')
    # check if dataframe is empty
    if df.empty:
        return [0, 0]
    # reindex dataframe
    df = df[['SO 1', 'SO Confirmed (Units) 1', 'Confirmed (Units) 1',
             'SO 2', 'SO Confirmed (Units) 2', 'Confirmed (Units) 2',
             'Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description']]
    # only keep valid table rows and remove any data entered by a user below the table
    df.dropna(axis=0, how='all', subset=['Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description'], inplace=True)
    # create new dataframes from orders dataframe
    wf1 = df[['SO 1', 'OSKU', 'Confirmed (Units) 1', 'SO Confirmed (Units) 1']]
    wf2 = df[['SO 2', 'OSKU', 'Confirmed (Units) 2', 'SO Confirmed (Units) 2']]
    # rename columns
    wf1 = wf1.rename(columns={'SO 1': 'SO', 'Confirmed (Units) 1': 'Confirmed',
                              'SO Confirmed (Units) 1': 'Original Order'})
    wf2 = wf2.rename(columns={'SO 2': 'SO', 'Confirmed (Units) 2': 'Confirmed',
                              'SO Confirmed (Units) 2': 'Original Order'})
    # concatenate both dataframes
    wf = pd.concat([wf1, wf2], ignore_index=True)
    # remove rows where nothing is listed in the SO Confirmed columns
    wf = wf[wf['Original Order'].notnull()]
    # fill blanks with 0
    wf = wf.fillna(0)
    # only keep rows that meet above conditions
    wf = wf[(wf['SO'] != 0)]
    wf = wf[(wf['Original Order'] <= 0)]
    wf = wf[(wf['Confirmed'] > 0)]
    # check if dataframe is empty
    if wf.empty:
        return [0, 0]
    # convert series back to lists
    salo_series = wf['SO'].tolist()
    osku_series = wf['OSKU'].tolist()
    conf_series = wf['Confirmed'].tolist()
    # shuffle the sales order dataframe
    df = match_shuffle_sorting(salo_series, osku_series, conf_series, 'New Confirmed (Units)').reset_index(drop=True)
    # copy dataframe to new dataframe
    cf = df.copy(deep=True)
    # dynamically calculate number of dataframe splits
    splits = math.ceil(df.shape[0] / math.ceil(math.sqrt(df.shape[0]) * 8))
    # create original splits and new splits var
    osplits = splits
    # create series for Winshuttle that leaves reason for rejection blank
    df['Rej'] = ''
    # reindex dataframe
    df = df[['SO', 'OSKU', 'Rej']]
    cf = cf[['SO', 'OSKU', 'New Confirmed (Units)']]
    # get series as list for shuffling
    salo_series1 = df['SO'].tolist()
    osku_series1 = df['OSKU'].tolist()
    rejc_series1 = df['Rej'].tolist()
    salo_series2 = cf['SO'].tolist()
    osku_series2 = cf['OSKU'].tolist()
    conf_series2 = cf['New Confirmed (Units)'].tolist()
    # shuffle the sales order dataframe
    df = match_shuffle_sorting(salo_series1, osku_series1, rejc_series1, 'Rej').reset_index(drop=True)
    cf = match_shuffle_sorting(salo_series2, osku_series2, conf_series2, 'New Confirmed (Units)').reset_index(drop=True)
    # create new series
    df['RUN LOG'] = ''
    cf['RUN LOG'] = ''
    df['VALIDATE LOG'] = ''
    cf['VALIDATE LOG'] = ''
    # reindex dataframe
    df = df[['RUN LOG', 'VALIDATE LOG', 'SO', 'OSKU', 'Rej']]
    cf = cf[['RUN LOG', 'VALIDATE LOG', 'SO', 'OSKU', 'New Confirmed (Units)']]
    # rename columns
    df = df.rename(columns={'SO': 'Sales Document VBAK-VBELN', 'OSKU': 'Material Number RV45A-PO_MATNR',
                            'Rej': 'Reason for rejection of quotations and sales orders VBAP-ABGRU'})
    cf = cf.rename(columns={'SO': 'Sales Document VBAK-VBELN', 'OSKU': 'Material Number RV45A-PO_MATNR',
                            'New Confirmed (Units)': 'Cumulative order quantity in sales unitsRV45A-KWMENG'})
    # create loop variables
    valu_flag = 0
    n_loops = 0
    # split dataframe into multiple sub-arrays to create Winshuttle scripts
    while True:
        # iterate loop variable
        n_loops += 1
        try:
            # split dataframe into list of roughly equal sized dataframes
            frames1 = np.array_split(df, splits)
            frames2 = np.array_split(cf, splits)
        except ValueError:
            # decrease splits until array_split stops returning an error
            while valu_flag == 0:
                try:
                    # split dataframe into list of roughly equal sized dataframes
                    frames1 = np.array_split(df, splits)
                    frames2 = np.array_split(cf, splits)
                    # update flag and exit while loop
                    valu_flag = 1
                    break
                # if array_split returns error increase splits and retry
                except ValueError:
                    # update number of splits
                    splits = osplits - n_loops
        break
    # check if original splits var was adjusted
    if splits != osplits:
        # add one split back to correct from final loop
        splits += 1
    # create new file for each dataframe in list
    for n, frame in enumerate(frames1):
        # convert numpy array to dataframe
        rf1 = pd.DataFrame(frames1[n])
        rf2 = pd.DataFrame(frames2[n])
        # create file name as a variable
        scriptfile = f"{savepath}SO UNREJ - ({str(n + 1).zfill(len(str(splits)))}-{str(splits)}) - {timestamp}.xlsx"
        # rewrite filepath with correct operating system separators
        scriptfile = os_split_fixer(scriptfile)
        # create a Pandas Excel writer using XlsxWriter as the engine
        writer = pd.ExcelWriter(scriptfile, engine='xlsxwriter')
        # create workbook and worksheet objects
        scriptbook = writer.book
        # write each DataFrame to a specific sheet and reset the index
        rf1.to_excel(writer, sheet_name='Unreject', index=False)
        rf2.to_excel(writer, sheet_name='Change', index=False)
        unrejc = writer.sheets['Unreject']
        change = writer.sheets['Change']
        # create formatting methods for workbook
        comma_format = scriptbook.add_format({'num_format': '#,##0', 'align': 'right'})
        nomma_format = scriptbook.add_format({'num_format': '#0', 'align': 'right'})
        left_format = scriptbook.add_format({'align': 'left'})
        header_format = scriptbook.add_format({'text_wrap': True, 'align': 'center', })
        header_format.set_bg_color('#DA9694')
        header_format.set_font_color('#FFFFFF')  # white
        header_format.set_align('vcenter')
        header_format.set_bold(True)
        # freeze first row
        unrejc.freeze_panes(1, 0)
        # set row and column formatting on Script worksheet
        unrejc.set_row(0, 75)
        unrejc.write('A1', 'RUN LOG', header_format)
        unrejc.write('B1', 'VALIDATE LOG', header_format)
        unrejc.write('C1', 'Sales Document VBAK-VBELN', header_format)
        unrejc.write('D1', 'Material Number RV45A-PO_MATNR', header_format)
        unrejc.write('E1', 'Reason for rejection of quotations and sales orders VBAP-ABGRU', header_format)
        unrejc.set_column('A:A', 45, left_format)
        unrejc.set_column('B:B', 14, nomma_format)
        unrejc.set_column('C:C', 14, nomma_format)
        unrejc.set_column('D:D', 14, nomma_format)
        unrejc.set_column('E:E', 14, comma_format)
        # create formatting methods for workbook
        comma_format = scriptbook.add_format({'num_format': '#,##0', 'align': 'right'})
        nomma_format = scriptbook.add_format({'num_format': '#0', 'align': 'right'})
        left_format = scriptbook.add_format({'align': 'left'})
        header_format = scriptbook.add_format({'text_wrap': True, 'align': 'center'})
        header_format.set_bg_color('#9AC4F5')  # Winshuttle blue
        header_format.set_font_color('#FFFFFF')  # white
        header_format.set_align('vcenter')
        header_format.set_bold(True)
        # freeze first row
        change.freeze_panes(1, 0)
        # set row and column formatting on Script worksheet
        change.set_row(0, 75)
        change.write('A1', 'RUN LOG', header_format)
        change.write('B1', 'VALIDATE LOG', header_format)
        change.write('C1', 'Sales Document VBAK-VBELN', header_format)
        change.write('D1', 'Material Number RV45A-PO_MATNR', header_format)
        change.write('E1', 'Cumulative order quantity in sales units RV45A-KWMENG', header_format)
        change.set_column('A:A', 45, left_format)
        change.set_column('B:B', 14, nomma_format)
        change.set_column('C:C', 14, nomma_format)
        change.set_column('D:D', 14, nomma_format)
        change.set_column('E:E', 14, comma_format)
        # save and close workbook
        writer.save()
    return [df.shape[0], splits]


def chng_unrej_win_splitter_onefile(file: str, savepath: str, timestamp: str) -> list:
    """
    Create Winshuttle scripts for unrejecting and changing OSKU volume on a sales order all in one file.

    Conditions
    1. Sales Order does not equal 0
    2. SO Confirmed units is 0
    3. Confirmed units is greater than 0
    """
    # turn off warnings
    pd.options.mode.chained_assignment = None
    warnings.filterwarnings('ignore', category=UserWarning)
    warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)
    # read Excel files to dataframes
    df = pd.read_excel(file, sheet_name='Orders')
    # check if dataframe is empty
    if df.empty:
        return [0, 0]
    # reindex dataframe
    df = df[['SO 1', 'SO Confirmed (Units) 1', 'Confirmed (Units) 1',
             'SO 2', 'SO Confirmed (Units) 2', 'Confirmed (Units) 2',
             'Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description']]
    # only keep valid table rows and remove any data entered by a user below the table
    df.dropna(axis=0, how='all', subset=['Pair', 'Plant', 'ShipTo', 'OSKU', 'Brand', 'Description'], inplace=True)
    # create new dataframes from orders dataframe
    wf1 = df[['SO 1', 'OSKU', 'Confirmed (Units) 1', 'SO Confirmed (Units) 1']]
    wf2 = df[['SO 2', 'OSKU', 'Confirmed (Units) 2', 'SO Confirmed (Units) 2']]
    # rename columns
    wf1 = wf1.rename(columns={'SO 1': 'SO', 'Confirmed (Units) 1': 'Confirmed',
                              'SO Confirmed (Units) 1': 'Original Order'})
    wf2 = wf2.rename(columns={'SO 2': 'SO', 'Confirmed (Units) 2': 'Confirmed',
                              'SO Confirmed (Units) 2': 'Original Order'})
    # concatenate both dataframes
    wf = pd.concat([wf1, wf2], ignore_index=True)
    # remove rows where nothing is listed in the SO Confirmed columns
    wf = wf[wf['Original Order'].notnull()]
    # fill blanks with 0
    wf = wf.fillna(0)
    # only keep rows that meet above conditions
    wf = wf[(wf['SO'] != 0)]
    wf = wf[(wf['Original Order'] <= 0)]
    wf = wf[(wf['Confirmed'] > 0)]
    # check if dataframe is empty
    if wf.empty:
        return [0, 0]
    # convert series back to lists
    salo_series = wf['SO'].tolist()
    osku_series = wf['OSKU'].tolist()
    conf_series = wf['Confirmed'].tolist()
    # shuffle the sales order dataframe
    df = match_shuffle_sorting(salo_series, osku_series, conf_series, 'New Confirmed (Units)').reset_index(drop=True)
    # copy dataframe to new dataframe
    cf = df.copy(deep=True)
    # create series for Winshuttle that leaves reason for rejection blank
    df['Rej'] = ''
    # reindex dataframe
    df = df[['SO', 'OSKU', 'Rej']]
    cf = cf[['SO', 'OSKU', 'New Confirmed (Units)']]
    # get series as list for shuffling
    salo_series1 = df['SO'].tolist()
    osku_series1 = df['OSKU'].tolist()
    rejc_series1 = df['Rej'].tolist()
    salo_series2 = cf['SO'].tolist()
    osku_series2 = cf['OSKU'].tolist()
    conf_series2 = cf['New Confirmed (Units)'].tolist()
    # shuffle the sales order dataframe
    df = match_shuffle_sorting(salo_series1, osku_series1, rejc_series1, 'Rej').reset_index(drop=True)
    cf = match_shuffle_sorting(salo_series2, osku_series2, conf_series2, 'New Confirmed (Units)').reset_index(drop=True)
    # create new series
    df['RUN LOG'] = ''
    cf['RUN LOG'] = ''
    df['VALIDATE LOG'] = ''
    cf['VALIDATE LOG'] = ''
    # reindex dataframe
    df = df[['RUN LOG', 'VALIDATE LOG', 'SO', 'OSKU', 'Rej']]
    cf = cf[['RUN LOG', 'VALIDATE LOG', 'SO', 'OSKU', 'New Confirmed (Units)']]
    # rename columns
    df = df.rename(columns={'SO': 'Sales Document VBAK-VBELN', 'OSKU': 'Material Number RV45A-PO_MATNR',
                            'Rej': 'Reason for rejection of quotations and sales orders VBAP-ABGRU'})
    cf = cf.rename(columns={'SO': 'Sales Document VBAK-VBELN', 'OSKU': 'Material Number RV45A-PO_MATNR',
                            'New Confirmed (Units)': 'Cumulative order quantity in sales unitsRV45A-KWMENG'})
    # create file name as a variable
    scriptfile = f"{savepath}SO UNREJ - (1 of 1) - {timestamp}.xlsx"
    # rewrite filepath with correct operating system separators
    scriptfile = os_split_fixer(scriptfile)
    # create a Pandas Excel writer using XlsxWriter as the engine
    writer = pd.ExcelWriter(scriptfile, engine='xlsxwriter')
    # create workbook and worksheet objects
    scriptbook = writer.book
    # write each DataFrame to a specific sheet and reset the index
    df.to_excel(writer, sheet_name='Unreject', index=False)
    cf.to_excel(writer, sheet_name='Change', index=False)
    unrejc = writer.sheets['Unreject']
    change = writer.sheets['Change']
    # create formatting methods for workbook
    comma_format = scriptbook.add_format({'num_format': '#,##0', 'align': 'right'})
    nomma_format = scriptbook.add_format({'num_format': '#0', 'align': 'right'})
    left_format = scriptbook.add_format({'align': 'left'})
    header_format = scriptbook.add_format({'text_wrap': True, 'align': 'center', })
    header_format.set_bg_color('#DA9694')
    header_format.set_font_color('#FFFFFF')  # white
    header_format.set_align('vcenter')
    header_format.set_bold(True)
    # freeze first row
    unrejc.freeze_panes(1, 0)
    # set row and column formatting on Script worksheet
    unrejc.set_row(0, 75)
    unrejc.write('A1', 'RUN LOG', header_format)
    unrejc.write('B1', 'VALIDATE LOG', header_format)
    unrejc.write('C1', 'Sales Document VBAK-VBELN', header_format)
    unrejc.write('D1', 'Material Number RV45A-PO_MATNR', header_format)
    unrejc.write('E1', 'Reason for rejection of quotations and sales orders VBAP-ABGRU', header_format)
    unrejc.set_column('A:A', 45, left_format)
    unrejc.set_column('B:B', 14, nomma_format)
    unrejc.set_column('C:C', 14, nomma_format)
    unrejc.set_column('D:D', 14, nomma_format)
    unrejc.set_column('E:E', 14, comma_format)
    # create formatting methods for workbook
    comma_format = scriptbook.add_format({'num_format': '#,##0', 'align': 'right'})
    nomma_format = scriptbook.add_format({'num_format': '#0', 'align': 'right'})
    left_format = scriptbook.add_format({'align': 'left'})
    header_format = scriptbook.add_format({'text_wrap': True, 'align': 'center'})
    header_format.set_bg_color('#9AC4F5')  # Winshuttle blue
    header_format.set_font_color('#FFFFFF')  # white
    header_format.set_align('vcenter')
    header_format.set_bold(True)
    # freeze first row
    change.freeze_panes(1, 0)
    # set row and column formatting on Script worksheet
    change.set_row(0, 75)
    change.write('A1', 'RUN LOG', header_format)
    change.write('B1', 'VALIDATE LOG', header_format)
    change.write('C1', 'Sales Document VBAK-VBELN', header_format)
    change.write('D1', 'Material Number RV45A-PO_MATNR', header_format)
    change.write('E1', 'Cumulative order quantity in sales units RV45A-KWMENG', header_format)
    change.set_column('A:A', 45, left_format)
    change.set_column('B:B', 14, nomma_format)
    change.set_column('C:C', 14, nomma_format)
    change.set_column('D:D', 14, nomma_format)
    change.set_column('E:E', 14, comma_format)
    # save and close workbook
    writer.save()
    return [df.shape[0], 1]


def generic_scriptor():
    """Dynamically create Winshuttle script files from any Excel file"""
    global chbox2
    global entry1
    # get checkbox status
    specify_number = chbox2.get()
    # check if onefile is checked or not
    if specify_number == 1:
        file_number = int(entry1.get())
    else:
        file_number = 0
        # select the source file
    Tk().withdraw()
    file = askopenfilename(title='Select an Excel file to split',
                           filetypes=[('Excel files', '.xlsx .xlsb .xlsm .xls')])
    # end function if no file is selected
    if not file:
        ctypes.windll.user32.MessageBoxW(0, 'No file selected.', 'Winshuttle Splitter', 0)
        return
    # get file basename
    filename = os.path.splitext(os.path.basename(file))[0]
    # create directory name
    foldername = f"{str(filename)} Splits - {str(get_timestamps()[7])}"
    # create new unique directory
    savepath, fpathname = uniq_dir_maker(foldername)
    if not savepath:
        return
    # start timer
    begin_time = datetime.datetime.now()
    # create progress bar window
    progr_bar_win = Tk()
    # set window size
    progr_bar_win.geometry('575x85')
    # set window always on top
    progr_bar_win.attributes('-topmost', True)
    # name progress bar title window
    progr_bar_win.title('Script Creation')
    # create application icon
    mc_icon = resource_path('mc_icon.ico')
    progr_bar_win.iconbitmap(mc_icon)
    # Gets the requested values of the height and width
    windowWidth = progr_bar_win.winfo_reqwidth()
    windowHeight = progr_bar_win.winfo_reqheight()
    # Gets both half the screen width/height and window width/height
    positionRight = int(progr_bar_win.winfo_screenwidth() / 2 - windowWidth / 2)
    positionDown = int(progr_bar_win.winfo_screenheight() / 2 - windowHeight / 2)
    positionRight = int(positionRight - (windowWidth * 1.5)) + 15
    positionDown = int(positionDown - (windowHeight / 2)) + 120
    # Positions the window in the center of the page.
    progr_bar_win.geometry("+{}+{}".format(positionRight, positionDown))
    # create label widget for % completion and string variable for updating
    progr_l_p = Label(progr_bar_win, text='0%', font=('Inconsolata', 11), bg='#FFFFFF')
    progr_l_p.place(x=525, y=40, anchor='ne')
    # create label widget for naming stages and string variable for updating
    progr_l_s = Label(progr_bar_win, text='Initializing...', font=('Inconsolata', 11), bg='#FFFFFF')
    progr_l_s.place(x=35, y=40)
    # create progress bar widget
    progr_bar = Progressbar(progr_bar_win, orient=HORIZONTAL, length=500, mode='determinate')
    progr_bar.place(x=30, y=15)
    # start progress bar and update window
    progr_bar.start()
    progr_bar.update_idletasks()
    time.sleep(.3)
    # increase progress bar and update window
    p_step = random.randint(5, 12)
    progr_bar['value'] = p_step
    progr_l_p.config(text=str(p_step) + '%')
    progr_l_s.config(text='Creating new files')
    progr_bar.update_idletasks()
    time.sleep(.3)
    # check
    try:
        # create new Winshuttle script files
        generic_win_splitter(file, savepath, filename, file_number)
    except:
        # deleted newly created directory and any files inside
        shutil.rmtree(fpathname)
        # end the progress bar and update window
        progr_bar.stop()
        progr_bar.update_idletasks()
        # kill progress bar and window
        progr_bar.destroy()
        progr_bar_win.destroy()
        # create pop up message warning
        ctypes.windll.user32.MessageBoxW(0,
                                         'An error has occurred. Please check the following suggestions and try again.\n\n  -  The file has been saved and closed\n  -  The file is not open or in use by another program',
                                         'Winshuttle Script Creator', 0)
        return
    # increase progress bar and update window
    p_step = random.randint(90, 95)
    progr_bar['value'] = p_step
    progr_l_p.config(text=str(p_step) + '%')
    progr_l_s.config(text='Finalizing Winshuttle files')
    progr_bar.update_idletasks()
    time.sleep(.3)
    # increase progress bar and update window
    p_step = 100
    progr_bar['value'] = p_step
    progr_l_p.config(text=str(p_step) + '%')
    progr_bar.update_idletasks()
    time.sleep(.3)
    # end the progress bar and update window
    progr_bar.stop()
    progr_bar.update_idletasks()
    # kill progress bar and window
    progr_bar.destroy()
    progr_bar_win.destroy()
    # end timer and format result
    hours, minutes, seconds = str(datetime.datetime.now() - begin_time).split(' ')[-1].split('.')[0].split(':')
    s_time = hours + ':' + minutes + ':' + seconds
    # create message box alert
    ctypes.windll.user32.MessageBoxW(0, 'Your Winshuttle scripts have been created successfully!\n\n' +
                                     '            Processing Time                - ' + str(s_time),
                                     '            Scripts Completed', 0)
    return


def generic_win_splitter(file: str, savepath: str, basename: str, num_files: int):
    """Split the file into multiple files"""
    # turn off warnings
    warnings.filterwarnings('ignore', category=UserWarning)
    warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)
    # read Excel files to dataframes and fill blanks with 0
    df = pd.read_excel(file)
    # count number of rows in dataframe
    dfrows = df.shape[0]
    # if number of files is not specified or is 0
    if num_files == 0:
        # dynamically calculate number of dataframe splits
        splits = math.ceil(dfrows / math.ceil(math.sqrt(dfrows) * 8))
    else:
        splits = num_files
    # create original splits and new splits var
    osplits = splits
    # create loop variables
    loop_flap = 0
    valu_flag = 0
    n_loops = 0
    # split dataframe into multiple sub-arrays to create Winshuttle scripts
    while loop_flap == 0:
        # iterate loop variable
        n_loops += 1
        try:
            # split dataframe into list of roughly equal sized dataframes
            frames = np.array_split(df, splits)
        except ValueError:
            # decrease splits until array_split stops returning an error
            while valu_flag == 0:
                try:
                    # split dataframe into list of roughly equal sized dataframes
                    frames = np.array_split(df, splits)
                    # update flag and exit while loop
                    valu_flag = 1
                    break
                # if array_split returns error increase splits and retry
                except ValueError:
                    # update number of splits
                    splits = osplits - n_loops
        # end while loop when dataframe has been properly split
        loop_flap = 1
        break
    # check if original splits var was adjusted
    if splits != osplits:
        # add one split back to correct from final loop
        splits += 1
    # create iterable integer variable
    n = 1
    # create new file for each dataframe in list
    for frame in frames:
        # convert numpy array to dataframe
        rf = pd.DataFrame(frame)
        # create file name as a variable
        scriptfile = savepath + ' ' + basename + ' - (' + \
                     str(n).zfill(len(str(splits))) + '-' + str(splits) + ').xlsx'
        # rewrite filepath with correct operating system separators
        scriptfile = os_split_fixer(scriptfile)
        # create a Pandas Excel writer using XlsxWriter as the engine
        writer = pd.ExcelWriter(scriptfile, engine='xlsxwriter')
        # create workbook and worksheet objects
        scriptbook = writer.book
        # write each DataFrame to a specific sheet and reset the index
        rf.to_excel(writer, sheet_name='Script', index=False)
        script = writer.sheets['Script']
        # create formatting methods for workbook
        header_format = scriptbook.add_format({'text_wrap': True, 'align': 'center', })
        # header_format.set_bg_color('#9AC4F5')
        header_format.set_align('vcenter')
        header_format.set_bold(True)
        # create table formatting for each worksheet
        format_excel(writer, rf, 'Script', 'SCRIPT', 'Table Style Medium 2', True)
        # freeze first row
        script.freeze_panes(1, 0)
        # hide gridlines
        script.hide_gridlines(2)
        # set row formatting on Script worksheet
        script.set_row(0, 75, header_format)
        # save and close workbook
        writer.save()
        # increase iterating variable
        n += 1
    return dfrows, splits


def winshuttle_menu():
    # declare menu variables
    global wroot
    lfht = 215
    lfwt = 900
    # ----- create widget variables
    bht = 1  # button height
    bwt = 6  # button width
    # ----- create root window
    wroot = tk.Tk()
    # set as active window
    wroot.focus_force()
    # Gets the requested values of the height and width
    windowWidth = wroot.winfo_reqwidth()
    windowHeight = wroot.winfo_reqheight()
    # Gets both half the screen width/height and window width/height
    positionRight = int(wroot.winfo_screenwidth() / 2 - windowWidth / 2)
    positionDown = int(wroot.winfo_screenheight() / 2 - windowHeight / 2)
    positionRight = int(positionRight - (windowWidth * 1.5))
    positionDown = int(positionDown - (windowHeight / 2))
    # Positions the window in the center of the page.
    wroot.geometry("+{}+{}".format(positionRight, positionDown))
    # name window
    wroot.title(jars_apptitle)
    mc_icon = resource_path('mc_icon.ico')
    wroot.iconbitmap(mc_icon)
    # create gui canvas
    canvas = tk.Canvas(wroot, height=lfht, width=lfwt)
    canvas.pack()
    # ----- Design GUI
    frame = tk.Frame(wroot, bg='#D8E0E4')
    frame.place(relx=0, rely=0, relwidth=1, relheight=1)
    fleft = Frame(wroot, bd=0, height=130, width=280, highlightbackground='#091F3F',
                  highlightcolor='#D8E0E4', highlightthickness=1)
    fleft.place(x=15, y=75)
    fright = Frame(wroot, bd=0, height=130, width=280, highlightbackground='#091F3F',
                   highlightcolor='#D8E0E4', highlightthickness=1)
    fright.place(x=310, y=75)
    fright2 = Frame(wroot, bd=0, height=130, width=280, highlightbackground='#091F3F',
                    highlightcolor='#D8E0E4', highlightthickness=1)
    fright2.place(x=605, y=75)
    # ----- Main Window frames and widgets
    tcenter = (lfwt / 2) + 40
    lflabel1 = Label(wroot, text='Winshuttle Utilities', fg='#091F3F', bg='#D8E0E4',
                     font=('Tuno', 32))
    lflabel1.place(x=tcenter, y=8, anchor='n')
    # create back button
    optbtn2 = Button(wroot, text='Back', height=bht, width=bwt, command=winshuttle_back,
                     font=('Century_Gothic', 12), fg='#091F3F', bg='#D8E0E4')
    optbtn2.place(x=30, y=30)
    # create confirmations script button
    global optbtn3
    optbtn3 = Button(wroot, text='Confirmations Scripts', height=bht, width=22, command=conf_planner_scriptor,
                     font=('Century_Gothic', 12), fg='#091F3F', bg='#D8E0E4')
    optbtn3.place(x=50, y=160)
    # create confirmation script label description
    optlbl3 = Label(wroot, text='Create Winshuttle scripts from a Confirmations Planner',
                    fg='#091F3F', font=('Century_Gothic', 14), wraplength=250)
    optlbl3.place(x=29, y=80, anchor='nw')
    # create checkbox for confirmation scripts one file
    global chbox1
    chbox1 = IntVar()
    check1 = Checkbutton(wroot, text='Create one file per script', variable=chbox1,
                         font=('Century_Gothic', 11), onvalue=1, offvalue=0)
    check1.place(x=47, y=130)

    # create confirmations changes button
    global optbtn4
    optbtn4 = Button(wroot, text='Confirmations Changes', height=bht, width=22, command=chng_conf_planner_scriptor, font=('Century_Gothic', 12), fg='#091F3F', bg='#D8E0E4')
    optbtn4.place(x=345, y=160)
    # create confirmation script label description
    optlbl4 = Label(wroot, text='Create Winshuttle scripts from a Confirmations Planner',
                    fg='#091F3F', font=('Century_Gothic', 14), wraplength=250)
    optlbl4.place(x=324, y=80, anchor='nw')
    # create checkbox for confirmation scripts one file
    global chbox4
    chbox4 = IntVar()
    check4 = Checkbutton(wroot, text='Create one file per script', variable=chbox4,
                         font=('Century_Gothic', 11), onvalue=1, offvalue=0)
    check4.place(x=342, y=130)

    # create file splitter button
    global optbtn1
    optbtn1 = Button(wroot, text='File Splitter', height=bht, width=22, command=generic_scriptor,
                     font=('Century_Gothic', 12), fg='#091F3F', bg='#D8E0E4')
    optbtn1.place(x=640, y=160)
    # create file splitter label description
    optlbl3 = Label(wroot, text='Split an Excel file into multiple even sized files',
                    fg='#091F3F', font=('Century_Gothic', 14), wraplength=250)
    optlbl3.place(x=635, y=80, anchor='nw')
    # create checkbox for number of files to split
    global chbox2
    chbox2 = IntVar()
    check2 = Checkbutton(wroot, text='Create number of files', variable=chbox2, command=win_number_enabler,
                         font=('Century_Gothic', 11), onvalue=1, offvalue=0)
    check2.place(x=647, y=130)
    # create entry box for number of splits for a file
    global enbox1
    enbox1 = IntVar(wroot, value=2)
    global entry1
    entry1 = Entry(wroot, textvariable=enbox1, width=3, state='disabled', justify=RIGHT, font=('Century_Gothic', 11))
    entry1.place(x=857, y=131, anchor='ne')
    # ----- create window loop
    wroot.mainloop()
    return


def win_number_enabler():
    """Enable or disable the entry box by clicking a checkbox"""
    if chbox2.get() == 1:
        entry1.configure(state='normal')
        wroot.update_idletasks()
    else:
        entry1.configure(state='disabled')
        wroot.update_idletasks()
    return


def open_winshuttle_menu():
    """From main menu open the UPD snapshot menu and close main menu"""
    # close jars main menu
    win.destroy()
    # open Winshuttle menu
    winshuttle_menu()
    return


def winshuttle_back():
    # close Winshuttle menu
    wroot.destroy()
    # open jars main menu
    jars_menu()
    return


def conf_prog_bar_step():
    # define progress bar global variables
    global my_progress
    global pbar
    while my_progress['value'] < 100:
        my_progress['value'] += 10
        pbar.update_idletasks()
        time.sleep(1)
    # create message box alert
    ctypes.windll.user32.MessageBoxW(0, 'Process Completed', 'Completed', 0)
    # reset progress bar
    my_progress['value'] = 0
    return


def confirmations_maker_btn():
    """Button that completes file loading and launches scorecard_maker"""
    # check whether checkbox is checked
    global conftype_intvar
    conftypechecked = conftype_intvar.get()
    # check if each file has been selected
    if odrfilenamex == '' or ddrfilenamex == '' or olmfilenamex == '':
        ctypes.windll.user32.MessageBoxW(0, 'Please select all files needed for processing.', 'Files not selected', 0)
    # create planner if all files are selected
    else:
        # generate confirmations planner
        if conftypechecked == 0:
            conf_planner_creator(odrfilenamex, ddrfilenamex, olmfilenamex, conftypechecked)
        # generate STO planner
        else:
            conf_planner_creator(odrfilenamex, ddrfilenamex, olmfilenamex, conftypechecked)
        # kill menu
        root.destroy()
        # reopen confirmations menu
        confirmations_menu()
    return


def confirmations_menu():
    # ----- main --> define variables
    global odrfilenamex
    global ddrfilenamex
    global olmfilenamex
    global root
    odrfilenamex = ''
    ddrfilenamex = ''
    olmfilenamex = ''
    lfht = 215
    lfwt = 600
    coltwo = 365
    coloff = coltwo + 43
    # ----- create widget variables
    bht = 1  # button height
    bwt = 6  # button width
    # ----- create root window
    root = tk.Tk()
    # set as active window
    root.focus_force()
    # Gets the requested values of the height and width
    windowWidth = root.winfo_reqwidth()
    windowHeight = root.winfo_reqheight()
    # Gets both half the screen width/height and window width/height
    positionRight = int(root.winfo_screenwidth() / 2 - windowWidth / 2)
    positionDown = int(root.winfo_screenheight() / 2 - windowHeight / 2)
    positionRight = int(positionRight - (windowWidth * 1.5))
    positionDown = int(positionDown - (windowHeight / 2))
    # Positions the window in the center of the page.
    root.geometry("+{}+{}".format(positionRight, positionDown))
    # name window
    root.title('Confirmations Menu')
    mc_icon = resource_path('mc_icon.ico')
    root.iconbitmap(mc_icon)
    # create gui canvas
    canvas = tk.Canvas(root, height=lfht, width=lfwt)
    canvas.pack()
    # ----- Design GUI
    frame = tk.Frame(root, bg='#D8E0E4')
    frame.place(relx=0, rely=0, relwidth=1, relheight=1)
    finfo = Frame(root, bd=0, height=130, width=575, highlightbackground='#091F3F',
                  highlightcolor='#D8E0E4', highlightthickness=1)
    finfo.place(x=15, y=75)
    # create labels and buttons
    global clabel1
    clabel1 = StringVar()
    clabel1.set('Select an Order Data Report')
    global clabel2
    clabel2 = StringVar()
    clabel2.set('Select a Delivery Data Report')
    global clabel3
    clabel3 = StringVar()
    clabel3.set('Select an OSKU Line Mapping Report')
    # ----- Main Window frames and widgets
    tcenter = (lfwt / 2) + 30
    clflabel1 = Label(root, text='Confirmations Planning', fg='#091F3F', bg='#D8E0E4',
                      font=('Tuno', 30))
    clflabel1.place(x=tcenter, y=8, anchor='n')
    clflabel2 = Label(root, text='by Joseph Arnson', fg='#091F3F',
                      font=('Tuno', 14))
    clflabel2.place(x=coltwo, y=86, anchor='nw')
    odrbtn = Button(root, text='ODR', width=bwt, command=open_odrfile,
                    font=('Century_Gothic', 12), bg='#FFB000', fg='#091F3F')
    odrbtn.place(x=30, y=90)
    odrlbl = Label(root, textvariable=clabel1, font=('Century_Gothic', 11))
    odrlbl.place(x=100, y=95)
    ddrbtn = Button(root, text='DDR', width=bwt, command=open_ddrfile,
                    font=('Century_Gothic', 12), bg='#FF6912', fg='#091F3F')
    ddrbtn.place(x=30, y=122)
    ddrlbl = Label(root, textvariable=clabel2, font=('Century_Gothic', 11))
    ddrlbl.place(x=100, y=127)
    olmbtn = Button(root, text='OLM', width=bwt, command=open_olmfile,
                    font=('Century_Gothic', 12), bg='#1496FF', fg='#091F3F')
    olmbtn.place(x=30, y=154)
    l3 = Label(root, textvariable=clabel3, font=('Century_Gothic', 11))
    l3.place(x=100, y=159)
    # create checkbutton variable
    global conftype_intvar
    conftype_intvar = IntVar()
    # set checkbutton as unchecked by default
    conftype_intvar.set(0)
    # create checkbutton for confirmation type
    global conftypebox
    conftypebox = Checkbutton(root, text='Create STO Planner', variable=conftype_intvar,
                              onvalue=1, offvalue=0, font=('Century_Gothic', 11))  # command=quiet_update_datasearch_ini
    conftypebox.place(x=coltwo, y=122)
    # create confirmations planner button
    global optbtn1
    optbtn1 = Button(root, text='Create Planner', height=bht, width=22, command=confirmations_maker_btn,
                     font=('Century_Gothic', 12), fg='#091F3F', bg='#D8E0E4')
    optbtn1.place(x=coltwo, y=154)
    optbtn2 = Button(root, text='Back', height=bht, width=bwt, command=conf_go_back,
                     font=('Century_Gothic', 12), fg='#091F3F', bg='#D8E0E4')
    optbtn2.place(x=30, y=30)
    # ----- create window loop
    root.mainloop()
    return


def open_confirmations_menu():
    """From main menu open the Distributor Scorecard menu and close main menu"""
    win.destroy()
    confirmations_menu()
    return


def conf_go_back():
    # close confirmations planning menu
    root.destroy()
    # open jars main menu
    jars_menu()
    return


def upd_clabel1(txt):
    """Function to label 1"""
    clabel1.set(txt)
    return


def upd_clabel2(txt):
    """Function to label 2"""
    clabel2.set(txt)
    return


def upd_clabel3(txt):
    """Function to label 3"""
    clabel3.set(txt)
    return


def open_odrfile():
    """Select an Order Data Report file for confirmations planning"""
    Tk().withdraw()  # prevent root window
    global odrfilenamex
    odrfilenamex = askopenfilename(title='Select an Order Data Report',
                                   filetypes=[('Excel files', '.xlsx .xls')])
    if odrfilenamex != '':
        upd_clabel1('File selected')
    else:
        upd_clabel1('Select an Order Data Report')
    return odrfilenamex, clabel1


def open_ddrfile():
    """Select a Delivery Data Report file for confirmations planning"""
    Tk().withdraw()  # prevent root window
    global ddrfilenamex
    ddrfilenamex = askopenfilename(title='Select a Delivery Data Report',
                                   filetypes=[('Excel files', '.xlsx .xls')])
    if ddrfilenamex != '':
        upd_clabel2('File selected')
    else:
        upd_clabel2('Select a Delivery Data Report')
    return ddrfilenamex, clabel2


def open_olmfile():
    """Select a OSKU Line Mapping file for confirmations planning"""
    Tk().withdraw()  # prevent root window
    global olmfilenamex
    olmfilenamex = askopenfilename(title='Select an OSKU Line Mapping Report',
                                   filetypes=[('Excel files', '.xlsx .xls')])
    if olmfilenamex != '':
        upd_clabel3('File selected')
    else:
        upd_clabel3('Select an OSKU Line Mapping Report')
    return olmfilenamex, clabel3


def jars_menu():
    """Launch main menu for JARS application"""
    # get picture paths
    navy_jar = resource_path('jars_logo_navy_1000f.png')
    blue_jar = resource_path('jars_logo_blue_1000f.png')
    gold_jar = resource_path('jars_logo_gold_1000f.png')
    oran_jar = resource_path('jars_logo_oran_1000f.png')
    # Define the tkinter instance
    global win
    win = tk.Tk()
    # set as active window
    win.focus_force()
    # name the window
    win.title(jars_apptitle)
    # Gets the requested values of the height and width
    windowWidth = win.winfo_reqwidth()
    windowHeight = win.winfo_reqheight()
    # Gets both half the screen width/height and window width/height
    positionRight = int(win.winfo_screenwidth() / 2 - windowWidth / 2)
    positionDown = int(win.winfo_screenheight() / 2 - windowHeight / 2)
    positionRight = int(positionRight - (windowWidth * 1.5))
    positionDown = int(positionDown - (windowHeight / 2))
    # Positions the window in the center of the page.
    win.geometry("+{}+{}".format(positionRight, positionDown))
    # create application icon
    mc_icon = resource_path('mc_icon.ico')
    win.iconbitmap(mc_icon)
    # Define the size of the tkinter frame
    wincan = tk.Canvas(win, height=300, width=700)
    wincan.pack()
    # Import the image using PhotoImage function
    click_btn1 = PhotoImage(file=navy_jar)
    click_btn2 = PhotoImage(file=blue_jar)
    click_btn3 = PhotoImage(file=gold_jar)
    click_btn4 = PhotoImage(file=oran_jar)

    # button 1
    img_label1 = Label(image=click_btn1)
    button1 = Button(win, image=click_btn1, command=open_scorecard_menu, borderwidth=0)
    button1.place(anchor='n', relx=.2, rely=.13)
    label1 = Label(win, text='Scorecard', fg='#091F3F', font=('Tuno', 16))
    label1.place(relx=.19, rely=.8, anchor='n')

    # button 2
    img_label12 = Label(image=click_btn2)
    button2 = Button(win, image=click_btn2, command=open_confirmations_menu, borderwidth=0)
    # button2 = Button(win, image=click_btn2, command=conf_planner_creator, borderwidth=0)
    button2.place(anchor='n', relx=.4, rely=.13)
    label2 = Label(win, text='Confirmations', fg='#091F3F', font=('Tuno', 16))
    label2.place(relx=.39, rely=.8, anchor='n')

    # button 3
    img_label3 = Label(image=click_btn3)
    button3 = Button(win, image=click_btn3, command=launch_datasearch, borderwidth=0)
    button3.place(anchor='n', relx=.6, rely=.13)
    label3 = Label(win, text='DataSearch', fg='#091F3F', font=('Tuno', 16))
    label3.place(relx=.6, rely=.8, anchor='n')

    # button 4
    img_label4 = Label(image=click_btn4)
    button4 = Button(win, image=click_btn4, command=open_winshuttle_menu, borderwidth=0)
    button4.place(anchor='n', relx=.8, rely=.13)
    label4 = Label(win, text='Winshuttle', fg='#091F3F', font=('Tuno', 16))
    label4.place(relx=.8, rely=.8, anchor='n')

    # run gui
    win.mainloop()


if __name__ == "__main__":
    jars_menu()
